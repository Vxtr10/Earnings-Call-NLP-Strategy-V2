{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (1.22.4)\n",
      "Requirement already satisfied: filelock in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (21.3)\n",
      "Requirement already satisfied: requests in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (1.26.10)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.0.dev0\n",
      "    Uninstalling transformers-4.26.0.dev0:\n",
      "      Successfully uninstalled transformers-4.26.0.dev0\n",
      "Successfully installed transformers-4.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyphen\n",
      "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
      "Successfully installed pyphen-0.13.2 textstat-0.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.25.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using finbert (91% accuracy) instead of Loughran and McDonalds (61% accuracy)\n",
    "https://towardsdatascience.com/how-nlp-has-evolved-for-financial-sentiment-analysis-fb2990d9b3ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a â\\x80\\x93 I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's â\\x80\\x93 and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"\"\"\n",
    "I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a â I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's â and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\n",
    "\"\"\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a  I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's  and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr2 = re.sub(r'[^A-Za-z0-9.:,!\\' ]', '', mystr) #removes special characters such as the â from above\n",
    "mystr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation. And it's and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr3 = re.sub('\\s+', ' ', mystr2) #removes multiple spaces to a single space (i.e. there is a  I --> there is a I)\n",
    "mystr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.9691150188446045}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)\n",
    "results = sentiment_nlp(mystr3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenise string to individual sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I mean, I can comment a little bit about it.',\n",
       " \"I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency.\",\n",
       " \"And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.\",\n",
       " \"And it's and again, it's not just for us, it's for all of our competitors as well.\",\n",
       " 'They are all seeing the same deterioration.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystrtok = (nltk_tokenizer.tokenize(mystr3))\n",
    "mystrtok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLS-Classification:\n",
    "\n",
    "# Forward-looking statements (FLS) inform investors of managers’ beliefs and opinions about firm's future events or results. Identifying forward-looking statements from corporate reports can assist investors in financial analysis. FinBERT-FLS is a FinBERT model fine-tuned on 3,500 manually annotated sentences from Management Discussion and Analysis section of annual reports of Russell 3000 firms.\n",
    "\n",
    "# There are 3 classifications of FLS:\n",
    "# Specific foward looking statment  (S-FLS)\n",
    "# Non-specific foward looking statment (NS-FLS)\n",
    "# Not foward looking statment (N-FLS)\n",
    "\n",
    "# We are trying to take the average sentiments of all different classes of FLS statements.\n",
    "# E.g.\n",
    "# 4 sentences that are classed as S-FLS, with sentiments [0.6, 0.7, 0.8, 0.9]\n",
    "# so the average sentiment is 0.75\n",
    "# --> Hence, repeat for NS-FLS and NFLS\n",
    "\n",
    "# But first, I need to change the sentiment into a single numerical value\n",
    "# i.e. {'label': 'Negative', 'score': 0.5338950753211975} --> single value\n",
    "\n",
    "# so for \n",
    "# sentence1 = {'label': 'Negative', 'score': 0.5338950753211975}\n",
    "# sentence2 = {'label': 'Neutral', 'score': 0.999138355255127}\n",
    "# sentence3 = {'label': 'Positive', 'score': 0.9999885559082031}\n",
    "\n",
    "# negative can be mapped to -1\n",
    "# neutral to 0\n",
    "# positive to 1\n",
    "\n",
    "# the sentiment score probability is multiplied by their -1, 0 or 1\n",
    "\n",
    "# so:\n",
    "# sentence1 = -0.5338950753211975\n",
    "# sentence2 = 0\n",
    "# sentence 3 = 0.9999885559082031\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# given \n",
    "# [{'label': 'Not FLS', 'score': 0.9779328107833862},\n",
    "#  {'label': 'Not FLS', 'score': 0.9796538949012756},\n",
    "#  {'label': 'Specific FLS', 'score': 0.8796855211257935}]\n",
    "\n",
    "# and given\n",
    "# [{'label': 'Negative', 'score': 0.5338950753211975},\n",
    "#  {'label': 'Neutral', 'score': 0.999138355255127},\n",
    "#  {'label': 'Positive', 'score': 0.9999885559082031}]\n",
    "\n",
    "# Sentiment score of Not FlS = (-0.5339+0)/2 = -0.26695\n",
    "# Sentiment score of Specific FLS = 0.99998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLS classification\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marcelo Fischer : Just to correct the income from operations, the loss from operations for net2phone was minus $1.8 billion.',\n",
       " 'Of EBITDA itself was about minus $100,000, okay?',\n",
       " 'So we are getting closer to reaching EBITDA profitability and based on our 2023 projections, we do hope to exit 2023 with net2phone being an EBITDA-positive company, even as it continues to grow at rates which are probably higher than the average UCaaS play on basically.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkstr = \"Marcelo Fischer : Just to correct the income from operations, the loss from operations for net2phone was minus $1.8 billion. Of EBITDA itself was about minus $100,000, okay? So we are getting closer to reaching EBITDA profitability and based on our 2023 projections, we do hope to exit 2023 with net2phone being an EBITDA-positive company, even as it continues to grow at rates which are probably higher than the average UCaaS play on basically.\"\n",
    "\n",
    "nltkstr= (nltk_tokenizer.tokenize(nltkstr))\n",
    "nltkstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Not FLS', 'score': 0.9779328107833862},\n",
       " {'label': 'Not FLS', 'score': 0.9796538949012756},\n",
       " {'label': 'Specific FLS', 'score': 0.8796855211257935}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performs FLS-classifications on the above 3 sentences\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)\n",
    "fls_results = fls_nlp(nltkstr)\n",
    "fls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Negative', 'score': 0.5338950753211975},\n",
       " {'label': 'Neutral', 'score': 0.999138355255127},\n",
       " {'label': 'Positive', 'score': 0.9999885559082031}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = sentiment_nlp(nltkstr)\n",
    "sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Complexity:\n",
    "# https://pypi.org/project/textstat/\n",
    "# I chose Flesch Reading Ease and the Gunning Fog Index because\n",
    "# The Flesch Reading Ease score is a simple and widely used readability formula that can quickly give you an idea of how easy or difficult the text is to read. It uses the average sentence length and the average number of syllables per word to calculate the score. A higher score indicates that the text is easier to read, while a lower score indicates that the text is more difficult to read. This could help you identify if the language used in the call is easily understood by the general public.\n",
    "# The Gunning Fog Index, on the other hand, is a more sophisticated readability formula that is often used for business and government documents. It also uses the average sentence length and the average number of words per sentence to calculate the number of years of education needed to understand a text. This readability measure is more suited to evaluate the complexity of the texts, and indicate the level of sophistication of the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Not FLS', 'score': 0.9779328107833862}, {'label': 'Not FLS', 'score': 0.9796538949012756}, {'label': 'Specific FLS', 'score': 0.8796855211257935}]\n"
     ]
    }
   ],
   "source": [
    "fls_results = fls_nlp(nltkstr)\n",
    "print(fls_results)\n",
    "n_flslist = []\n",
    "s_flslist = []\n",
    "ns_flslist = []\n",
    "for i in range(0, len(nltkstr)):\n",
    "    sentence = nltkstr[i]\n",
    "    if fls_results[i]['label'] == 'Not FLS':\n",
    "        n_flslist.append(sentence)\n",
    "    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "        s_flslist.append(sentence)\n",
    "    elif fls_results[i]['label'] == 'Non-specific FLS':\n",
    "        ns_flslist.append(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marcelo Fischer : Just to correct the income from operations, the loss from operations for net2phone was minus $1.8 billion.',\n",
       " 'Of EBITDA itself was about minus $100,000, okay?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences that areclassed as \"Not FLS\"\n",
    "n_flslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So we are getting closer to reaching EBITDA profitability and based on our 2023 projections, we do hope to exit 2023 with net2phone being an EBITDA-positive company, even as it continues to grow at rates which are probably higher than the average UCaaS play on basically.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences that areclassed as \"Specific FLS\"\n",
    "s_flslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences that areclassed as \"Not Specific FLS\"\n",
    "ns_flslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flsliststr = ' '.join(n_flslist) # turns all of the non fls sentences into one paragraph for future analysis.\n",
    "\n",
    "# maps sentiment data so it outputs a single sentiment value\n",
    "def map_sentiments(sentiment_result):\n",
    "    sentiment_result = sentiment_result[0]\n",
    "    if sentiment_result['label'] == 'Negative':\n",
    "        return -1 * sentiment_result['score']\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Neutral':\n",
    "        return 0\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Positive':\n",
    "        return sentiment_result['score']\n",
    "\n",
    "def get_NLP_values(liststr):\n",
    "    # further analysis includes finding sentiment and word complexity.\n",
    "    if len(liststr) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        # maps sentiment data so it outputs a single sentiment value\n",
    "        sentiment_result = sentiment_nlp(liststr)\n",
    "        # gets \n",
    "        sentiment_score = map_sentiments(sentiment_result)\n",
    "\n",
    "        # word complexity:\n",
    "        flesch_score = textstat.flesch_reading_ease(liststr)\n",
    "        gunning_fog_score = textstat.gunning_fog(liststr)\n",
    "\n",
    "        return sentiment_score, flesch_score, gunning_fog_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 57.77 8.36\n",
      "0.9999885559082031 24.79 24.49\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "n_flsliststr = ' '.join(n_flslist) # turns all of the non fls sentences into one paragraph for future analysis.\n",
    "s_flsliststr = ' '.join(s_flslist)\n",
    "ns_flsliststr = ' '.join(ns_flslist)\n",
    "\n",
    "n_fls_sentiment_score, n_fls_flesch_score, n_fls_gunning_fog_score = get_NLP_values(n_flsliststr)\n",
    "s_fls_sentiment_score, s_fls_flesch_score, s_fls_gunning_fog_score = get_NLP_values(s_flsliststr)\n",
    "ns_fls_sentiment_score, ns_fls_flesch_score, ns_fls_gunning_fog_score = get_NLP_values(ns_flsliststr)\n",
    "\n",
    "\n",
    "print(n_fls_sentiment_score, n_fls_flesch_score, n_fls_gunning_fog_score)\n",
    "print(s_fls_sentiment_score, s_fls_flesch_score, s_fls_gunning_fog_score)\n",
    "print(ns_fls_sentiment_score, ns_fls_flesch_score, ns_fls_gunning_fog_score)\n",
    "\n",
    "# 0      -->    neutral sentiment\n",
    "# 57.77  -->    flesch_score value\n",
    "# 8.36   -->    gunning_fog_score value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of .csv file info for each stock:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# META DATA\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 0. Year of transcript release\n",
    "# 1. Quarter of transcript release\n",
    "# 2. Date of transcript release\n",
    "# 3. Earnings Transcript contents\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Feature Extractions:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 4. EPS surprise value\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Transcript Features:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 5. Whole transcript - net sentiment\n",
    "# 6. Whole transcript - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole transcript - net word complex\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Pre release:\n",
    "# 8. Whole pre-release - net sentiment\n",
    "# 9. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 10. Whole pre-release - net word complexity\n",
    "#\n",
    "#\n",
    "# 11. Specific foward looking statment - sentiment\n",
    "# 12. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 13. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 14. Non Specific Forward looking statement - sentiment \n",
    "# 15. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - word complexity\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Questions & Answers:\n",
    "# 20. Whole Q&A - net sentiment\n",
    "# 21. Whole Q&A – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 22. Whole Q&A - net word complexity\n",
    "# \n",
    "# 23. all question (aggregate) - sentiment\n",
    "# 24. all question (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 25. all question (aggregate) - word complex\n",
    "# 26. all reply (aggregate) - sentiment\n",
    "# 27. all reply (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 28. all reply (aggregate) - word complex\n",
    "#\n",
    "# For all replies (aggregate):\n",
    "# 29: Specific foward looking statment - sentiment \n",
    "# 30: Specific foward looking statment – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 31: Specific foward looking statment - word complexity\n",
    "# 32: Non Specific Forward looking statement - sentiment\n",
    "# 33: Non Specific Forward looking statement – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 34: Non Specific Forward looking statement - word complexity\n",
    "# 35: Not Foward looking statement - sentiment\n",
    "# 36: Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 37: Not Foward looking statement - word complexity\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 38: \"margin\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 39: \"cost\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 40: \"revenue\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 41: \"earnings\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 42: \"growth\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 43: \"EBIDTA\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 44: \"leverage\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 45: \"debt\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 46: \"price\" – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 47: Rolling frame of Whole Transcripts - TF-IDF value\n",
    "# 48: Rolling frame of Pre-releases - TF-IDF value\n",
    "# 49: Rolling frame of Management Sentences (Their Replies) - TF-IDF\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 50: Stock price difference between Day 0 (Day earnings call is released) and Day 10\n",
    "# 51: Stock price difference between Day 0 (Day earnings call is released) and Day 20\n",
    "# 52: Stock price difference between Day 0 (Day earnings call is released) and Day 30\n",
    "# 53: Stock price difference between Day 0 (Day earnings call is released) and Day 40\n",
    "# 54: Stock price difference between Day 0 (Day earnings call is released) and Day 50\n",
    "# 55: Stock price difference between Day 0 (Day earnings call is released) and Day 60\n",
    "# 56: Stock price difference between Day 0 (Day earnings call is released) and Day 70\n",
    "# 57: Stock price difference between Day 0 (Day earnings call is released) and Day 80\n",
    "# 58: Stock price difference between Day 0 (Day earnings call is released) and Day 90\n",
    "#\n",
    "# 59: Stock price difference between Day 1 (Day earnings call is released) and Day 10\n",
    "# 60: Stock price difference between Day 1 (Day earnings call is released) and Day 20\n",
    "# 61: Stock price difference between Day 1 (Day earnings call is released) and Day 30\n",
    "# 62: Stock price difference between Day 1 (Day earnings call is released) and Day 40\n",
    "# 63: Stock price difference between Day 1 (Day earnings call is released) and Day 50\n",
    "# 64: Stock price difference between Day 1 (Day earnings call is released) and Day 60\n",
    "# 65: Stock price difference between Day 1 (Day earnings call is released) and Day 70\n",
    "# 66: Stock price difference between Day 1 (Day earnings call is released) and Day 80\n",
    "# 67: Stock price difference between Day 1 (Day earnings call is released) and Day 90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF IDF value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
