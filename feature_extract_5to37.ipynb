{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(path):\n",
    "    mytranscript = pd.read_csv(path).iloc[[2]].values[0][0] \n",
    "    mytranscript = re.sub(r'[^A-Za-z0-9.,:!\\'\\n ]', '', mytranscript)\n",
    "    mytranscript = re.sub('[^\\S\\n]+', ' ', mytranscript) #replaces multiple spaces to single space, without deleting newlines \\n in the process\n",
    "    mytranscript = mytranscript.splitlines() # finds transcript\n",
    "    return mytranscript\n",
    "\n",
    "def split_transcript(mytranscript):    \n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    for i in range(0, len(mytranscript)):\n",
    "        speech_bubble = mytranscript[i].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space so the IF condition below can run smoothly\n",
    "        # finds the following condition (what operator says) and splits the transcript into 2)\n",
    "        if (i > 2) and ((\"operator:\" in speech_bubble) and ((\"question\" in speech_bubble) or (\"go ahead\" in speech_bubble) or (\"operator instructions\" in speech_bubble))):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "    return transcript_safe_harbour, transcript_questions\n",
    "\n",
    "def get_file_speaker_names(sector, stock):\n",
    "    write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "    speaker_names = np.loadtxt(write_path, delimiter='\\n', dtype=str)\n",
    "    return speaker_names\n",
    "    \n",
    "# finds a list of analyst names for a single .csv file\n",
    "def find_analyst_names(speaker_names, transcript_questions):\n",
    "    analyst_names = []\n",
    "    # the programme recognises the question is being asked by an analyst when the following conditions are met:\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "        if \"operator:\" in speech_bubble:\n",
    "            for name in speaker_names:\n",
    "                namelist = name.split()\n",
    "                if name.lower() != \"operator\": \n",
    "                    for name_2 in namelist: # cycle through each name in the name_list\n",
    "                        name_2 = name_2.lower()\n",
    "                        # checks if the speaker name happens to be in the speech_bubble, if it is, then the person speaking is an analyst\n",
    "                        # also len(name) > 2 is used to avoid the problem with single letters being registered as in the speech_bubble \n",
    "                        # (e.g. the letter \"A\" in the name \"A Gayn Erickson\" will be in the speech_bubble, but Gayn Erickson is not an analyst, so \"A\" is not counted)\n",
    "                        if (name_2 in speech_bubble) and len(name_2) > 2:\n",
    "                            analyst_names.append(name)\n",
    "                    if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "                        analyst_names.append(name) \n",
    "                        \n",
    "    analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "    return analyst_names\n",
    "\n",
    "def get_analyst_management_sentences(analyst_names, transcript_questions):\n",
    "    analyst_sentences = []\n",
    "    management_sentences = []\n",
    "\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        if speaker_name in analyst_names:\n",
    "            analyst_sentences.append(speech_bubble)\n",
    "\n",
    "        elif speaker_name.lower() == \"operator\":\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            management_sentences.append(speech_bubble)\n",
    "\n",
    "    return analyst_sentences, management_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is is string is is string'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}  # create an empty dictionary to store the children of the node\n",
    "        self.is_word = False  # flag to indicate if the node represents the end of a word\n",
    "        self.fail = None  # pointer to the fail node\n",
    "        self.word = None  # word stored in the node\n",
    "\n",
    "class AhoCorasick:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()  # initialize the root of the Trie\n",
    "        self.build_trie(words)  # build the Trie from the list of words\n",
    "        self.build_ac_automata()  # build the AC automata from the Trie\n",
    "\n",
    "    def build_trie(self, words):\n",
    "        for word in words:\n",
    "            node = self.root  # start at the root of the Trie\n",
    "            for char in word:\n",
    "                if char not in node.children:  # if the character is not in the children dictionary, create a new TrieNode\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]  # move to the child node\n",
    "            node.is_word = True  # set the is_word flag to indicate the end of a word\n",
    "            node.word = word  # store the word in the node\n",
    "\n",
    "    def build_ac_automata(self):\n",
    "        queue = []  # initialize a queue to keep track of the nodes to visit\n",
    "\n",
    "        for node in self.root.children.values():  # start with the children of the root node\n",
    "            queue.append(node)  # add the child node to the queue\n",
    "            node.fail = self.root  # set the fail pointer to the root\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)  # get the next node to visit from the queue\n",
    "            for char, child in node.children.items():  # iterate through the children of the current node\n",
    "                queue.append(child)  # add the child to the queue\n",
    "                fail_node = node.fail  # start at the fail node of the current node\n",
    "                while fail_node is not None and char not in fail_node.children:  # follow the fail pointers until a match is found or the root is reached\n",
    "                    fail_node = fail_node.fail\n",
    "                if fail_node is None:\n",
    "                    child.fail = self.root\n",
    "                else:\n",
    "                    child.fail = fail_node.children[char]\n",
    "                child.is_word |= child.fail.is_word\n",
    "\n",
    "    def search(self, text):\n",
    "        node = self.root\n",
    "        new_text = text\n",
    "        for i, char in enumerate(text):\n",
    "            while node is not None and char not in node.children:\n",
    "                node = node.fail\n",
    "            if node is None:\n",
    "                node = self.root\n",
    "                continue\n",
    "            node = node.children[char]\n",
    "            if node.is_word:\n",
    "                new_text = new_text.replace(node.word, '')\n",
    "        return new_text\n",
    "\n",
    "# Usage\n",
    "list1 = ['string1', 'string2', 'string3']\n",
    "sentence = \"string1 is string2 is string is string3 is string\"\n",
    "ac = AhoCorasick(list1)\n",
    "new_sentence = ac.search(sentence) # deletes a particular string from new_sentence if that string is presnet in list1\n",
    "new_sentence = re.sub('[^\\S\\n]+', ' ', new_sentence)\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.9691150188446045}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation. And it's and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\"\n",
    "result = sentiment_nlp(mystr)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiments(sentiment_result):\n",
    "    sentiment_result = sentiment_result[0]\n",
    "    if sentiment_result['label'] == 'Negative':\n",
    "        return -1 * sentiment_result['score'], \"negative\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Neutral':\n",
    "        return 0, \"neutral\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Positive':\n",
    "        return sentiment_result['score'], \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_paragraph_into_sentences(temp):\n",
    "    sentences = nltk.sent_tokenize(temp)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLS classification\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NLP_values(liststr):\n",
    "    # further analysis includes finding sentiment and word complexity.\n",
    "    if len(liststr) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        # maps sentiment data so it outputs a single sentiment value\n",
    "        sentiment_result = sentiment_nlp(liststr)\n",
    "        # gets \n",
    "        sentiment_score = map_sentiments(sentiment_result)\n",
    "\n",
    "        # word complexity:\n",
    "        flesch_score = textstat.flesch_reading_ease(liststr)\n",
    "        gunning_fog_score = textstat.gunning_fog(liststr)\n",
    "\n",
    "        return sentiment_score, flesch_score, gunning_fog_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "\n",
    "def getFeature5to22(pre_release, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    net_sentiment_list = []\n",
    "    flesch_list = []\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    feature_extract_5 = 0\n",
    "    feature_extract_6 = 0\n",
    "    feature_extract_7 = 0\n",
    "    feature_extract_8 = 0\n",
    "    feature_extract_9 = 0\n",
    "    feature_extract_10 = 0\n",
    "    feature_extract_11 = 0\n",
    "    feature_extract_12 = 0\n",
    "    feature_extract_13 = 0\n",
    "    feature_extract_14 = 0\n",
    "    feature_extract_15 = 0\n",
    "    feature_extract_16 = 0\n",
    "    feature_extract_17 = 0\n",
    "    feature_extract_18 = 0\n",
    "    feature_extract_19 = 0\n",
    "    feature_extract_20 = 0\n",
    "    feature_extract_21 = 0\n",
    "    feature_extract_22 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in pre_release:\n",
    "            try:\n",
    "                new_speech_bubble = ac.search(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                    # gets text complexity\n",
    "                    flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                    flesch_list.append(flesch_score)\n",
    "\n",
    "                    new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "                    # print(new_speech_bubble_list)\n",
    "                    fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                    # print(fls_results)\n",
    "                    for i in range(0, len(new_speech_bubble_list)):\n",
    "                        sentence = new_speech_bubble_list[i]\n",
    "                        if fls_results[i]['label'] == 'Not FLS':\n",
    "                            n_flslist.append(sentence)\n",
    "                        elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                            s_flslist.append(sentence)\n",
    "                        elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                            ns_flslist.append(sentence) \n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, fls1_sentiment_list, net1_positive, net1_negative, net1_neutral = get_fls_features(s_flslist)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, fls2_sentiment_list, net2_positive, net2_negative, net2_neutral = get_fls_features(ns_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, fls3_sentiment_list, net3_positive, net3_negative, net3_neutral = get_fls_features(n_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            numb_s_flslist = len(s_flslist)\n",
    "            numb_ns_flslist = len(ns_flslist)\n",
    "            numb_n_flslist = len(n_flslist)\n",
    "            total = numb_s_flslist + numb_ns_flslist + numb_n_flslist\n",
    "\n",
    "            feature_extract_21 = numb_s_flslist/total\n",
    "            feature_extract_22 = numb_ns_flslist/total\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        net_positive = net1_positive + net2_positive + net3_positive\n",
    "        net_negative = net1_negative + net2_negative + net3_negative\n",
    "        net_neutral = net1_neutral + net2_neutral + net3_neutral\n",
    "        net_sentiment_list = fls1_sentiment_list + fls2_sentiment_list + fls3_sentiment_list\n",
    "        \n",
    "        try:\n",
    "            feature_extract_5 = statistics.mean(net_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_6 = net_positive/(net_negative+net_positive+net_neutral)\n",
    "            feature_extract_7 = net_negative/(net_negative+net_positive+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_8 = statistics.mean(flesch_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fea_ext_list5to22 = [feature_extract_5, feature_extract_6, feature_extract_7, feature_extract_8, feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, feature_extract_21, feature_extract_22]\n",
    "\n",
    "    except:\n",
    "        return fea_ext_list5to22\n",
    "    \n",
    "    return fea_ext_list5to22\n",
    "\n",
    "\n",
    "def get_fls_features(flslist):\n",
    "    fls_sentiment_list = []\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "    \n",
    "    feature_extract_1 = 0\n",
    "    feature_extract_2 = 0\n",
    "    feature_extract_3 = 0\n",
    "    feature_extract_4 = 0\n",
    "\n",
    "    for each_fls in flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        fls_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "    try:\n",
    "        feature_extract_1 = statistics.mean(fls_sentiment_list)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_2 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_3 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_4 = textstat.flesch_reading_ease(' '.join(flslist))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return feature_extract_1, feature_extract_2, feature_extract_3, feature_extract_4, fls_sentiment_list, net_positive, net_negative, net_neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment\n",
    "# 24. Whole Q&A – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 25. Whole Q&A – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment\n",
    "# 28. all question (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 30. all question (aggregate) - word complex\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment\n",
    "# 32. all reply (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 34. all reply (aggregate) - word complex\n",
    "#\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 38. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 39. Non Specific Forward looking statement - sentiment \n",
    "# 40. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 41. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 42. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 43. Not Foward looking statement - sentiment\n",
    "# 44. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 45. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 46. Not Foward looking statement - word complexity\n",
    "\n",
    "\n",
    "# 1. get rid of the speaker names from each \"speech bubble\" (i.e. analyst_speech, management_speech)\n",
    "\n",
    "# 2. FOR loop - goes through each speech bubble, and parse them to sentences\n",
    "#   2a. in the For loop - classify the sentences (S-FLS, NS-FLS, N-FLS)\n",
    "#   2b. Creates x3 2D FLS lists where each 1D value of the list is [\"the sentence\", \"is this sentence a question or a reply?\"] (titled: s_FLS_2D, ns_FLS_2D, n_FLS_2D)\n",
    "\n",
    "# 3. Goes through each FLS_2D list and find sentiment of each with the following conditions:\n",
    "# 3a. if sentence is s_FLS\n",
    "    # 3ai. if sentence is a \"question\":\n",
    "        #- Sentiment value is appended to \"question_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"question_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"s_fls_complex_list\"\n",
    "\n",
    "    # 3ai. if sentence is a \"reply\":\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"reply_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"s_fls_complex_list\"\n",
    "\n",
    "\n",
    "# 3b. if sentence is ns_FLS\n",
    "    # 3bi. if sentence is a \"question\":\n",
    "        #- Sentiment value is appended to \"question_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"question_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"ns_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"ns_fls_complex_list\"\n",
    "\n",
    "    # 3bi. if sentence is a \"reply\":\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"reply_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"ns_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"ns_fls_complex_list\"\n",
    "\n",
    "# 3c. if sentence is n_FLS\n",
    "    # 3ci. if sentence is a \"question\":\n",
    "        #- Sentiment value is appended to \"question_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"question_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"n_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"n_fls_complex_list\"\n",
    "\n",
    "    # 3ci. if sentence is a \"reply\":\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"reply_complex_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #\n",
    "        #- Sentiment value is appended to \"n_fls_sentiment_list\"\n",
    "        #- Word Complexity is appended to \"n_fls_complex_list\"\n",
    "\n",
    "\n",
    "def getFeature23to46(analyst_speech, management_speech, speaker_names):\n",
    "    fea_ext_list23to46 = [0]*24\n",
    "    \n",
    "    # print(analyst_sentences, management_sentence)\n",
    "    # get_fls_features(flslists)\n",
    "\n",
    "\n",
    "    # 1. get rid of the speaker names from each \"speech bubble\" (i.e. analyst_speech, management_speech)\n",
    "\n",
    "    # 2. FOR loop - goes through each speech bubble, and parse them to sentences\n",
    "    #   2a. in the For loop - classify the sentences (S-FLS, NS-FLS, N-FLS)\n",
    "    #   2b. Creates x3 2D FLS lists where each 1D value of the list is [\"the sentence\", \"is this sentence a question or a reply?\"] (titled: s_FLS_2D, ns_FLS_2D, n_FLS_2D)\n",
    "\n",
    "    # 3. Goes through each FLS_2D list and find sentiment of each with the following conditions:\n",
    "        # 3a. if sentence is s_FLS\n",
    "            # 3ai. if sentence is a \"question\":\n",
    "                #- Sentiment value is appended to \"question_sentiment_list\"\n",
    "                #- Word Complexity is appended to \"question_complex_list\"\n",
    "                #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "                #\n",
    "                #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "                #- Word Complexity is appended to \"s_fls_complex_list\"\n",
    "\n",
    "            # 3ai. if sentence is a \"reply\":\n",
    "                #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "                #- Word Complexity is appended to \"reply_complex_list\"\n",
    "                #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "                #\n",
    "                #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "                #- Word Complexity is appended to \"s_fls_complex_list\"\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(0, len(new_speech_bubble_list)):\n",
    "        sentence = new_speech_bubble_list[i]\n",
    "        if fls_results[i]['label'] == 'Not FLS':\n",
    "            n_flslist.append(sentence)\n",
    "        elif fls_results[i]['label'] == 'Specific FLS':\n",
    "            s_flslist.append(sentence)\n",
    "        elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "            ns_flslist.append(sentence) \n",
    "\n",
    "\n",
    "    fea_ext_list23to46[0] = 99999999\n",
    "    \n",
    "    return fea_ext_list23to46\n",
    "    #for replies:\n",
    "    # try:\n",
    "    #     feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12 = get_fls_features(s_flslist)\n",
    "\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # try:\n",
    "    #     feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16 = get_fls_features(ns_flslist)\n",
    "\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # try:\n",
    "    #     feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20 = get_fls_features(n_flslist)\n",
    "\n",
    "    # except:\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99999999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "fea_ext_list23to46 = getFeature23to46(analyst_sentences, management_sentences, speaker_names)\n",
    "print(fea_ext_list23to46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46377516070077585, 0.5581395348837209, 0.08139534883720931, 61.514, -0.22222865952385795, 0.1111111111111111, 0.3333333333333333, 47.89, 0, 0.0, 0.0, 56.76, 0.5078843824611687, 0.5900621118012422, 0.06832298136645963, 60.35, 0.05232558139534884, 0.011627906976744186]\n"
     ]
    }
   ],
   "source": [
    "sector = \"tech\"\n",
    "stock = \"AAPL\"\n",
    "\n",
    "path = \"sectors/tech/AAPL/AAPL20224.csv\"\n",
    "\n",
    "mytranscript = get_transcript(path)\n",
    "transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "speaker_names = get_file_speaker_names(sector, stock)\n",
    "analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "\n",
    "not_current_analyst_names = []\n",
    "\n",
    "for names in speaker_names:\n",
    "    if names not in analyst_names:\n",
    "        not_current_analyst_names.append(names)\n",
    "        \n",
    "analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "fea_ext_list5to22 = getFeature5to22(transcript_safe_harbour, speaker_names)\n",
    "print(fea_ext_list5to22)\n",
    "\n",
    "fea_ext_list23to46 = getFeature23to46(analyst_sentences, management_sentences, speaker_names)\n",
    "print(fea_ext_list23to46)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46377516070077585,\n",
       " 0.5581395348837209,\n",
       " 0.08139534883720931,\n",
       " 61.514,\n",
       " -0.22222865952385795,\n",
       " 0.1111111111111111,\n",
       " 0.3333333333333333,\n",
       " 47.89,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 56.76,\n",
       " 0.5078843824611687,\n",
       " 0.5900621118012422,\n",
       " 0.06832298136645963,\n",
       " 60.35,\n",
       " 0.05232558139534884,\n",
       " 0.011627906976744186]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_ext_list5to22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speaker_names = [word + ':' for word in speaker_names]\n",
    "mytranscript = mytranscript[0:-2]\n",
    "\n",
    "ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "for sentence in mytranscript:\n",
    "    new_sentence = ac.search(sentence)\n",
    "    new_sentence = re.sub('[^\\S\\n]+', ' ', new_sentence)\n",
    "    if new_sentence[0] == \" \": \n",
    "        new_sentence = new_sentence.replace(\" \", \"\", 1) # replace the first space bar with an empty string\n",
    "        # ' is is string is is string' to 'is is string is is string'\n",
    "    print(new_sentence)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"semiconductors\"\n",
    "stock = \"AEHR\"\n",
    "\n",
    "sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[1-9]**[1-9]*[1-4].*')\n",
    "sector_files.sort(reverse=True)\n",
    "for path in sector_files: # for every .csv path of that stock\n",
    "    mytranscript = get_transcript(path)\n",
    "    transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "    speaker_names = get_file_speaker_names(sector, stock)\n",
    "    analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of .csv file info for each stock:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# META DATA\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 0. Year of transcript release\n",
    "# 1. Quarter of transcript release\n",
    "# 2. Date of transcript release\n",
    "# 3. Earnings Transcript contents\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Feature Extractions:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 4. EPS surprise value\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Transcript Features:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment\n",
    "# 24. Whole Q&A – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 25. Whole Q&A – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment\n",
    "# 28. all question (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 30. all question (aggregate) - word complex\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment\n",
    "# 32. all reply (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 34. all reply (aggregate) - word complex\n",
    "#\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 38. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 39. Non Specific Forward looking statement - sentiment \n",
    "# 40. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 41. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 42. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 43. Not Foward looking statement - sentiment\n",
    "# 44. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 45. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 46. Not Foward looking statement - word complexity\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 47: \"margin\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 48: \"cost\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 49: \"revenue\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 50: \"earnings\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 51: \"growth\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 52: \"EBIDTA\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 53: \"leverage\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 54: \"debt\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 55: \"price\" – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 56: Rolling frame of Whole Transcripts - TF-IDF value\n",
    "# 57: Rolling frame of Pre-releases - TF-IDF value\n",
    "# 58: Rolling frame of Management Sentences (Their Replies) - TF-IDF\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 59: Stock price difference between Day 0 (Day earnings call is released) and Day 10\n",
    "# 60: Stock price difference between Day 0 (Day earnings call is released) and Day 20\n",
    "# 61: Stock price difference between Day 0 (Day earnings call is released) and Day 30\n",
    "# 62: Stock price difference between Day 0 (Day earnings call is released) and Day 40\n",
    "# 63: Stock price difference between Day 0 (Day earnings call is released) and Day 50\n",
    "# 64: Stock price difference between Day 0 (Day earnings call is released) and Day 60\n",
    "# 65: Stock price difference between Day 0 (Day earnings call is released) and Day 70\n",
    "# 66: Stock price difference between Day 0 (Day earnings call is released) and Day 80\n",
    "# 67: Stock price difference between Day 0 (Day earnings call is released) and Day 90\n",
    "#\n",
    "# 68: Stock price difference between Day 1 (Day earnings call is released) and Day 10\n",
    "# 69: Stock price difference between Day 1 (Day earnings call is released) and Day 20\n",
    "# 70: Stock price difference between Day 1 (Day earnings call is released) and Day 30\n",
    "# 71: Stock price difference between Day 1 (Day earnings call is released) and Day 40\n",
    "# 72: Stock price difference between Day 1 (Day earnings call is released) and Day 50\n",
    "# 73: Stock price difference between Day 1 (Day earnings call is released) and Day 60\n",
    "# 74: Stock price difference between Day 1 (Day earnings call is released) and Day 70\n",
    "# 75: Stock price difference between Day 1 (Day earnings call is released) and Day 80\n",
    "# 76: Stock price difference between Day 1 (Day earnings call is released) and Day 90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete operator name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.spglobal.com/marketintelligence/en/news-insights/blog/analyzing-sentiment-in-quarterly-earnings-calls-q2-2022\n",
    "\n",
    "\n",
    "# https://www.amenityanalytics.com/case-studies/earnings-call-transcript-analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF ----> from management sentences (Replies + pre-release)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
