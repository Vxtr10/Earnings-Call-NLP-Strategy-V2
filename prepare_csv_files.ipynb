{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numblist = [1,2,3,4,5,6,7]\n",
    "\n",
    "sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "urllist = []\n",
    "\n",
    "for sector in sectorlist:\n",
    "    for number in numblist:\n",
    "        urllist.append('https://simplywall.st/stocks/us/' + sector + '/market-cap-large?page=' + str(number))\n",
    "\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "df = pd.DataFrame(columns=['href','ticker'])\n",
    "\n",
    "for i in range(0, len(urllist)+1):\n",
    "    if count == 7:   \n",
    "            count = 0\n",
    "            df.to_csv('hrefs/ref_' + (sectorlist[count2]) + '.csv', index=False)\n",
    "            count2 += 1\n",
    "            df = pd.DataFrame(columns=['href','ticker'])\n",
    "        \n",
    "    try:\n",
    "        url = urllist[i]\n",
    "        print(url)\n",
    "        count+=1\n",
    "        html_text = requests.get(url).text\n",
    "        \n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        body = soup.find('tbody').find_all('tr')\n",
    "\n",
    "        for j in body:\n",
    "            temp = j.find_all('td')[1]\n",
    "            insert_row = {\"href\": (temp.a['href']), \"ticker\": ''.join(temp.find('b').contents)}\n",
    "            df = pd.concat([df, pd.DataFrame([insert_row])])\n",
    "\n",
    "    except:\n",
    "        continue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "# \"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "# \"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "# \"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "# \"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "sectorlist = [\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "for sector in sectorlist:\n",
    "    df = pd.read_csv('hrefs/ref_' + (sector) + '.csv')\n",
    "    \n",
    "    hrefs = df[\"href\"]\n",
    "    df_fair_value = pd.DataFrame()\n",
    "    for href in hrefs:\n",
    "        time.sleep(1.5)\n",
    "        url = \"https://simplywall.st\" + str(href) + \"/valuation\"\n",
    "        print(url)\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        body = soup.find_all('blockquote')\n",
    "        count = 0\n",
    "        fair_value = np.NaN\n",
    "        for i in body:\n",
    "            count += 1\n",
    "            if count < 25:\n",
    "                try:\n",
    "                    q = ''.join(i.find('p').find_all('span')[3].contents)\n",
    "                    if q[2] == \"$\":\n",
    "                        fair_value = q\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                break\n",
    "        insert_row = {\"fair_value\": fair_value}\n",
    "        \n",
    "        df_fair_value = pd.concat([df_fair_value, pd.DataFrame([insert_row])])\n",
    "        print(fair_value)\n",
    "    df_initial = pd.read_csv('hrefs/ref_'+sector+'.csv')\n",
    "    \n",
    "    \n",
    "    #df_initial = df_initial.drop('fair_value', axis=1)\n",
    "   \n",
    "    temp2 = df_fair_value.to_numpy()\n",
    "    \n",
    "    df_initial.insert(loc=2, column=\"fair_value\", value=temp2)\n",
    "    df_initial.to_csv('hrefs/ref_' + (sector) + '.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', df_next.shape[0]+1)\n",
    "\n",
    "df_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_csv('hrefs/ref_'+\"automobiles\"+'.csv')\n",
    "temp2 = df_fair_value.to_numpy()\n",
    "df_initial.insert(loc=2, column=\"fair_value\", value=temp2)\n",
    "df_initial.to_csv('hrefs/ref_' + (sector) + '_copy.csv', index=False)\n",
    "\n",
    "df_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletes all duplicates in dataframe\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "for sector in sectorlist:\n",
    "    df_initial = pd.read_csv('hrefs/ref_'+sector+'.csv')\n",
    "    print(df_initial)\n",
    "    df_initial = df_initial.drop_duplicates()\n",
    "    df_initial.to_csv('hrefs/ref_'+sector+'.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transcript(ticker, date_yr, date_qtr, i):\n",
    "    try:\n",
    "        url = 'https://roic.ai/transcripts/' + ticker + '?y=' + str(date_yr) + '&q=' + str(date_qtr)\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        # site below is used to visualise JSON files (indented)\n",
    "        # https://jsonformatter.org/\n",
    "        # finds the 15th <script/> and strips the unnecsssary data so it can be read in JSON\n",
    "        script = soup.find_all('script')[15].text.strip()  \n",
    "        data = json.loads(script)\n",
    "        transcript_data = data['props']['pageProps']['transcriptdata']['content']  # loads the transcript content\n",
    "        exact_date = datetime.strptime(data['props']['pageProps']['data']['data']['earningscalls'][i]['date'], '%Y-%m-%d %H:%M:%S')\n",
    "        #exact_date = 'day 0', earnings transcript release date\n",
    "        ntr = yf.Ticker(ticker)\n",
    "    except:\n",
    "        transcript_data = np.NaN\n",
    "        exact_date = np.NaN\n",
    "        \n",
    "    return transcript_data, exact_date\n",
    "\n",
    "def ticker_transcript(ticker):\n",
    "    #loops around each ticker, feature extracts each by calling relevant functions below\n",
    "    try:\n",
    "        url = 'https://roic.ai/transcripts/' + ticker\n",
    "\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "        script = soup.find_all('script')[15].text.strip()\n",
    "        # finds json data (i.e. transcript)\n",
    "        data = json.loads(script)\n",
    "        first_er_date = data['props']['pageProps']['data']['data']['earningscalls'][0]\n",
    "        last_er_date = data['props']['pageProps']['data']['data']['earningscalls'][-1]\n",
    "        # reads the json data and finds the first and last date for which the transcript is held and its set as limits\n",
    "        # perhaps I worded it badly...\n",
    "        # first_er_date_yr represents the most recent transcript's date (i.e. 2022)\n",
    "        # last_er_date represents the last transcript's date (e.g. 2006)\n",
    "\n",
    "        first_er_date_yr = first_er_date['year'] \n",
    "        first_er_date_qtr = first_er_date['quarter']\n",
    "        last_er_date_yr = last_er_date['year']\n",
    "        # calculates the year differences between the dates of first and last transcript so it can be looped\n",
    "        yr_difference = first_er_date_yr-last_er_date_yr\n",
    "        date_yr = first_er_date_yr\n",
    "        date_qtr = first_er_date_qtr\n",
    "\n",
    "        arr_ticker_infos = np.empty((0, 4), str)\n",
    "        arr_full_transcript = np.array([], str) #array of all of the transcripts, every year and quarter\n",
    "        q = -1\n",
    "\n",
    "        for i in range(0, yr_difference + 1):\n",
    "            #there are two seperate loops below:\n",
    "            if date_yr == first_er_date_yr:\n",
    "                # the first loop is to loop through the most recent transcript as the quarter does not end with 4\n",
    "                # as of when this programme is written, the most recent transcript is Q2 of 2022\n",
    "                for j in range(0, first_er_date_qtr):\n",
    "                    q += 1\n",
    "                    #finds the relevant 13 initial dataset as described in page 2 of the Doc\n",
    "                    #it finds them by calling the function find_transcript() located below\n",
    "                    full_transcript, exact_date = find_transcript(ticker, date_yr, date_qtr, q)\n",
    "                    arr_full_transcript = np.append(arr_full_transcript, full_transcript)\n",
    "                    arr_ticker_infos = np.append(arr_ticker_infos, np.array([[ticker, date_yr, date_qtr, exact_date]]), axis=0)\n",
    "                    date_qtr -= 1\n",
    "                    if date_qtr == 0:\n",
    "                        date_qtr = 4\n",
    "            else:\n",
    "                # the second loop loops through the rest of the transcripts\n",
    "                for j in range(0, 4):\n",
    "                    q += 1\n",
    "                    \n",
    "                    full_transcript, exact_date = find_transcript(ticker, date_yr, date_qtr, q)\n",
    "                    arr_full_transcript = np.append(arr_full_transcript, full_transcript)\n",
    "                    arr_ticker_infos = np.append(arr_ticker_infos, np.array([[ticker, date_yr, date_qtr, exact_date]]), axis=0)\n",
    "\n",
    "                    date_qtr -= 1\n",
    "                    if date_qtr == 0:\n",
    "                        date_qtr = 4\n",
    "            date_yr -= 1\n",
    "    except:\n",
    "        arr_full_transcript = np.NaN\n",
    "        arr_ticker_infos = np.NaN\n",
    "        \n",
    "    return arr_full_transcript, arr_ticker_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds transcript and creates the .csv files\n",
    "for sector in sectorlist:\n",
    "    df_stock_list = pd.read_csv('hrefs/ref_'+sector+'.csv')['ticker']\n",
    "    for stock in df_stock_list:\n",
    "        try:\n",
    "            try: \n",
    "                filelist = os.listdir(\"sectors/\"+sector)\n",
    "            except:\n",
    "                filelist = []\n",
    "\n",
    "            if stock not in filelist:\n",
    "                print(stock)\n",
    "\n",
    "                arr_full_transcript, arr_ticker_infos = ticker_transcript(stock)\n",
    "                FOLDERPATH = 'sectors/' + sector + '/' + str(stock)\n",
    "                if not os.path.exists(FOLDERPATH):\n",
    "                    os.makedirs(FOLDERPATH)\n",
    "                for i in range(0,len(arr_ticker_infos)):\n",
    "                    year = arr_ticker_infos[i][1]\n",
    "                    quarter = arr_ticker_infos[i][2]\n",
    "                    release_date = arr_ticker_infos[i][3]\n",
    "                    single_transcript = arr_full_transcript[i]\n",
    "                    df_transcript_details = pd.DataFrame([year, quarter, release_date, single_transcript])\n",
    "                    FILEPATH = 'sectors/' + sector + '/' + str(stock) + '/' + str(stock) + str(year) + str(quarter) + \".csv\"\n",
    "                    df_transcript_details.to_csv(FILEPATH, header=False, index=False)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum=0\n",
    "for sector in sectorlist:\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    sum += len(filelist)\n",
    "    \n",
    "print(sum) #amount of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes all csv files with NaN values as transcript data\n",
    "for sector in sectorlist:\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for stock in filelist:\n",
    "        sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "        for directory_link in sector_files:\n",
    "            try:\n",
    "                df_transcript = pd.read_csv(directory_link)\n",
    "                my_transcript = df_transcript.loc[2]\n",
    "                if str(my_transcript[0]) == \"nan\":\n",
    "                    print(directory_link)\n",
    "                    os.remove(directory_link)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletes stock directories with less than 5 earnings calls\n",
    "for sector in sectorlist:\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for stock in filelist:\n",
    "        sector_files = glob.glob('sectors/'+sector+'/'+str(stock))\n",
    "        for directory_link in sector_files:\n",
    "            if len(os.listdir(directory_link)) < 5:\n",
    "                print(directory_link)\n",
    "                shutil.rmtree(directory_link, ignore_errors=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sectors/automobiles/XPEL/XPEL20151.csv', 'sectors/automobiles/XPEL/XPEL20152.csv', 'sectors/automobiles/XPEL/XPEL20153.csv', 'sectors/automobiles/XPEL/XPEL20154.csv', 'sectors/automobiles/XPEL/XPEL20161.csv', 'sectors/automobiles/XPEL/XPEL20162.csv', 'sectors/automobiles/XPEL/XPEL20163.csv', 'sectors/automobiles/XPEL/XPEL20164.csv', 'sectors/automobiles/XPEL/XPEL20171.csv', 'sectors/automobiles/XPEL/XPEL20172.csv', 'sectors/automobiles/XPEL/XPEL20173.csv', 'sectors/automobiles/XPEL/XPEL20174.csv', 'sectors/automobiles/XPEL/XPEL20181.csv', 'sectors/automobiles/XPEL/XPEL20182.csv', 'sectors/automobiles/XPEL/XPEL20183.csv', 'sectors/automobiles/XPEL/XPEL20184.csv', 'sectors/automobiles/XPEL/XPEL20191.csv', 'sectors/automobiles/XPEL/XPEL20192.csv', 'sectors/automobiles/XPEL/XPEL20193.csv', 'sectors/automobiles/XPEL/XPEL20194.csv', 'sectors/automobiles/XPEL/XPEL20201.csv', 'sectors/automobiles/XPEL/XPEL20202.csv', 'sectors/automobiles/XPEL/XPEL20204.csv', 'sectors/automobiles/XPEL/XPEL20211.csv', 'sectors/automobiles/XPEL/XPEL20212.csv', 'sectors/automobiles/XPEL/XPEL20213.csv', 'sectors/automobiles/XPEL/XPEL20214.csv', 'sectors/automobiles/XPEL/XPEL20221.csv', 'sectors/automobiles/XPEL/XPEL20222.csv', 'sectors/automobiles/XPEL/XPEL20223.csv']\n"
     ]
    }
   ],
   "source": [
    "# sorts directory into a list\n",
    "sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "for sector in sectorlist:\n",
    "    \n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for stock in filelist:\n",
    "        all_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "        all_files.sort(reverse=True)\n",
    "        print(all_files)\n",
    "\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                2015\n",
      "0                                                  1\n",
      "1                                2015-03-31 17:00:00\n",
      "2  Operator: Greetings, and welcome to the XPEL T...\n"
     ]
    }
   ],
   "source": [
    "url = pd.read_csv(\"sectors/automobiles/XPEL/XPEL20151.csv\")\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare earnings transcript sentiments between companies in the same sector and see variances, then rank them\n",
    "\n",
    "https://www.benzinga.com/quote/AAPL\n",
    "\n",
    "\n",
    "how much it missed/beats earnings\n",
    "\n",
    "compare analyst questions with responce\n",
    "\n",
    "have a list of all operator/speaker names to delete\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
