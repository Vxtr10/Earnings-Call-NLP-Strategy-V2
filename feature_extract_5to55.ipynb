{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(path):\n",
    "    mytranscript = pd.read_csv(path).iloc[[2]].values[0][0] \n",
    "    mytranscript = re.sub(r'[^A-Za-z0-9.,:!\\'\\n ]', '', mytranscript)\n",
    "    mytranscript = mytranscript.replace(\".\", \". \")\n",
    "    mytranscript = re.sub('[^\\S\\n]+', ' ', mytranscript) #replaces multiple spaces to single space, without deleting newlines \\n in the process\n",
    "    mytranscript = mytranscript.splitlines() # finds transcript\n",
    "    return mytranscript\n",
    "\n",
    "def split_transcript(mytranscript):    \n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    for i in range(0, len(mytranscript)):\n",
    "        speech_bubble = mytranscript[i].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space so the IF condition below can run smoothly\n",
    "        # finds the following condition (what operator says) and splits the transcript into 2)\n",
    "        if (i > 1) and ((\"operator:\" in speech_bubble) and ((\"question\" in speech_bubble) or (\"go ahead\" in speech_bubble) or (\"operator instructions\" in speech_bubble))):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "        elif (i > 1 ) and (\"operator\" in speech_bubble) and (\"question\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i+1]\n",
    "            transcript_questions = mytranscript[i+1:]\n",
    "            break\n",
    "        elif (i > 1 ) and (\"operator:\" in speech_bubble) and (\"first\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "\n",
    "    return transcript_safe_harbour, transcript_questions\n",
    "\n",
    "def get_file_speaker_names(sector, stock):\n",
    "    write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "    speaker_names = np.loadtxt(write_path, delimiter='\\t', dtype=str)\n",
    "    return speaker_names\n",
    "    \n",
    "# finds a list of analyst names for a single .csv file\n",
    "def find_analyst_names(speaker_names, transcript_questions):\n",
    "    analyst_names = []\n",
    "    # the programme recognises the question is being asked by an analyst when the following conditions are met:\n",
    "    for index in range(0, len(transcript_questions)-1):\n",
    "        speech_bubble = transcript_questions[index].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "        if (\"operator:\" in speech_bubble) or (\"operator :\" in speech_bubble):\n",
    "            for name in speaker_names:\n",
    "                namelist = name.split()\n",
    "                if (name.lower() != \"operator\") and (\"representative\" not in name.lower()) and (\"corporate\" not in name.lower()) and (\"company\" not in name.lower()):\n",
    "                    for name_2 in namelist: # cycle through each name in the name_list\n",
    "                        name_2 = name_2.lower()\n",
    "                        # checks if the speaker name happens to be in the speech_bubble, if it is, then the person speaking is an analyst\n",
    "                        # also len(name) > 2 is used to avoid the problem with single letters being registered as in the speech_bubble \n",
    "                        # (e.g. the letter \"A\" in the name \"A Gayn Erickson\" will be in the speech_bubble, but Gayn Erickson is not an analyst, so \"A\" is not counted)\n",
    "                        if (((\" \"+name_2+\" \" in speech_bubble) and len(name_2) > 2) and ((\"end\" not in speech_bubble) and (\"closing\" not in speech_bubble) and ((\"turn\" not in speech_bubble) or ((\"over\" not in speech_bubble))))) and (name_2 in transcript_questions[index+1].lower()):\n",
    "                            analyst_names.append(name)\n",
    "                    if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "                        analyst_names.append(name) \n",
    "                        \n",
    "    analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "    return analyst_names\n",
    "\n",
    "def get_analyst_management_sentences(analyst_names, transcript_questions):\n",
    "    analyst_sentences = []\n",
    "    management_sentences = []\n",
    "\n",
    "    # get analyst sentence\n",
    "    for index in range(0, len(transcript_questions)-1):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "\n",
    "        #finds the name of the speaker\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        if index > 3:\n",
    "            for name in analyst_names:\n",
    "                namelist = name.split()\n",
    "                if (speech_bubble not in analyst_sentences):\n",
    "                    # checks if the name of the current speaker is in the \"analyst_names\" list, if it is, then this speaker is considered as an analyst\n",
    "                    if speaker_name in analyst_names:\n",
    "                        analyst_sentences.append(speech_bubble)\n",
    "\n",
    "                    elif (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "                        pass\n",
    "\n",
    "                    elif ((namelist[0] in speaker_name) or (namelist[-1] in speaker_name)) and ((\"operator:\" in transcript_questions[index-1].lower()) or (\"operator :\" in transcript_questions[index-1].lower())):\n",
    "                        # in the case where:\n",
    "                        # Operator: [Operator Instructions]Our first question is from Jeffrey Van Sinderen with B. Riley FBR. Please proceed.\n",
    "                        # JeffreySinderen: Good morning, everyone. Can you speak a little bit more about the sales progression you've seen in China...\n",
    "                        # Jeffrey Van Sindere is an analyst, but is referenced as JeffreySinderen in the text, the previous find_analyst_names() function did not pick up this\n",
    "                        # However, now the name \"JeffreySinderen\" is registered as an analyst name through this function.\n",
    "                        analyst_names.append(speaker_name)\n",
    "                        analyst_sentences.append(speech_bubble)\n",
    "\n",
    "    # get management sentence\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        \n",
    "        # dont want operator's sentence \n",
    "        if (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "            pass\n",
    "        \n",
    "        elif (speech_bubble not in analyst_sentences):\n",
    "            management_sentences.append(speech_bubble)\n",
    "    return analyst_sentences, management_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is is string is is string'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "        self.fail = None\n",
    "        self.word = None\n",
    "\n",
    "class AhoCorasick:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        self.build_trie(words)\n",
    "        self.build_ac_automata()\n",
    "\n",
    "    def build_trie(self, words):\n",
    "        for word in words:\n",
    "            node = self.root\n",
    "            for char in word:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_word = True\n",
    "            node.word = word\n",
    "\n",
    "    def build_ac_automata(self):\n",
    "        queue = []\n",
    "\n",
    "        for node in self.root.children.values():\n",
    "            queue.append(node)\n",
    "            node.fail = self.root\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            for char, child in node.children.items():\n",
    "                queue.append(child)\n",
    "                fail_node = node.fail\n",
    "                while fail_node is not None and char not in fail_node.children:\n",
    "                    fail_node = fail_node.fail\n",
    "                if fail_node is None:\n",
    "                    child.fail = self.root\n",
    "                else:\n",
    "                    child.fail = fail_node.children[char]\n",
    "                child.is_word |= child.fail.is_word\n",
    "\n",
    "    def remove_words(self, text):\n",
    "        node = self.root\n",
    "        new_text = text\n",
    "        for i, char in enumerate(text):\n",
    "            while node is not None and char not in node.children:\n",
    "                node = node.fail\n",
    "            if node is None:\n",
    "                node = self.root\n",
    "                continue\n",
    "            node = node.children[char]\n",
    "            if node.is_word:\n",
    "                new_text = new_text.replace(node.word, '')\n",
    "        return new_text\n",
    "\n",
    "\n",
    "# Usage\n",
    "list1 = ['string1', 'string2', 'string3']\n",
    "sentence = \"string1 is string2 is string is string3 is string\"\n",
    "ac = AhoCorasick(list1)\n",
    "new_sentence = ac.remove_words(sentence) # deletes a particular string from new_sentence if that string is presnet in list1\n",
    "new_sentence = re.sub('[^\\S\\n]+', ' ', new_sentence)\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"protobuf<=3.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textstat\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = \"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation. And it's and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\"\n",
    "result = sentiment_nlp(mystr)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def map_sentiments(sentiment_result):\n",
    "    sentiment_result = sentiment_result[0]\n",
    "    if sentiment_result['label'] == 'Negative':\n",
    "        return -1 * sentiment_result['score'], \"negative\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Neutral':\n",
    "        return 0, \"neutral\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Positive':\n",
    "        return sentiment_result['score'], \"positive\"\n",
    "\n",
    "\n",
    "def split_paragraph_into_sentences(temp):\n",
    "    sentences = nltk.sent_tokenize(temp)\n",
    "    return sentences\n",
    "\n",
    "def get_NLP_values(liststr):\n",
    "    # further analysis includes finding sentiment and word complexity.\n",
    "    if len(liststr) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        # maps sentiment data so it outputs a single sentiment value\n",
    "        sentiment_result = sentiment_nlp(liststr)\n",
    "        # gets \n",
    "        sentiment_score = map_sentiments(sentiment_result)\n",
    "\n",
    "        # word complexity:\n",
    "        flesch_score = textstat.flesch_reading_ease(liststr)\n",
    "        gunning_fog_score = textstat.gunning_fog(liststr)\n",
    "\n",
    "        return sentiment_score, flesch_score, gunning_fog_score\n",
    "        \n",
    "# FLS classification\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "\n",
    "def getFeature5to22(pre_release, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    net_sentiment_list = []\n",
    "    flesch_list = []\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    feature_extract_5 = 0\n",
    "    feature_extract_6 = 0\n",
    "    feature_extract_7 = 0\n",
    "    feature_extract_8 = 0\n",
    "    feature_extract_9 = 0\n",
    "    feature_extract_10 = 0\n",
    "    feature_extract_11 = 0\n",
    "    feature_extract_12 = 0\n",
    "    feature_extract_13 = 0\n",
    "    feature_extract_14 = 0\n",
    "    feature_extract_15 = 0\n",
    "    feature_extract_16 = 0\n",
    "    feature_extract_17 = 0\n",
    "    feature_extract_18 = 0\n",
    "    feature_extract_19 = 0\n",
    "    feature_extract_20 = 0\n",
    "    feature_extract_21 = 0\n",
    "    feature_extract_22 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in pre_release:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                flesch_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence) \n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, fls1_sentiment_list, net1_positive, net1_negative, net1_neutral = get_fls_features(s_flslist)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, fls2_sentiment_list, net2_positive, net2_negative, net2_neutral = get_fls_features(ns_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, fls3_sentiment_list, net3_positive, net3_negative, net3_neutral = get_fls_features(n_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            numb_s_flslist = len(s_flslist)\n",
    "            numb_ns_flslist = len(ns_flslist)\n",
    "            numb_n_flslist = len(n_flslist)\n",
    "            total = numb_s_flslist + numb_ns_flslist + numb_n_flslist\n",
    "\n",
    "            feature_extract_21 = numb_s_flslist/total\n",
    "            feature_extract_22 = numb_ns_flslist/total\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            net_positive = net1_positive + net2_positive + net3_positive\n",
    "            net_negative = net1_negative + net2_negative + net3_negative\n",
    "            net_neutral = net1_neutral + net2_neutral + net3_neutral\n",
    "            net_sentiment_list = fls1_sentiment_list + fls2_sentiment_list + fls3_sentiment_list\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_5 = statistics.mean(net_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_6 = net_positive/(net_negative+net_positive+net_neutral)\n",
    "            feature_extract_7 = net_negative/(net_negative+net_positive+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_8 = statistics.mean(flesch_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fea_ext_list5to22 = [feature_extract_5, feature_extract_6, feature_extract_7, feature_extract_8, feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, feature_extract_21, feature_extract_22]\n",
    "\n",
    "    except:\n",
    "        fea_ext_list5to22 = [0]*18\n",
    "    \n",
    "    return fea_ext_list5to22\n",
    "\n",
    "\n",
    "def get_fls_features(flslist):\n",
    "    fls_sentiment_list = []\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "    \n",
    "    feature_extract_1 = 0\n",
    "    feature_extract_2 = 0\n",
    "    feature_extract_3 = 0\n",
    "    feature_extract_4 = 0\n",
    "\n",
    "    for each_fls in flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        fls_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "    try:\n",
    "        feature_extract_1 = statistics.mean(fls_sentiment_list)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_2 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_3 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_4 = textstat.flesch_reading_ease(' '.join(flslist))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return feature_extract_1, feature_extract_2, feature_extract_3, feature_extract_4, fls_sentiment_list, net_positive, net_negative, net_neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment \"net_sentiment_list\"\n",
    "# 24. Whole Q&A – #of_NETpositive(sentiment)/#of_NETnegative+NETpositive+NETneutral(sentiment)\n",
    "# 25. Whole Q&A – #of_NETnegative(sentiment)/#of_NETnegative+NETpositive+NETneutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity \"net_text_complex_list\"\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment \"question_sentiment_list\"\n",
    "# 28. all question (aggregate) – #of_Qpositive(sentiment)/#of_Qnegative+Qpositive+Qneutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_Qnegative(sentiment)/#of_Qnegative+Qpositive+Qneutral(sentiment)\n",
    "# 30. all question (aggregate) - net word complexity \"questions_complex_list\"\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment \"reply_sentiment_list\"\n",
    "# 32. all reply (aggregate) – #of_Rpositive(sentiment)/#of_Rnegative+Rpositive+Rneutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_Rnegative(sentiment)/#of_Rnegative+Rpositive+Rneutral(sentiment)\n",
    "# 34. all reply (aggregate) - net word complexity \"reply_complex_list\"\n",
    "\n",
    "\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_SFLSpositive(sentiment)/#of_SFLSnegative+SFLSpositive+SFLSneutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_SFLSnegative(sentiment)/#of_SFLSnegative+SFLSpositive+SFLSneutral(sentiment)\n",
    "#\n",
    "# 38. Non Specific Forward looking statement - sentiment \n",
    "# 39. Non Specific Forward looking statement - #of_NSFLSpositive(sentiment)/#of_NSFLSnegative+NSFLSpositive+NSFLSneutral(sentiment)\n",
    "# 40. Non Specific Forward looking statement - #of_NSFLSnegative(sentiment)/#of_NSFLSnegative+NSFLSpositive+NSFLSneutral(sentiment)\n",
    "#\n",
    "# 41. Not Foward looking statement - sentiment\n",
    "# 42. Not Foward looking statement - #of_NFLSpositive(sentiment)/#of_NFLSnegative+NFLSpositive+NFLSneutral(sentiment)\n",
    "# 43. Not Foward looking statement - #of_NFLSnegative(sentiment)/#of_NFLSnegative+NFLSpositive+NFLSneutral(sentiment)\n",
    "\n",
    "\n",
    "# 1. get rid of the speaker names from each \"speech bubble\" (i.e. analyst_speech, management_speech)\n",
    "\n",
    "# 2. FOR loop of each speech bubble\n",
    "    # 2a. Parse them to sentences:\n",
    "    # 2b. For each in sentences:\n",
    "        #- classify the sentences (S-FLS, NS-FLS, N-FLS)\n",
    "        #- Creates x3 FLS lists \n",
    "    # 2c. finds text complexity of each speech bubble\n",
    "    # 2d. adds text complexity value of each speech buble to net_text_complex_list \n",
    "\n",
    "# 3. Goes through each FLS list and find sentiment of each with the following conditions:\n",
    "    # 3a. if sentence is s_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- SFLSnet_positive, SFLSnet_negative, or SFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "\n",
    "    # 3b. if sentence is ns_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- NSFLSnet_positive, NSFLSnet_negative, or NSFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"ns_fls_sentiment_list\"\n",
    "\n",
    "    # 3c. if sentence is n_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- NFLSnet_positive, NFLSnet_negative, or NFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"n_fls_sentiment_list\"\n",
    "\n",
    "# 4. finds text complexity of s_FLS, ns_FLS, n_FLS\n",
    "\n",
    "# 5. get features based on the mean of relevant lists\n",
    "\n",
    "def getFeature23to43(analyst_speech, management_speech, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    questions_complex_list = []\n",
    "    reply_complex_list = []\n",
    "    net_text_complex_list  = []\n",
    "\n",
    "    # list of sentiments for all S_FLS, N_FLS, NS_FLS classes\n",
    "    s_fls_sentiment_list = []\n",
    "    n_fls_sentiment_list = []\n",
    "    ns_fls_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"question\"\n",
    "    question_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"reply\"\n",
    "    reply_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences in the Q&A section\n",
    "    net_sentiment_list = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    Qpositive = 0\n",
    "    Qnegative = 0\n",
    "    Qneutral = 0\n",
    "\n",
    "    Rpositive = 0\n",
    "    Rnegative = 0\n",
    "    Rneutral = 0\n",
    "\n",
    "    SFLSpositive = 0\n",
    "    SFLSnegative = 0\n",
    "    SFLSneutral = 0\n",
    "\n",
    "    NSFLSpositive = 0\n",
    "    NSFLSnegative = 0\n",
    "    NSFLSneutral = 0\n",
    "\n",
    "    NFLSpositive = 0\n",
    "    NFLSnegative = 0\n",
    "    NFLSneutral = 0 \n",
    "\n",
    "    feature_extract_23 = 0\n",
    "    feature_extract_24 = 0\n",
    "    feature_extract_25 = 0\n",
    "    feature_extract_26 = 0\n",
    "\n",
    "    feature_extract_27 = 0\n",
    "    feature_extract_28 = 0\n",
    "    feature_extract_29 = 0\n",
    "    feature_extract_30 = 0\n",
    "\n",
    "    feature_extract_31 = 0\n",
    "    feature_extract_32 = 0\n",
    "    feature_extract_33 = 0\n",
    "    feature_extract_34 = 0\n",
    "\n",
    "    feature_extract_35 = 0\n",
    "    feature_extract_36 = 0\n",
    "    feature_extract_37 = 0\n",
    "    \n",
    "    feature_extract_38 = 0\n",
    "    feature_extract_39 = 0\n",
    "    feature_extract_40 = 0\n",
    "    \n",
    "    feature_extract_41 = 0\n",
    "    feature_extract_42 = 0\n",
    "    feature_extract_43 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in analyst_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                questions_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    sentiment_result = sentiment_nlp(sentence)\n",
    "                    sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "                    question_sentiment_list.append(sentiment_score)\n",
    "                    net_sentiment_list.append(sentiment_score)\n",
    "\n",
    "                    if positivity_value == \"positive\":\n",
    "                        net_positive += 1\n",
    "                        Qpositive += 1\n",
    "\n",
    "                    elif positivity_value == \"negative\":\n",
    "                        net_negative += 1\n",
    "                        Qnegative += 1\n",
    "\n",
    "                    else:\n",
    "                        net_neutral += 1\n",
    "                        Qneutral += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for speech_bubble in management_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                reply_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # for \"n_flslist\":\n",
    "        n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, n_flslist, n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"s_flslist\":\n",
    "        s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, s_flslist, s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"ns_flslist\":\n",
    "        ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, ns_flslist, ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral)\n",
    "\n",
    "        feature_extract_23 = statistics.mean(net_sentiment_list)\n",
    "\n",
    "        try:\n",
    "            feature_extract_24 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "            feature_extract_25 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_26 = statistics.mean(net_text_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_27 = statistics.mean(question_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_28 = Qpositive/(Qpositive+Qnegative+Qneutral)\n",
    "            feature_extract_29 = Qnegative/(Qpositive+Qnegative+Qneutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_30 = statistics.mean(questions_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_31 = statistics.mean(reply_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_32 = Rpositive/(Rpositive+Rnegative+Rneutral)\n",
    "            feature_extract_33 = Rnegative/(Rpositive+Rnegative+Rneutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_34 = statistics.mean(reply_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_35 = statistics.mean(s_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_36 = SFLSpositive/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "            feature_extract_37 = SFLSnegative/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            feature_extract_38 = statistics.mean(ns_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_39 = NSFLSpositive/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "            feature_extract_40 = NSFLSnegative/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            feature_extract_41 = statistics.mean(n_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_42 = NFLSpositive/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "            feature_extract_43 = NFLSnegative/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return [feature_extract_23, feature_extract_24, feature_extract_25, feature_extract_26, feature_extract_27, feature_extract_28, feature_extract_29, feature_extract_30, feature_extract_31, feature_extract_32, feature_extract_33, feature_extract_34, feature_extract_35, feature_extract_36, feature_extract_37, feature_extract_38, feature_extract_39, feature_extract_40, feature_extract_41, feature_extract_42, feature_extract_43]\n",
    "\n",
    "def get_SentimentLists_from_FLS(net_sentiment_list, THIS_flslist, THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral):\n",
    "    for each_fls_sentence in THIS_flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls_sentence)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        THISfls_sentiment_list.append(sentiment_score)\n",
    "        reply_sentiment_list.append(sentiment_score)\n",
    "        net_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "            FLSpositive += 1\n",
    "            Rpositive += 1\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "            FLSnegative += 1\n",
    "            Rnegative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "            FLSneutral += 1\n",
    "            Rneutral += 1\n",
    "\n",
    "    return THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 44: \"margin\" - average sentiment\n",
    "# 45: \"margin\" - pos/total sentiment\n",
    "# 46: \"margin\" - neg/total sentiment\n",
    "\n",
    "# 47: \"cost\" - average sentiment\n",
    "# 48: \"cost\" - pos/total sentiment\n",
    "# 49: \"cost\" - neg/total sentiment\n",
    "\n",
    "# 50: \"revenue\" - average sentiment\n",
    "# 51: \"revenue\" - pos/total sentiment\n",
    "# 52: \"revenue\" - neg/total sentiment\n",
    "\n",
    "# 53: \"earnings or EBIDTA\" - average sentiment\n",
    "# 54: \"earnings or EBIDTA\" - pos/total sentiment\n",
    "# 55: \"earnings or EBIDTA\" - neg/total sentiment\n",
    "\n",
    "# 56: \"growth\" - average sentiment\n",
    "# 57: \"growth\" - pos/total sentiment\n",
    "# 58: \"growth\" - neg/total sentiment\n",
    "\n",
    "# 59: \"leverage or debt\" -  average sentiment\n",
    "# 60: \"leverage or debt\" -  pos/total sentiment\n",
    "# 61: \"leverage or debt\" -  neg/total sentiment\n",
    "\n",
    "# 62: \"industry or sector\" – average sentiment\n",
    "# 63: \"industry or sector\" – pos/total sentiment\n",
    "# 64: \"industry or sector\" – neg/total sentiment\n",
    "\n",
    "# 65: \"operation\" - average sentiment \n",
    "# 66: \"operation\" - pos/total sentiment\n",
    "# 67: \"operation\" - neg/total sentiment\n",
    "\n",
    "# 68: \"cashflow\" - average sentiment \n",
    "# 69: \"cashflow\" - pos/total sentiment\n",
    "# 70: \"cashflow\" - neg/total sentiment\n",
    "\n",
    "# 71: \"dividend/share buyback\" - average sentiment \n",
    "# 72: \"dividend/share buyback\" - pos/total sentiment\n",
    "# 73: \"dividend/share buyback\" - neg/total sentiment\n",
    "\n",
    "\n",
    "def deepCleanTranscript(mytranscript, speaker_names):\n",
    "    updatedTranscript = ' '.join(mytranscript)\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    updatedTranscript = ac.remove_words(updatedTranscript)\n",
    " \n",
    "    if updatedTranscript[0] == \" \": \n",
    "        updatedTranscript = updatedTranscript.replace(\" \", \"\", 1)\n",
    "\n",
    "    updatedTranscript = re.sub('[^\\S\\n]+', ' ', updatedTranscript)\n",
    "    updatedTranscript.lower()\n",
    "\n",
    "    return updatedTranscript\n",
    "\n",
    "def getFeature44to73(mytranscript, speaker_names):\n",
    "    marginSentimentList = []\n",
    "    mar_positive = 0\n",
    "    mar_negative = 0\n",
    "    mar_neutral = 0\n",
    "\n",
    "    costSentimentList = []\n",
    "    cost_positive = 0\n",
    "    cost_negative = 0\n",
    "    cost_neutral = 0\n",
    "\n",
    "    revenueSentimentList = []\n",
    "    rev_positive = 0\n",
    "    rev_negative = 0\n",
    "    rev_neutral = 0\n",
    "\n",
    "    earningsEBIDTASentimentList = []\n",
    "    ear_positive = 0\n",
    "    ear_negative = 0\n",
    "    ear_neutral = 0\n",
    "    \n",
    "    growthSentimentList = []\n",
    "    gro_positive = 0\n",
    "    gro_negative = 0\n",
    "    gro_neutral = 0\n",
    "\n",
    "    leverageDebtSentimentList = []\n",
    "    lev_positive = 0\n",
    "    lev_negative = 0\n",
    "    lev_neutral = 0\n",
    "\n",
    "    IndSentimentList = []\n",
    "    ind_positive = 0\n",
    "    ind_negative = 0\n",
    "    ind_neutral = 0\n",
    "\n",
    "    operationSentimentList = []\n",
    "    ope_positive = 0\n",
    "    ope_negative = 0\n",
    "    ope_neutral = 0\n",
    "\n",
    "    cashflowSentimentList = []\n",
    "    cash_positive = 0\n",
    "    cash_negative = 0\n",
    "    cash_neutral = 0\n",
    "\n",
    "    dividendSentimentList = []\n",
    "    div_positive = 0\n",
    "    div_negative = 0\n",
    "    div_neutral = 0\n",
    "\n",
    "    feature_extract_44 = 0\n",
    "    feature_extract_45 = 0\n",
    "    feature_extract_46 = 0\n",
    "    feature_extract_47 = 0\n",
    "    feature_extract_48 = 0\n",
    "    feature_extract_49 = 0\n",
    "    feature_extract_50 = 0\n",
    "    feature_extract_51 = 0\n",
    "    feature_extract_52 = 0\n",
    "    feature_extract_53 = 0\n",
    "    feature_extract_54 = 0\n",
    "    feature_extract_55 = 0\n",
    "    feature_extract_56 = 0\n",
    "    feature_extract_57 = 0\n",
    "    feature_extract_58 = 0\n",
    "    feature_extract_59 = 0\n",
    "    feature_extract_60 = 0\n",
    "    feautre_extract_61 = 0\n",
    "    feature_extract_62 = 0\n",
    "    feature_extract_63 = 0\n",
    "    feature_extract_64 = 0\n",
    "    feature_extract_65 = 0\n",
    "    feature_extract_66 = 0\n",
    "    feature_extract_67 = 0\n",
    "    feature_extract_68 = 0\n",
    "    feature_extract_69 = 0 \n",
    "    feature_extract_70 = 0\n",
    "    feature_extract_71 = 0\n",
    "    feature_extract_72 = 0\n",
    "    feature_extract_73 = 0\n",
    "\n",
    "    updatedTranscript = deepCleanTranscript(mytranscript, speaker_names)\n",
    "    \n",
    "    updatedTranscriptList = split_paragraph_into_sentences(updatedTranscript)\n",
    "\n",
    "    for mysentence in updatedTranscriptList:\n",
    "        if (\" margin\" in mysentence) or (\" return\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            marginSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                mar_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                mar_negative += 1\n",
    "\n",
    "            else:\n",
    "                mar_neutral += 1\n",
    "            \n",
    "\n",
    "        if \" cost\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            costSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                cost_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                cost_negative += 1\n",
    "\n",
    "            else:\n",
    "                cost_neutral += 1\n",
    "\n",
    "        if (\" revenue\" in mysentence) or (\" top line\" in mysentence) or (\" sales\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            revenueSentimentList.append(sentiment_score)\n",
    "\n",
    "            if positivity_value == \"positive\":\n",
    "                rev_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                rev_negative += 1\n",
    "\n",
    "            else:\n",
    "                rev_neutral += 1\n",
    "\n",
    "        if (\" earning\" in mysentence) or (\" EBIT\" in mysentence) or (\" profit\" in mysentence) or (\" bottom line\" in mysentence) or (\" net income\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            earningsEBIDTASentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ear_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ear_negative += 1\n",
    "\n",
    "            else:\n",
    "                ear_neutral += 1\n",
    "\n",
    "        if (\" growth\" in mysentence) or (\" organic\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            growthSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                gro_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                gro_negative += 1\n",
    "\n",
    "            else:\n",
    "                gro_neutral += 1\n",
    "\n",
    "        if (\" leverage\" in mysentence) or (\" debt\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            leverageDebtSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                lev_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                lev_negative += 1\n",
    "\n",
    "            else:\n",
    "                lev_neutral += 1\n",
    "\n",
    "        if (\" industry\" in mysentence) or (\" industr\" in mysentence) or (\" economy\" in mysentence) or (\" economi”\" in mysentence) or (\" sector\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            IndSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ind_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ind_negative += 1\n",
    "\n",
    "            else:\n",
    "                ind_neutral += 1\n",
    "\n",
    "        if \" operation\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            operationSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ope_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ope_negative += 1\n",
    "\n",
    "            else:\n",
    "                ope_neutral += 1\n",
    "        \n",
    "        if (\" cashflow\" in mysentence) or (\" cash flow\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            cashflowSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                cash_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                cash_negative += 1\n",
    "\n",
    "            else:\n",
    "                cash_neutral += 1\n",
    "\n",
    "        if (\" dividend\" in mysentence) or (\" buyback\" in mysentence) or (\" repurchase\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            dividendSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                div_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                div_negative += 1\n",
    "\n",
    "            else:\n",
    "                div_neutral += 1\n",
    "\n",
    "    try:\n",
    "        feature_extract_44 = statistics.mean(marginSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_45 = mar_positive/(mar_positive+mar_negative+mar_neutral)\n",
    "        feature_extract_46 = mar_negative/(mar_positive+mar_negative+mar_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_47 = statistics.mean(costSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_48 = cost_positive/(cost_positive+cost_negative+cost_neutral)\n",
    "        feature_extract_49 = cost_negative/(cost_positive+cost_negative+cost_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_50 = statistics.mean(revenueSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_51 = rev_positive/(rev_positive+rev_negative+rev_neutral)\n",
    "        feature_extract_52 = rev_negative/(rev_positive+rev_negative+rev_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_53 = statistics.mean(earningsEBIDTASentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_54 = ear_positive/(ear_positive+ear_negative+ear_neutral)\n",
    "        feature_extract_55 = ear_negative/(ear_positive+ear_negative+ear_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_56 = statistics.mean(growthSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_57 = gro_positive/(gro_positive+gro_negative+gro_neutral)\n",
    "        feature_extract_58 = gro_negative/(gro_positive+gro_negative+gro_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_59 = statistics.mean(leverageDebtSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_60 = lev_positive/(lev_positive+lev_negative+lev_neutral)\n",
    "        feature_extract_61 = lev_negative/(lev_positive+lev_negative+lev_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_62 = statistics.mean(IndSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_63 = ind_positive/(ind_positive+ind_negative+ind_neutral)\n",
    "        feature_extract_64 = ind_negative/(ind_positive+ind_negative+ind_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_65 = statistics.mean(operationSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_66 = ope_positive/(ope_positive+ope_negative+ope_neutral)\n",
    "        feature_extract_67 = ope_negative/(ope_positive+ope_negative+ope_neutral)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_68 = statistics.mean(cashflowSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_69 = cash_positive/(cash_positive+cash_negative+cash_neutral)\n",
    "        feature_extract_70 = cash_negative/(cash_positive+cash_negative+cash_neutral)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_71 = statistics.mean(dividendSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_72 = div_positive/(div_positive+div_negative+div_neutral)\n",
    "        feature_extract_73 = div_negative/(div_positive+div_negative+div_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return [feature_extract_44, feature_extract_45, feature_extract_46, feature_extract_47, feature_extract_48, feature_extract_49, feature_extract_50, feature_extract_51, feature_extract_52, feature_extract_53, feature_extract_54, feature_extract_55, feature_extract_56, feature_extract_57, feature_extract_58, feature_extract_59, feature_extract_60, feautre_extract_61, feature_extract_62, feature_extract_63, feature_extract_64, feature_extract_65, feature_extract_66, feature_extract_67, feature_extract_68, feature_extract_69, feature_extract_70, feature_extract_71, feature_extract_72, feature_extract_73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing TFIDF\n",
    "\n",
    "sector = \"tech\"\n",
    "stock = \"AAPL\"\n",
    "\n",
    "rolling_path_of_four_transcript = [\n",
    "    'sectors/tech/AAPL/AAPL20224.csv', \n",
    "    'sectors/tech/AAPL/AAPL20223.csv',\n",
    "    'sectors/tech/AAPL/AAPL20222.csv',\n",
    "    'sectors/tech/AAPL/AAPL20221.csv'\n",
    "    ]\n",
    "\n",
    "WHOLE_rolling_frame_of_four_transcripts = []\n",
    "PRERELEASE_rolling_frame_of_four_transcripts = []\n",
    "MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts = []\n",
    "ANALYST_SENTENCES_rolling_frame_of_four_transcripts = []\n",
    "\n",
    "\n",
    "for path in rolling_path_of_four_transcript:\n",
    "    wholeTranscript = get_transcript(path)\n",
    "    transcript_safe_harbour, transcript_questions = split_transcript(wholeTranscript)\n",
    "    speaker_names = get_file_speaker_names(sector, stock)\n",
    "    analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "\n",
    "    analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "    wholeTranscript = deepCleanTranscript(wholeTranscript, speaker_names)\n",
    "    transcript_safe_harbour = deepCleanTranscript(transcript_safe_harbour, speaker_names)\n",
    "    management_sentences = deepCleanTranscript(management_sentences, speaker_names)\n",
    "    analyst_sentences = deepCleanTranscript(analyst_sentences, speaker_names)\n",
    "\n",
    "    WHOLE_rolling_frame_of_four_transcripts.append(wholeTranscript)\n",
    "    PRERELEASE_rolling_frame_of_four_transcripts.append(transcript_safe_harbour)\n",
    "    MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts.append(management_sentences)\n",
    "    ANALYST_SENTENCES_rolling_frame_of_four_transcripts.append(analyst_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['thanks', 'thank', 'really', 'said', 'say', 'yes', 'no', 've', 'll', 'don']\n",
    "all_stop_words = list(ENGLISH_STOP_WORDS) + custom_stop_words\n",
    "\n",
    "def tf_idf(transcript):\n",
    "    #1. Removes stop words, 2. finds tf.idf value, used as a weight\n",
    "    vectoriser = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=100,\n",
    "        ngram_range=(1, 3), # 1 to trigram as they are all common in finance (i.e. earnings per share, free cash flow etc.)\n",
    "        stop_words=all_stop_words # removes stop words (i.e. irrevelant day to day words)\n",
    "    )\n",
    "    #vectorises tfidf values into a vector\n",
    "    tfidf_vec = vectoriser.fit_transform(transcript)\n",
    "    \n",
    "    feature_names = vectoriser.get_feature_names()\n",
    "    for i, value in enumerate(tfidf_vec[0].toarray()[0]):\n",
    "        if value > 0:\n",
    "            print(f\"{feature_names[i]}:{value}\")\n",
    "\n",
    "    return tfidf_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf_idf(MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = linear_kernel(value) # finds the cosine similarity matrix\n",
    "print(cos) # cosine similarity matrix\n",
    "\n",
    "print(cos[0,1]) #(compares the first transcript with the second transcript)\n",
    "print(cos[0,2]) #(compares the first transcript with the third transcript)\n",
    "print(cos[0,3]) #(compares the first transcript with the fourth transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 74: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 75: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 76: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 77: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 78: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 79: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 80: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 81: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 82: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 83: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 84: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 85: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 4th transcript\n",
    "\n",
    "\n",
    "def find_cosineSimilarity(thistranscript):\n",
    "    tf_idf_value = tf_idf(thistranscript)\n",
    "    cosineMatrix = linear_kernel(tf_idf_value) # finds the cosine similarity matrix\n",
    "\n",
    "    cos1_2 = cosineMatrix[0,1] # (compares the first transcript with the second transcript)\n",
    "    cos1_3 = cosineMatrix[0,2] # (compares the first transcript with the third transcript)\n",
    "    cos1_4 = cosineMatrix[0,3] # (compares the first transcript with the fourth transcript)\n",
    "\n",
    "    return cos1_2, cos1_3, cos1_4\n",
    "\n",
    "def tf_idf(transcript):\n",
    "    custom_stop_words = ['thanks', 'thank', 'really', 'said', 'say', 'yes', 'no', 've', 'll', 'don']\n",
    "    all_stop_words = list(ENGLISH_STOP_WORDS) + custom_stop_words\n",
    "\n",
    "    vectoriser = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=100,\n",
    "        ngram_range=(1, 3), # 1 to trigram as they are all common in finance (i.e. earnings per share, free cash flow etc.)\n",
    "        stop_words=all_stop_words # removes stop words (i.e. irrevelant day to day words)\n",
    "    )\n",
    "\n",
    "    #vectorises tfidf values into a vector\n",
    "    tfidf_vec = vectoriser.fit_transform(transcript)\n",
    "\n",
    "    return tfidf_vec\n",
    "    \n",
    "def getFeature74to85(sector, stock, rolling_path_of_four_transcript):\n",
    "\n",
    "    WHOLE_rolling_frame_of_four_transcripts = []\n",
    "    PRERELEASE_rolling_frame_of_four_transcripts = []\n",
    "    MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts = []\n",
    "    ANALYST_SENTENCES_rolling_frame_of_four_transcripts = []\n",
    "\n",
    "\n",
    "    for path in rolling_path_of_four_transcript:\n",
    "        wholeTranscript = get_transcript(path)\n",
    "        transcript_safe_harbour, transcript_questions = split_transcript(wholeTranscript)\n",
    "        speaker_names = get_file_speaker_names(sector, stock)\n",
    "        analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "        \n",
    "        analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "        wholeTranscript = deepCleanTranscript(wholeTranscript, speaker_names)\n",
    "        transcript_safe_harbour = deepCleanTranscript(transcript_safe_harbour, speaker_names)\n",
    "        management_sentences = deepCleanTranscript(management_sentences, speaker_names)\n",
    "        analyst_sentences = deepCleanTranscript(analyst_sentences, speaker_names)\n",
    "\n",
    "        WHOLE_rolling_frame_of_four_transcripts.append(wholeTranscript)\n",
    "        PRERELEASE_rolling_frame_of_four_transcripts.append(transcript_safe_harbour)\n",
    "        MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts.append(management_sentences)\n",
    "        ANALYST_SENTENCES_rolling_frame_of_four_transcripts.append(analyst_sentences)\n",
    "\n",
    "    feature_extract_74, feature_extract_75, feature_extract_76 = find_cosineSimilarity(WHOLE_rolling_frame_of_four_transcripts)\n",
    "\n",
    "    feature_extract_77, feature_extract_78, feature_extract_79 = find_cosineSimilarity(PRERELEASE_rolling_frame_of_four_transcripts)\n",
    "\n",
    "    feature_extract_80, feature_extract_81, feature_extract_82 = find_cosineSimilarity(MANAGEMENT_SENTENCES_rolling_frame_of_four_transcripts)\n",
    "\n",
    "    feature_extract_83, feature_extract_84, feature_extract_85 = find_cosineSimilarity(ANALYST_SENTENCES_rolling_frame_of_four_transcripts)\n",
    "    \n",
    "    \n",
    "    return [feature_extract_74, feature_extract_75, feature_extract_76, feature_extract_77, feature_extract_78, feature_extract_79, feature_extract_80, feature_extract_81, feature_extract_82, feature_extract_83, feature_extract_84, feature_extract_85]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_ext_list74to85 = getFeature74to85(sector, stock, rolling_path_of_four_transcript)\n",
    "fea_ext_list74to85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "sector_files.sort(reverse=True)\n",
    "\n",
    "for i in range (0, len(sector_files)):\n",
    "    if i < len(sector_files)-3:\n",
    "        rolling_path_of_four_transcript = sector_files[i:i+4]\n",
    "        print(rolling_path_of_four_transcript)\n",
    "        fea_ext_list52to63 = getFeature52to63(sector, stock, rolling_path_of_four_transcript)\n",
    "        print(fea_ext_list52to63)\n",
    "        print(\"________________________________\")\n",
    "    else:\n",
    "        print(sector_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analyst_names(speaker_names, transcript_questions):\n",
    "    analyst_names = []\n",
    "    # the programme recognises the question is being asked by an analyst when the following conditions are met:\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "        if \"operator:\" in speech_bubble:\n",
    "            for name in speaker_names:\n",
    "                namelist = name.split()\n",
    "                if name.lower() != \"operator\": \n",
    "                    for name_2 in namelist: # cycle through each name in the name_list\n",
    "                        name_2 = name_2.lower()\n",
    "                        # checks if the speaker name happens to be in the speech_bubble, if it is, then the person speaking is an analyst\n",
    "                        # also len(name) > 2 is used to avoid the problem with single letters being registered as in the speech_bubble \n",
    "                        # (e.g. the letter \"A\" in the name \"A Gayn Erickson\" will be in the speech_bubble, but Gayn Erickson is not an analyst, so \"A\" is not counted)\n",
    "                        if (((\" \"+name_2+\" \" in speech_bubble) and len(name_2) > 2) and ((\"end\" not in speech_bubble) and (\"closing\" not in speech_bubble) and ((\"turn\" not in speech_bubble) or ((\"over\" not in speech_bubble))))) and (name_2 in transcript_questions[index+1].lower()):\n",
    "                            print(name)\n",
    "                            print(speech_bubble) \n",
    "                            analyst_names.append(name)\n",
    "                    if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "                        analyst_names.append(name) \n",
    "                        \n",
    "    analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "    return analyst_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_transcript(mytranscript):    \n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    for i in range(0, len(mytranscript)):\n",
    "        speech_bubble = mytranscript[i].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space so the IF condition below can run smoothly\n",
    "        # finds the following condition (what operator says) and splits the transcript into 2)\n",
    "        if (i > 0) and ((\"operator:\" in speech_bubble) and ((\"question\" in speech_bubble) or (\"go ahead\" in speech_bubble) or (\"operator instructions\" in speech_bubble))):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "        elif (i > 0 ) and (\"operator\" in speech_bubble) and (\"question\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i+1]\n",
    "            transcript_questions = mytranscript[i+1:]\n",
    "            break\n",
    "        elif (i > 0 ) and (\"operator:\" in speech_bubble) and (\"first\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "\n",
    "        elif (i >0) and (\"first\" in speech_bubble) and (\"question\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "\n",
    "\n",
    "    return transcript_safe_harbour, transcript_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_colon(string):\n",
    "    lines = string.split(\"\\n\")\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if \":\" in line:\n",
    "            name_start3 = line.rfind(\".\", 0, line.index(\":\"))\n",
    "            name_start2 = line.rfind(\"?\", 0, line.index(\":\"))\n",
    "            name_start = max(name_start2, name_start3)\n",
    "            if name_start == -1:\n",
    "                name_start = 0\n",
    "            else:\n",
    "                name_start += 1\n",
    "            name = line[:string.index(\":\")].strip()\n",
    "            result.append(name + \": \" + line[line.index(\":\") + 1:].strip())\n",
    "\n",
    "            print(result)\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "input_string = \"\"\"\n",
    "Operator: Good day, ladies and gentlemen. My name is Sedaris, a Lynn? Lynn Antipas Tyson: Thank you, Sedaris.\n",
    "\"\"\"\n",
    "output_string = split_by_colon(input_string)\n",
    "print(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING DATA FOR FORD, entire transcript is one string rather than being seperated by \\n\n",
    "\n",
    "def split_by_colon(string):\n",
    "    result = []\n",
    "    while \":\" in string:\n",
    "        name_start3 = string.rfind(\".\", 0, string.index(\":\"))\n",
    "        name_start2 = string.rfind(\"?\", 0, string.index(\":\"))\n",
    "        name_start = max(name_start2, name_start3)\n",
    "        if name_start == -1:\n",
    "            name_start = 0\n",
    "        else:\n",
    "            name_start += 1\n",
    "        name = string[name_start:string.index(\":\")].strip()\n",
    "        result.append(name + \": \" + string[string.index(\":\") + 1:].strip())\n",
    "        string = string[string.index(\":\") + 1:]\n",
    "    return (result)\n",
    "\n",
    "sector = \"automobiles\"\n",
    "stock = \"F\"\n",
    "\n",
    "path = 'sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20193.csv'\n",
    "input_string = pd.read_csv(path).iloc[[2]].values[0][0] \n",
    "\n",
    "output_list = split_by_colon(input_string)\n",
    "\n",
    "result = []\n",
    "for i in range(0, len(output_list)-1):\n",
    "    currentstring = output_list[i]\n",
    "    nextstring = output_list[i+1]\n",
    "    '\\n'.join(currentstring)\n",
    "    index = currentstring.find(nextstring)\n",
    "    if index != -1:\n",
    "        currentstring = currentstring[:index]\n",
    "    result.append(currentstring)\n",
    "result.append(nextstring)\n",
    "\n",
    "mystr = \"\\n\".join(result)\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = [row for row in reader]\n",
    "\n",
    "data[3] = [mystr]\n",
    "\n",
    "with open(path, \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analyst_management_sentences(analyst_names, transcript_questions):\n",
    "    analyst_sentences = []\n",
    "    management_sentences = []\n",
    "\n",
    "    # get analyst sentence\n",
    "    for index in range(0, len(transcript_questions)):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "\n",
    "        #finds the name of the speaker\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        for name in analyst_names:\n",
    "            namelist = name.split()\n",
    "            if (speech_bubble not in analyst_sentences):\n",
    "                # checks if the name of the current speaker is in the \"analyst_names\" list, if it is, then this speaker is considered as an analyst\n",
    "                if speaker_name in analyst_names:\n",
    "                    analyst_sentences.append(speech_bubble)\n",
    "\n",
    "                elif (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "                    pass\n",
    "\n",
    "                elif ((namelist[0] in speaker_name) or (namelist[-1] in speaker_name)) and ((\"operator:\" in transcript_questions[index-1].lower()) or (\"operator :\" in transcript_questions[index-1].lower())):\n",
    "                    # in the case where:\n",
    "                    # Operator: [Operator Instructions]Our first question is from Jeffrey Van Sinderen with B. Riley FBR. Please proceed.\n",
    "                    # JeffreySinderen: Good morning, everyone. Can you speak a little bit more about the sales progression you've seen in China...\n",
    "                    # Jeffrey Van Sindere is an analyst, but is referenced as JeffreySinderen in the text, the previous find_analyst_names() function did not pick up this\n",
    "                    # However, now the name \"JeffreySinderen\" is registered as an analyst name through this function.\n",
    "                    analyst_names.append(speaker_name)\n",
    "                    analyst_sentences.append(speech_bubble)\n",
    "\n",
    "    # get management sentence\n",
    "    for index in range(0, len(transcript_questions)):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        \n",
    "        # dont want operator's sentence \n",
    "        if (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "            pass\n",
    "        \n",
    "        elif (speech_bubble not in analyst_sentences):\n",
    "            management_sentences.append(speech_bubble)\n",
    "    return analyst_sentences, management_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"automobiles\"\n",
    "stock = \"NIU\"\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "path = 'sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20222.csv'\n",
    "with open(path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    lengthofList = len(list(reader))\n",
    "wholeTranscript = get_transcript(path)\n",
    "transcript_safe_harbour, transcript_questions = split_transcript(wholeTranscript)\n",
    "speaker_names = get_file_speaker_names(sector, stock)\n",
    "analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "\n",
    "analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "analyst_names\n",
    "# wholeTranscript = deepCleanTranscript(wholeTranscript, speaker_names)\n",
    "# transcript_safe_harbour = deepCleanTranscript(transcript_safe_harbour, speaker_names)\n",
    "# management_sentences = deepCleanTranscript(management_sentences, speaker_names)\n",
    "# analyst_sentences = deepCleanTranscript(analyst_sentences, speaker_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_analyst_names(speaker_names, transcript_questions):\n",
    "#     i = 0\n",
    "#     analyst_names = []\n",
    "#     analyst_names.append(\"A Martin Viecha\")\n",
    "#     analyst_names.append(\"Martin Viecha\")\n",
    "#     # the programme recognises the question is being asked by an analyst when the following conditions are met:\n",
    "#     for index in range(0, len(transcript_questions)-2):\n",
    "#         speech_bubble = transcript_questions[index].lower()\n",
    "#         speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "#         if (\"martin viecha:\" in speech_bubble) or (\"martin viecha :\" in speech_bubble) or (\"operator:\" in speech_bubble) or (\"operator :\" in speech_bubble):\n",
    "#             i+=1\n",
    "#             if i > 3:\n",
    "#                 for name in speaker_names:\n",
    "#                     namelist = name.split()\n",
    "#                     if (name.lower() != \"operator\") and (\"elon\" not in name.lower()) and (\"representative\" not in name.lower()) and (\"corporate\" not in name.lower()) and (\"company\" not in name.lower()):\n",
    "#                         for name_2 in namelist: # cycle through each name in the name_list\n",
    "#                             name_2 = name_2.lower()\n",
    "#                             # checks if the speaker name happens to be in the speech_bubble, if it is, then the person speaking is an analyst\n",
    "#                             # also len(name) > 2 is used to avoid the problem with single letters being registered as in the speech_bubble \n",
    "#                             # (e.g. the letter \"A\" in the name \"A Gayn Erickson\" will be in the speech_bubble, but Gayn Erickson is not an analyst, so \"A\" is not counted)\n",
    "#                             if (((\" \"+name_2+\" \" in speech_bubble) and len(name_2) > 2) and ((\"end\" not in speech_bubble) and (\"closing\" not in speech_bubble) and ((\"turn\" not in speech_bubble) or ((\"over\" not in speech_bubble))))) and (name_2 in transcript_questions[index+1].lower()):\n",
    "#                                 analyst_names.append(name)\n",
    "#                         if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "#                             analyst_names.append(name) \n",
    "                        \n",
    "#     analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "#     return analyst_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "sector_files.sort(reverse=True)\n",
    "\n",
    "for i in range (0, len(sector_files)-4): # for every .csv path of that stock\n",
    "    path = sector_files[i]\n",
    "    print(path)\n",
    "    with open(path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        lengthofList = len(list(reader))\n",
    "    if lengthofList == 5:\n",
    "        mytranscript = get_transcript(path)\n",
    "        transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "        speaker_names = get_file_speaker_names(sector, stock)\n",
    "        analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "        analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "        fea_ext_list5to22 = getFeature5to22(transcript_safe_harbour, speaker_names)\n",
    "\n",
    "        fea_ext_list23to43 = getFeature23to43(analyst_sentences, management_sentences, speaker_names)\n",
    "        fea_ext_list44to73 = getFeature44to73(mytranscript, speaker_names)\n",
    "\n",
    "        if i < len(sector_files)-3:\n",
    "            rolling_path_of_four_transcript = sector_files[i:i+4]\n",
    "            fea_ext_list74to85 = getFeature74to85(sector, stock, rolling_path_of_four_transcript)\n",
    "        else:\n",
    "            fea_ext_list74to85 = [None]*12\n",
    "\n",
    "        stock_return_list = get_stock_returns(path, stock)\n",
    "\n",
    "        list_add_to_csv = fea_ext_list5to22 + fea_ext_list23to43 + fea_ext_list44to73+ fea_ext_list74to85 + stock_return_list\n",
    "\n",
    "    #if there are 5 items in the list\n",
    "        with open(path, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for item in list_add_to_csv:\n",
    "                writer.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 86: Stock price difference between Day 0 (Day earnings call is released) and Day 10\n",
    "# 87: Stock price difference between Day 0 (Day earnings call is released) and Day 30\n",
    "# 88: Stock price difference between Day 0 (Day earnings call is released) and Day 50\n",
    "# 89: Stock price difference between Day 0 (Day earnings call is released) and Day 70\n",
    "# 90: Stock price difference between Day 0 (Day earnings call is released) and Day 90\n",
    "#\n",
    "# 91: Stock price difference between Day 1 (Day earnings call is released) and Day 10\n",
    "# 92: Stock price difference between Day 1 (Day earnings call is released) and Day 30\n",
    "# 93: Stock price difference between Day 1 (Day earnings call is released) and Day 50\n",
    "# 94: Stock price difference between Day 1 (Day earnings call is released) and Day 70\n",
    "# 95: Stock price difference between Day 1 (Day earnings call is released) and Day 90\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def find_percentage_change(hist):\n",
    "    try:\n",
    "        percentage_change = ((hist['Open'][-1])/(hist['Open'][0])-1) # % change in price of day X to day X+Y\n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    return percentage_change\n",
    "\n",
    "def get_stock_returns(path, stock):\n",
    "    date1 = pd.read_csv(path).iloc[[1]].values[0][0] \n",
    "    date1 = datetime.strptime(date1, '%Y-%m-%d %H:%M:%S').date()\n",
    "\n",
    "    yfTicker = yf.Ticker(stock)\n",
    "    TickerHistory = yfTicker.history\n",
    "\n",
    "    hist1 = TickerHistory(start=date1, end=date1 + timedelta(days=10))  # day 0 to day 10\n",
    "    hist2 = TickerHistory(start=date1, end=date1 + timedelta(days=30))   # day 0 to day 30\n",
    "    hist3 = TickerHistory(start=date1, end=date1 + timedelta(days=50))\n",
    "    hist4 = TickerHistory(start=date1, end=date1 + timedelta(days=70))      \n",
    "    hist5 = TickerHistory(start=date1, end=date1 + timedelta(days=90))   \n",
    "\n",
    "    hist6 = TickerHistory(start=date1 + timedelta(days=1), end=date1 + timedelta(days=10))   #day 1 to day 11\n",
    "    hist7 = TickerHistory(start=date1 + timedelta(days=1), end=date1 + timedelta(days=30))   #day 1 to day 31\n",
    "    hist8 = TickerHistory(start=date1 + timedelta(days=1), end=date1 + timedelta(days=50))   \n",
    "    hist9 = TickerHistory(start=date1 + timedelta(days=1), end=date1 + timedelta(days=70))\n",
    "    hist10 = TickerHistory(start=date1 + timedelta(days=1), end=date1 + timedelta(days=90))\n",
    "\n",
    "    percentage_change1 = find_percentage_change(hist1)\n",
    "    percentage_change2 = find_percentage_change(hist2)\n",
    "    percentage_change3 = find_percentage_change(hist3)\n",
    "    percentage_change4 = find_percentage_change(hist4)\n",
    "    percentage_change5 = find_percentage_change(hist5)\n",
    "    percentage_change6 = find_percentage_change(hist6)\n",
    "    percentage_change7 = find_percentage_change(hist7)\n",
    "    percentage_change8 = find_percentage_change(hist8)\n",
    "    percentage_change9 = find_percentage_change(hist9)\n",
    "    percentage_change10 = find_percentage_change(hist10)\n",
    "            \n",
    "    return [percentage_change1, percentage_change2, percentage_change3, percentage_change4, percentage_change5, percentage_change6, percentage_change7, percentage_change8, percentage_change9, percentage_change10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectorlist = [\"banks\", \"consumer-retailing\", \"tech\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\", \"telecom\", \"transportation\", \"utilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banks sector start ................................................................\n",
      "PEBO\n",
      "TFC\n",
      "UMPQ\n",
      "CBU\n",
      "EWBC\n",
      "NBHC\n",
      "HOMB\n",
      "FITB\n",
      "EBC\n",
      "HTLF\n",
      "PB\n",
      "PFC\n",
      "SSB\n"
     ]
    }
   ],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "for sector in sectorlist:\n",
    "    print(sector, \"sector start ................................................................\")\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for stock in filelist:\n",
    "        print(stock)\n",
    "        sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "        sector_files.sort(reverse=True)\n",
    "        if stock not in [\"UWMC\", \"FBC\", \"TBK\", \"BKU\"]:\n",
    "            ticker = yf.Ticker(stock)\n",
    "            market_cap = ticker.fast_info['market_cap']\n",
    "            for i in range (0, len(sector_files)): # for every .csv path of that stock\n",
    "                path = sector_files[i]\n",
    "                with open(path, 'r') as file:\n",
    "                    reader = csv.reader(file)\n",
    "                    lengthofList = len(list(reader))\n",
    "                if lengthofList == 5:\n",
    "                    mytranscript = get_transcript(path)\n",
    "                    transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "                    speaker_names = get_file_speaker_names(sector, stock)\n",
    "                    analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "                            \n",
    "                    analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "                    fea_ext_list5to22 = getFeature5to22(transcript_safe_harbour, speaker_names)\n",
    "\n",
    "                    fea_ext_list23to43 = getFeature23to43(analyst_sentences, management_sentences, speaker_names)\n",
    "\n",
    "                    fea_ext_list44to73 = getFeature44to73(mytranscript, speaker_names)\n",
    "\n",
    "                    if i < len(sector_files)-3:\n",
    "                        rolling_path_of_four_transcript = sector_files[i:i+4]\n",
    "                        fea_ext_list74to85 = getFeature74to85(sector, stock, rolling_path_of_four_transcript)\n",
    "                    else:\n",
    "                        fea_ext_list74to85 = [None]*12\n",
    "\n",
    "                    stock_return_list = get_stock_returns(path, stock)\n",
    "\n",
    "                    list_add_to_csv = fea_ext_list5to22 + fea_ext_list23to43 + fea_ext_list44to73+ fea_ext_list74to85 + stock_return_list + [market_cap]\n",
    "                    \n",
    "\n",
    "                #if there are 5 items in the list\n",
    "                    with open(path, 'a', newline='') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        for item in list_add_to_csv:\n",
    "                            writer.writerow([item])\n",
    "                \n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of .csv file info for each stock:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# META DATA\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 0. Year of transcript release\n",
    "# 1. Quarter of transcript release\n",
    "# 2. Date of transcript release\n",
    "# 3. Earnings Transcript contents\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Feature Extractions:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 4. EPS surprise value\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Transcript Features:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment\n",
    "# 24. Whole Q&A – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 25. Whole Q&A – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment\n",
    "# 28. all question (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 30. all question (aggregate) - word complex\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment\n",
    "# 32. all reply (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 34. all reply (aggregate) - word complex\n",
    "#\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 38. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 39. Non Specific Forward looking statement - sentiment \n",
    "# 40. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 41. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 42. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 43. Not Foward looking statement - sentiment\n",
    "# 44. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 45. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 46. Not Foward looking statement - word complexity\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 44: \"margin\" - average sentiment\n",
    "# 45: \"margin\" - pos/total sentiment\n",
    "# 46: \"margin\" - neg/total sentiment\n",
    "\n",
    "# 47: \"cost\" - average sentiment\n",
    "# 48: \"cost\" - pos/total sentiment\n",
    "# 49: \"cost\" - neg/total sentiment\n",
    "\n",
    "# 50: \"revenue\" - average sentiment\n",
    "# 51: \"revenue\" - pos/total sentiment\n",
    "# 52: \"revenue\" - neg/total sentiment\n",
    "\n",
    "# 53: \"earnings or EBIDTA\" - average sentiment\n",
    "# 54: \"earnings or EBIDTA\" - pos/total sentiment\n",
    "# 55: \"earnings or EBIDTA\" - neg/total sentiment\n",
    "\n",
    "# 56: \"growth\" - average sentiment\n",
    "# 57: \"growth\" - pos/total sentiment\n",
    "# 58: \"growth\" - neg/total sentiment\n",
    "\n",
    "# 59: \"leverage or debt\" -  average sentiment\n",
    "# 60: \"leverage or debt\" -  pos/total sentiment\n",
    "# 61: \"leverage or debt\" -  neg/total sentiment\n",
    "\n",
    "# 62: \"industry or sector\" – average sentiment\n",
    "# 63: \"industry or sector\" – pos/total sentiment\n",
    "# 64: \"industry or sector\" – neg/total sentiment\n",
    "\n",
    "# 65: \"operation\" - average sentiment \n",
    "# 66: \"operation\" - pos/total sentiment\n",
    "# 67: \"operation\" - neg/total sentiment\n",
    "\n",
    "# 68: \"cashflow\" - average sentiment \n",
    "# 69: \"cashflow\" - pos/total sentiment\n",
    "# 70: \"cashflow\" - neg/total sentiment\n",
    "\n",
    "# 71: \"dividend/share buyback\" - average sentiment \n",
    "# 72: \"dividend/share buyback\" - pos/total sentiment\n",
    "# 73: \"dividend/share buyback\" - neg/total sentiment\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 74: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 75: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 76: Rolling frame of Whole Transcripts - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 77: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 78: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 79: Rolling frame of Pre-releases - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 80: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 81: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 82: Rolling frame of Management Sentences (Their Replies) - Cosine of TFIDF between 1st and 4th transcript\n",
    "# 83: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 2nd transcript\n",
    "# 84: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 3rd transcript\n",
    "# 85: Rolling frame of Analyst Sentences (Their Replies) - Cosine of TFIDF between 1st and 4th transcript\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 86: Stock price difference between Day 0 (Day earnings call is released) and Day 10\n",
    "# 87: Stock price difference between Day 0 (Day earnings call is released) and Day 30\n",
    "# 88: Stock price difference between Day 0 (Day earnings call is released) and Day 50\n",
    "# 89: Stock price difference between Day 0 (Day earnings call is released) and Day 70\n",
    "# 90: Stock price difference between Day 0 (Day earnings call is released) and Day 90\n",
    "#\n",
    "# 91: Stock price difference between Day 1 (Day earnings call is released) and Day 10\n",
    "# 92: Stock price difference between Day 1 (Day earnings call is released) and Day 30\n",
    "# 93: Stock price difference between Day 1 (Day earnings call is released) and Day 50\n",
    "# 94: Stock price difference between Day 1 (Day earnings call is released) and Day 70\n",
    "# 95: Stock price difference between Day 1 (Day earnings call is released) and Day 90\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# 96: Market Cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete operator name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.spglobal.com/marketintelligence/en/news-insights/blog/analyzing-sentiment-in-quarterly-earnings-calls-q2-2022\n",
    "\n",
    "\n",
    "# https://www.amenityanalytics.com/case-studies/earnings-call-transcript-analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF ----> from management sentences (Replies + pre-release)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
