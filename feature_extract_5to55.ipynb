{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(path):\n",
    "    mytranscript = pd.read_csv(path).iloc[[2]].values[0][0] \n",
    "    mytranscript = re.sub(r'[^A-Za-z0-9.,:!\\'\\n ]', '', mytranscript)\n",
    "    mytranscript = re.sub('[^\\S\\n]+', ' ', mytranscript) #replaces multiple spaces to single space, without deleting newlines \\n in the process\n",
    "    mytranscript = mytranscript.splitlines() # finds transcript\n",
    "    return mytranscript\n",
    "\n",
    "def split_transcript(mytranscript):    \n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    for i in range(0, len(mytranscript)):\n",
    "        speech_bubble = mytranscript[i].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space so the IF condition below can run smoothly\n",
    "        # finds the following condition (what operator says) and splits the transcript into 2)\n",
    "        if (i > 2) and ((\"operator:\" in speech_bubble) and ((\"question\" in speech_bubble) or (\"go ahead\" in speech_bubble) or (\"operator instructions\" in speech_bubble))):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "    return transcript_safe_harbour, transcript_questions\n",
    "\n",
    "def get_file_speaker_names(sector, stock):\n",
    "    write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "    speaker_names = np.loadtxt(write_path, delimiter='\\t', dtype=str)\n",
    "    return speaker_names\n",
    "    \n",
    "# finds a list of analyst names for a single .csv file\n",
    "def find_analyst_names(speaker_names, transcript_questions):\n",
    "    analyst_names = []\n",
    "    # the programme recognises the question is being asked by an analyst when the following conditions are met:\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "        if \"operator:\" in speech_bubble:\n",
    "            for name in speaker_names:\n",
    "                namelist = name.split()\n",
    "                if name.lower() != \"operator\": \n",
    "                    for name_2 in namelist: # cycle through each name in the name_list\n",
    "                        name_2 = name_2.lower()\n",
    "                        # checks if the speaker name happens to be in the speech_bubble, if it is, then the person speaking is an analyst\n",
    "                        # also len(name) > 2 is used to avoid the problem with single letters being registered as in the speech_bubble \n",
    "                        # (e.g. the letter \"A\" in the name \"A Gayn Erickson\" will be in the speech_bubble, but Gayn Erickson is not an analyst, so \"A\" is not counted)\n",
    "                        if (name_2 in speech_bubble) and len(name_2) > 2:\n",
    "                            analyst_names.append(name)\n",
    "                    if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "                        analyst_names.append(name) \n",
    "                        \n",
    "    analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "    return analyst_names\n",
    "\n",
    "def get_analyst_management_sentences(analyst_names, transcript_questions):\n",
    "    analyst_sentences = []\n",
    "    management_sentences = []\n",
    "\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        if speaker_name in analyst_names:\n",
    "            analyst_sentences.append(speech_bubble)\n",
    "\n",
    "        elif speaker_name.lower() == \"operator\":\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            management_sentences.append(speech_bubble)\n",
    "\n",
    "    return analyst_sentences, management_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is is string is is string'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "        self.fail = None\n",
    "        self.word = None\n",
    "\n",
    "class AhoCorasick:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        self.build_trie(words)\n",
    "        self.build_ac_automata()\n",
    "\n",
    "    def build_trie(self, words):\n",
    "        for word in words:\n",
    "            node = self.root\n",
    "            for char in word:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_word = True\n",
    "            node.word = word\n",
    "\n",
    "    def build_ac_automata(self):\n",
    "        queue = []\n",
    "\n",
    "        for node in self.root.children.values():\n",
    "            queue.append(node)\n",
    "            node.fail = self.root\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            for char, child in node.children.items():\n",
    "                queue.append(child)\n",
    "                fail_node = node.fail\n",
    "                while fail_node is not None and char not in fail_node.children:\n",
    "                    fail_node = fail_node.fail\n",
    "                if fail_node is None:\n",
    "                    child.fail = self.root\n",
    "                else:\n",
    "                    child.fail = fail_node.children[char]\n",
    "                child.is_word |= child.fail.is_word\n",
    "\n",
    "    def remove_words(self, text):\n",
    "        node = self.root\n",
    "        new_text = text\n",
    "        for i, char in enumerate(text):\n",
    "            while node is not None and char not in node.children:\n",
    "                node = node.fail\n",
    "            if node is None:\n",
    "                node = self.root\n",
    "                continue\n",
    "            node = node.children[char]\n",
    "            if node.is_word:\n",
    "                new_text = new_text.replace(node.word, '')\n",
    "        return new_text\n",
    "\n",
    "\n",
    "# Usage\n",
    "list1 = ['string1', 'string2', 'string3']\n",
    "sentence = \"string1 is string2 is string is string3 is string\"\n",
    "ac = AhoCorasick(list1)\n",
    "new_sentence = ac.remove_words(sentence) # deletes a particular string from new_sentence if that string is presnet in list1\n",
    "new_sentence = re.sub('[^\\S\\n]+', ' ', new_sentence)\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf<=3.20.1\n",
      "  Downloading protobuf-3.20.1-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.1 which is incompatible.\n",
      "grpcio-status 1.51.1 requires protobuf>=4.21.6, but you have protobuf 3.20.1 which is incompatible.\n",
      "google-cloud-firestore 2.9.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
      "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"protobuf<=3.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textstat\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.9691150784492493}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation. And it's and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\"\n",
    "result = sentiment_nlp(mystr)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sentiments(sentiment_result):\n",
    "    sentiment_result = sentiment_result[0]\n",
    "    if sentiment_result['label'] == 'Negative':\n",
    "        return -1 * sentiment_result['score'], \"negative\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Neutral':\n",
    "        return 0, \"neutral\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Positive':\n",
    "        return sentiment_result['score'], \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_paragraph_into_sentences(temp):\n",
    "    sentences = nltk.sent_tokenize(temp)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLS classification\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NLP_values(liststr):\n",
    "    # further analysis includes finding sentiment and word complexity.\n",
    "    if len(liststr) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        # maps sentiment data so it outputs a single sentiment value\n",
    "        sentiment_result = sentiment_nlp(liststr)\n",
    "        # gets \n",
    "        sentiment_score = map_sentiments(sentiment_result)\n",
    "\n",
    "        # word complexity:\n",
    "        flesch_score = textstat.flesch_reading_ease(liststr)\n",
    "        gunning_fog_score = textstat.gunning_fog(liststr)\n",
    "\n",
    "        return sentiment_score, flesch_score, gunning_fog_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "\n",
    "def getFeature5to22(pre_release, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    net_sentiment_list = []\n",
    "    flesch_list = []\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    feature_extract_5 = 0\n",
    "    feature_extract_6 = 0\n",
    "    feature_extract_7 = 0\n",
    "    feature_extract_8 = 0\n",
    "    feature_extract_9 = 0\n",
    "    feature_extract_10 = 0\n",
    "    feature_extract_11 = 0\n",
    "    feature_extract_12 = 0\n",
    "    feature_extract_13 = 0\n",
    "    feature_extract_14 = 0\n",
    "    feature_extract_15 = 0\n",
    "    feature_extract_16 = 0\n",
    "    feature_extract_17 = 0\n",
    "    feature_extract_18 = 0\n",
    "    feature_extract_19 = 0\n",
    "    feature_extract_20 = 0\n",
    "    feature_extract_21 = 0\n",
    "    feature_extract_22 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in pre_release:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                flesch_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence) \n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, fls1_sentiment_list, net1_positive, net1_negative, net1_neutral = get_fls_features(s_flslist)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, fls2_sentiment_list, net2_positive, net2_negative, net2_neutral = get_fls_features(ns_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, fls3_sentiment_list, net3_positive, net3_negative, net3_neutral = get_fls_features(n_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            numb_s_flslist = len(s_flslist)\n",
    "            numb_ns_flslist = len(ns_flslist)\n",
    "            numb_n_flslist = len(n_flslist)\n",
    "            total = numb_s_flslist + numb_ns_flslist + numb_n_flslist\n",
    "\n",
    "            feature_extract_21 = numb_s_flslist/total\n",
    "            feature_extract_22 = numb_ns_flslist/total\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        net_positive = net1_positive + net2_positive + net3_positive\n",
    "        net_negative = net1_negative + net2_negative + net3_negative\n",
    "        net_neutral = net1_neutral + net2_neutral + net3_neutral\n",
    "        net_sentiment_list = fls1_sentiment_list + fls2_sentiment_list + fls3_sentiment_list\n",
    "        \n",
    "        try:\n",
    "            feature_extract_5 = statistics.mean(net_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_6 = net_positive/(net_negative+net_positive+net_neutral)\n",
    "            feature_extract_7 = net_negative/(net_negative+net_positive+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_8 = statistics.mean(flesch_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fea_ext_list5to22 = [feature_extract_5, feature_extract_6, feature_extract_7, feature_extract_8, feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, feature_extract_21, feature_extract_22]\n",
    "\n",
    "    except:\n",
    "        return fea_ext_list5to22\n",
    "    \n",
    "    return fea_ext_list5to22\n",
    "\n",
    "\n",
    "def get_fls_features(flslist):\n",
    "    fls_sentiment_list = []\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "    \n",
    "    feature_extract_1 = 0\n",
    "    feature_extract_2 = 0\n",
    "    feature_extract_3 = 0\n",
    "    feature_extract_4 = 0\n",
    "\n",
    "    for each_fls in flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        fls_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "    try:\n",
    "        feature_extract_1 = statistics.mean(fls_sentiment_list)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_2 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_3 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_4 = textstat.flesch_reading_ease(' '.join(flslist))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return feature_extract_1, feature_extract_2, feature_extract_3, feature_extract_4, fls_sentiment_list, net_positive, net_negative, net_neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment \"net_sentiment_list\"\n",
    "# 24. Whole Q&A – #of_NETpositive(sentiment)/#of_NETnegative+NETpositive+NETneutral(sentiment)\n",
    "# 25. Whole Q&A – #of_NETnegative(sentiment)/#of_NETnegative+NETpositive+NETneutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity \"net_text_complex_list\"\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment \"question_sentiment_list\"\n",
    "# 28. all question (aggregate) – #of_Qpositive(sentiment)/#of_Qnegative+Qpositive+Qneutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_Qnegative(sentiment)/#of_Qnegative+Qpositive+Qneutral(sentiment)\n",
    "# 30. all question (aggregate) - net word complexity \"questions_complex_list\"\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment \"reply_sentiment_list\"\n",
    "# 32. all reply (aggregate) – #of_Rpositive(sentiment)/#of_Rnegative+Rpositive+Rneutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_Rnegative(sentiment)/#of_Rnegative+Rpositive+Rneutral(sentiment)\n",
    "# 34. all reply (aggregate) - net word complexity \"reply_complex_list\"\n",
    "\n",
    "\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_SFLSpositive(sentiment)/#of_SFLSnegative+SFLSpositive+SFLSneutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_SFLSnegative(sentiment)/#of_SFLSnegative+SFLSpositive+SFLSneutral(sentiment)\n",
    "#\n",
    "# 38. Non Specific Forward looking statement - sentiment \n",
    "# 39. Non Specific Forward looking statement - #of_NSFLSpositive(sentiment)/#of_NSFLSnegative+NSFLSpositive+NSFLSneutral(sentiment)\n",
    "# 40. Non Specific Forward looking statement - #of_NSFLSnegative(sentiment)/#of_NSFLSnegative+NSFLSpositive+NSFLSneutral(sentiment)\n",
    "#\n",
    "# 41. Not Foward looking statement - sentiment\n",
    "# 42. Not Foward looking statement - #of_NFLSpositive(sentiment)/#of_NFLSnegative+NFLSpositive+NFLSneutral(sentiment)\n",
    "# 43. Not Foward looking statement - #of_NFLSnegative(sentiment)/#of_NFLSnegative+NFLSpositive+NFLSneutral(sentiment)\n",
    "\n",
    "\n",
    "# 1. get rid of the speaker names from each \"speech bubble\" (i.e. analyst_speech, management_speech)\n",
    "\n",
    "# 2. FOR loop of each speech bubble\n",
    "    # 2a. Parse them to sentences:\n",
    "    # 2b. For each in sentences:\n",
    "        #- classify the sentences (S-FLS, NS-FLS, N-FLS)\n",
    "        #- Creates x3 FLS lists \n",
    "    # 2c. finds text complexity of each speech bubble\n",
    "    # 2d. adds text complexity value of each speech buble to net_text_complex_list \n",
    "\n",
    "# 3. Goes through each FLS list and find sentiment of each with the following conditions:\n",
    "    # 3a. if sentence is s_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- SFLSnet_positive, SFLSnet_negative, or SFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"s_fls_sentiment_list\"\n",
    "\n",
    "    # 3b. if sentence is ns_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- NSFLSnet_positive, NSFLSnet_negative, or NSFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"ns_fls_sentiment_list\"\n",
    "\n",
    "    # 3c. if sentence is n_FLS\n",
    "        #- sentiment value is appended to \"reply_sentiment_list\"\n",
    "        #- net_positive, net_negative, or net_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- NFLSnet_positive, NFLSnet_negative, or NFLSnet_neutral += 1 (total number of sentences with positive sentiments)\n",
    "        #- Sentiment value is appended to \"n_fls_sentiment_list\"\n",
    "\n",
    "# 4. finds text complexity of s_FLS, ns_FLS, n_FLS\n",
    "\n",
    "# 5. get features based on the mean of relevant lists\n",
    "\n",
    "def getFeature23to43(analyst_speech, management_speech, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    questions_complex_list = []\n",
    "    reply_complex_list = []\n",
    "    net_text_complex_list  = []\n",
    "\n",
    "    # list of sentiments for all S_FLS, N_FLS, NS_FLS classes\n",
    "    s_fls_sentiment_list = []\n",
    "    n_fls_sentiment_list = []\n",
    "    ns_fls_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"question\"\n",
    "    question_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"reply\"\n",
    "    reply_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences in the Q&A section\n",
    "    net_sentiment_list = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    Qpositive = 0\n",
    "    Qnegative = 0\n",
    "    Qneutral = 0\n",
    "\n",
    "    Rpositive = 0\n",
    "    Rnegative = 0\n",
    "    Rneutral = 0\n",
    "\n",
    "    SFLSpositive = 0\n",
    "    SFLSnegative = 0\n",
    "    SFLSneutral = 0\n",
    "\n",
    "    NSFLSpositive = 0\n",
    "    NSFLSnegative = 0\n",
    "    NSFLSneutral = 0\n",
    "\n",
    "    NFLSpositive = 0\n",
    "    NFLSnegative = 0\n",
    "    NFLSneutral = 0 \n",
    "\n",
    "    feature_extract_23 = 0\n",
    "    feature_extract_24 = 0\n",
    "    feature_extract_25 = 0\n",
    "    feature_extract_26 = 0\n",
    "\n",
    "    feature_extract_27 = 0\n",
    "    feature_extract_28 = 0\n",
    "    feature_extract_29 = 0\n",
    "    feature_extract_30 = 0\n",
    "\n",
    "    feature_extract_31 = 0\n",
    "    feature_extract_32 = 0\n",
    "    feature_extract_33 = 0\n",
    "    feature_extract_34 = 0\n",
    "\n",
    "    feature_extract_35 = 0\n",
    "    feature_extract_36 = 0\n",
    "    feature_extract_37 = 0\n",
    "    \n",
    "    feature_extract_38 = 0\n",
    "    feature_extract_39 = 0\n",
    "    feature_extract_40 = 0\n",
    "    \n",
    "    feature_extract_41 = 0\n",
    "    feature_extract_42 = 0\n",
    "    feature_extract_43 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in analyst_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                questions_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    sentiment_result = sentiment_nlp(sentence)\n",
    "                    sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "                    question_sentiment_list.append(sentiment_score)\n",
    "                    net_sentiment_list.append(sentiment_score)\n",
    "\n",
    "                    if positivity_value == \"positive\":\n",
    "                        net_positive += 1\n",
    "                        Qpositive += 1\n",
    "\n",
    "                    elif positivity_value == \"negative\":\n",
    "                        net_negative += 1\n",
    "                        Qnegative += 1\n",
    "\n",
    "                    else:\n",
    "                        net_neutral += 1\n",
    "                        Qneutral += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for speech_bubble in management_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.search(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                reply_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # for \"n_flslist\":\n",
    "        n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, n_flslist, n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"s_flslist\":\n",
    "        s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, s_flslist, s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"ns_flslist\":\n",
    "        ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, ns_flslist, ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral)\n",
    "\n",
    "        feature_extract_23 = statistics.mean(net_sentiment_list)\n",
    "        feature_extract_24 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "        feature_extract_25 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "        feature_extract_26 = statistics.mean(net_text_complex_list)\n",
    "\n",
    "        feature_extract_27 = statistics.mean(question_sentiment_list)\n",
    "        feature_extract_28 = Qpositive/(Qpositive+Qnegative+Qneutral)\n",
    "        feature_extract_29 = Qnegative/(Qpositive+Qnegative+Qneutral)\n",
    "        feature_extract_30 = statistics.mean(questions_complex_list)\n",
    "\n",
    "        feature_extract_31 = statistics.mean(reply_sentiment_list)\n",
    "        feature_extract_32 = Rpositive/(Rpositive+Rnegative+Rneutral)\n",
    "        feature_extract_33 = Rnegative/(Rpositive+Rnegative+Rneutral)\n",
    "        feature_extract_34 = statistics.mean(reply_complex_list)\n",
    "\n",
    "        feature_extract_35 = statistics.mean(s_fls_sentiment_list)\n",
    "        feature_extract_36 = SFLSpositive/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "        feature_extract_37 = SFLSnegative/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "        \n",
    "        feature_extract_38 = statistics.mean(ns_fls_sentiment_list)\n",
    "        feature_extract_39 = NSFLSpositive/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "        feature_extract_40 = NSFLSnegative/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "        \n",
    "        feature_extract_41 = statistics.mean(n_fls_sentiment_list)\n",
    "        feature_extract_42 = NFLSpositive/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "        feature_extract_43 = NFLSnegative/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return [feature_extract_23, feature_extract_24, feature_extract_25, feature_extract_26, feature_extract_27, feature_extract_28, feature_extract_29, feature_extract_30, feature_extract_31, feature_extract_32, feature_extract_33, feature_extract_34, feature_extract_35, feature_extract_36, feature_extract_37, feature_extract_38, feature_extract_39, feature_extract_40, feature_extract_41, feature_extract_42, feature_extract_43]\n",
    "\n",
    "def get_SentimentLists_from_FLS(net_sentiment_list, THIS_flslist, THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral):\n",
    "\n",
    "    for each_fls_sentence in THIS_flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls_sentence)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        THISfls_sentiment_list.append(sentiment_score)\n",
    "        reply_sentiment_list.append(sentiment_score)\n",
    "        net_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "            FLSpositive += 1\n",
    "            Rpositive += 1\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "            FLSnegative += 1\n",
    "            Rnegative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "            FLSneutral += 1\n",
    "            Rneutral += 1\n",
    "\n",
    "    return THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08479071868693128 0.1948529411764706 0.11029411764705882 82.24652173913043 -0.0321318330345573 0.07692307692307693 0.10989010989010989 83.25190476190475 0.14357498502204433 0.2541436464088398 0.11049723756906077 81.402 0.04064054787158966 0.25 0.25 -0.16460241874059042 0.0 0.16666666666666666 0.15679605035056846 0.2631578947368421 0.10526315789473684\n"
     ]
    }
   ],
   "source": [
    "feature_extract_23, feature_extract_24, feature_extract_25, feature_extract_26, feature_extract_27, feature_extract_28, feature_extract_29, feature_extract_30, feature_extract_31, feature_extract_32, feature_extract_33, feature_extract_34, feature_extract_35, feature_extract_36, feature_extract_37, feature_extract_38, feature_extract_39, feature_extract_40, feature_extract_41, feature_extract_42, feature_extract_43 = getFeature23to43(analyst_sentences, management_sentences, speaker_names)\n",
    "print(feature_extract_23, feature_extract_24, feature_extract_25, feature_extract_26, feature_extract_27, feature_extract_28, feature_extract_29, feature_extract_30, feature_extract_31, feature_extract_32, feature_extract_33, feature_extract_34, feature_extract_35, feature_extract_36, feature_extract_37, feature_extract_38, feature_extract_39, feature_extract_40, feature_extract_41, feature_extract_42, feature_extract_43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 44: \"margin\" - average sentiment\n",
    "# 45: \"cost\" - average sentiment\n",
    "# 46: \"revenue\" - average sentiment\n",
    "# 47: \"earnings or EBIDTA\" - average sentiment\n",
    "# 48: \"growth\" - average sentiment\n",
    "# 49: \"leverage or debt\" -  average sentiment\n",
    "# 50: \"price\" – average sentiment\n",
    "# 51: \"operation\" - average sentiment \n",
    "def deepCleanTranscript(mytranscript):\n",
    "    updatedTranscript = ' '.join(mytranscript)\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    updatedTranscript = ac.remove_words(updatedTranscript)\n",
    "\n",
    "    if updatedTranscript[0] == \" \": \n",
    "        updatedTranscript = updatedTranscript.replace(\" \", \"\", 1)\n",
    "\n",
    "    updatedTranscript = re.sub('[^\\S\\n]+', ' ', updatedTranscript)\n",
    "    updatedTranscript.lower()\n",
    "\n",
    "    return updatedTranscript\n",
    "\n",
    "def getFeature44to50(mytranscript, speaker_names):\n",
    "    marginSentimentList = []\n",
    "    costSentimentList = []\n",
    "    revenueSentimentList = []\n",
    "    earningsEBIDTASentimentList = []\n",
    "    growthSentimentList = []\n",
    "    leverageDebtSentimentList = []\n",
    "    priceSentimentList = []\n",
    "    operationSentimentList = []\n",
    "\n",
    "    \n",
    "\n",
    "    updatedTranscript = deepCleanTranscript(mytranscript)\n",
    "    \n",
    "    updatedTranscriptList = split_paragraph_into_sentences(updatedTranscript)\n",
    "\n",
    "\n",
    "    for mysentence in updatedTranscriptList:\n",
    "        if (\" margin\" in mysentence) or (\" return\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            marginSentimentList.append(sentiment_score)\n",
    "\n",
    "        if \" cost\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            costSentimentList.append(sentiment_score)\n",
    "\n",
    "        if (\" revenue\" in mysentence) or (\" top line\" in mysentence) or (\" sales\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            revenueSentimentList.append(sentiment_score)\n",
    "\n",
    "        if (\" earning\" in mysentence) or (\" EBIDTA\" in mysentence) or (\" profit\" in mysentence) or (\" bottom line\" in mysentence) or (\" net income\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            earningsEBIDTASentimentList.append(sentiment_score)\n",
    "\n",
    "        if (\" growth\" in mysentence) or (\" organic\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            growthSentimentList.append(sentiment_score)\n",
    "\n",
    "        if (\" leverage\" in mysentence) or (\" debt\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            leverageDebtSentimentList.append(sentiment_score)\n",
    "\n",
    "        if \" price\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            priceSentimentList.append(sentiment_score)\n",
    "\n",
    "        if \" operation\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            operationSentimentList.append(sentiment_score)\n",
    "\n",
    "\n",
    "    feature_extract_44 = statistics.mean(marginSentimentList)\n",
    "\n",
    "    feature_extract_45 = statistics.mean(costSentimentList)\n",
    "\n",
    "    feature_extract_46 = statistics.mean(revenueSentimentList)\n",
    "\n",
    "    feature_extract_47 = statistics.mean(earningsEBIDTASentimentList)\n",
    "\n",
    "    feature_extract_48 = statistics.mean(growthSentimentList)\n",
    "\n",
    "    feature_extract_49 = statistics.mean(leverageDebtSentimentList)\n",
    "\n",
    "    feature_extract_50 = statistics.mean(priceSentimentList)\n",
    "\n",
    "    feature_extract_51 = statistics.mean(operationSentimentList)\n",
    "\n",
    "    return feature_extract_44, feature_extract_45, feature_extract_46, feature_extract_47, feature_extract_48, feature_extract_49, feature_extract_50, feature_extract_51\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing TFIDF\n",
    "\n",
    "rolling_path_of_five_transcript = [\n",
    "    'sectors/tech/AAPL/AAPL20224.csv', \n",
    "    'sectors/tech/AAPL/AAPL20223.csv',\n",
    "    'sectors/tech/AAPL/AAPL20222.csv',\n",
    "    'sectors/tech/AAPL/AAPL20221.csv',\n",
    "    'sectors/tech/AAPL/AAPL20214.csv'\n",
    "    ]\n",
    "\n",
    "WHOLE_rolling_frame_of_five_transcripts = []\n",
    "PRERELEASE_rolling_frame_of_five_transcripts = []\n",
    "MANAGEMENT_SENTENCES_rolling_frame_of_five_transcripts = []\n",
    "ANALYST_SENTENCES_rolling_frame_of_five_transcripts = []\n",
    "\n",
    "\n",
    "for path in rolling_path_of_five_transcript:\n",
    "    wholeTranscript = get_transcript(path)\n",
    "    transcript_safe_harbour, transcript_questions = split_transcript(wholeTranscript)\n",
    "    speaker_names = get_file_speaker_names(sector, stock)\n",
    "    analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "    not_current_analyst_names = []\n",
    "    for names in speaker_names:\n",
    "        if names not in analyst_names:\n",
    "            not_current_analyst_names.append(names)\n",
    "    analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "    wholeTranscript = deepCleanTranscript(wholeTranscript)\n",
    "    transcript_safe_harbour = deepCleanTranscript(transcript_safe_harbour)\n",
    "    management_sentences = deepCleanTranscript(management_sentences)\n",
    "    analyst_sentences = deepCleanTranscript(analyst_sentences)\n",
    "\n",
    "    WHOLE_rolling_frame_of_five_transcripts.append(wholeTranscript)\n",
    "    PRERELEASE_rolling_frame_of_five_transcripts.append(transcript_safe_harbour)\n",
    "    MANAGEMENT_SENTENCES_rolling_frame_of_five_transcripts.append(management_sentences)\n",
    "    ANALYST_SENTENCES_rolling_frame_of_five_transcripts.append(analyst_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stop_words = ['thanks', 'thank', 'really', 'said', 'say', 'yes', 'no', 've', 'll', 'don']\n",
    "all_stop_words = list(ENGLISH_STOP_WORDS) + custom_stop_words\n",
    "\n",
    "def tf_idf(transcript):\n",
    "    #1. Removes stop words, 2. finds tf.idf value, used as a weight\n",
    "    vectoriser = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        max_features=100,\n",
    "        ngram_range=(1, 3), # 1 to trigram as they are all common in finance (i.e. earnings per share, free cash flow etc.)\n",
    "        stop_words=all_stop_words # removes stop words (i.e. irrevelant day to day words)\n",
    "    )\n",
    "    #vectorises tfidf values into a vector\n",
    "    tfidf_vec = vectoriser.fit_transform(transcript)\n",
    "    \n",
    "    # feature_names = vectoriser.get_feature_names()\n",
    "    # for i, value in enumerate(tfidf_vec[0].toarray()[0]):\n",
    "    #     if value > 0:\n",
    "    #         print(f\"{feature_names[i]}:{value}\")\n",
    "\n",
    "    return tfidf_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 43)\t0.018700862653961002\n",
      "  (0, 46)\t0.026283409951665233\n",
      "  (0, 32)\t0.13141704975832616\n",
      "  (0, 5)\t0.10513363980666093\n",
      "  (0, 94)\t0.018700862653961002\n",
      "  (0, 54)\t0.0442208660928422\n",
      "  (0, 41)\t0.0221104330464211\n",
      "  (0, 27)\t0.018700862653961002\n",
      "  (0, 77)\t0.0442208660928422\n",
      "  (0, 90)\t0.037401725307922004\n",
      "  (0, 74)\t0.0221104330464211\n",
      "  (0, 81)\t0.0221104330464211\n",
      "  (0, 67)\t0.0221104330464211\n",
      "  (0, 11)\t0.037401725307922004\n",
      "  (0, 48)\t0.037401725307922004\n",
      "  (0, 39)\t0.056102587961883006\n",
      "  (0, 3)\t0.018700862653961002\n",
      "  (0, 42)\t0.018700862653961002\n",
      "  (0, 87)\t0.07480345061584401\n",
      "  (0, 82)\t0.0221104330464211\n",
      "  (0, 49)\t0.018700862653961002\n",
      "  (0, 75)\t0.093504313269805\n",
      "  (0, 56)\t0.056102587961883006\n",
      "  (0, 40)\t0.0442208660928422\n",
      "  (0, 9)\t0.11220517592376601\n",
      "  :\t:\n",
      "  (4, 63)\t0.01722505651441407\n",
      "  (4, 23)\t0.03445011302882814\n",
      "  (4, 12)\t0.01722505651441407\n",
      "  (4, 78)\t0.08146222253594357\n",
      "  (4, 61)\t0.05167516954324221\n",
      "  (4, 37)\t0.03445011302882814\n",
      "  (4, 34)\t0.10335033908648442\n",
      "  (4, 35)\t0.03445011302882814\n",
      "  (4, 30)\t0.10335033908648442\n",
      "  (4, 72)\t0.2239257346873829\n",
      "  (4, 25)\t0.08612528257207035\n",
      "  (4, 96)\t0.08146222253594357\n",
      "  (4, 91)\t0.06890022605765628\n",
      "  (4, 14)\t0.01722505651441407\n",
      "  (4, 71)\t0.258375847716211\n",
      "  (4, 64)\t0.20670067817296883\n",
      "  (4, 86)\t0.05167516954324221\n",
      "  (4, 38)\t0.03445011302882814\n",
      "  (4, 98)\t0.10182777816992947\n",
      "  (4, 7)\t0.08612528257207035\n",
      "  (4, 83)\t0.06890022605765628\n",
      "  (4, 22)\t0.2928259607450392\n",
      "  (4, 17)\t0.10182777816992947\n",
      "  (4, 8)\t0.06890022605765628\n",
      "  (4, 44)\t0.1550255086297266\n"
     ]
    }
   ],
   "source": [
    "value = tf_idf(MANAGEMENT_SENTENCES_rolling_frame_of_five_transcripts)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.74374996 0.68378515 0.62397491 0.67727902]\n",
      " [0.74374996 1.         0.70188012 0.59992368 0.64490322]\n",
      " [0.68378515 0.70188012 1.         0.68754767 0.63122875]\n",
      " [0.62397491 0.59992368 0.68754767 1.         0.70525114]\n",
      " [0.67727902 0.64490322 0.63122875 0.70525114 1.        ]]\n",
      "0.7437499632091964\n",
      "0.6837851541528949\n",
      "0.6239749065362483\n",
      "0.6772790154553162\n"
     ]
    }
   ],
   "source": [
    "cos = linear_kernel(value) # finds the cosine similarity matrix\n",
    "print(cos) # cosine similarity matrix\n",
    "\n",
    "print(cos[0,1]) #(compares the first transcript with the second transcript)\n",
    "print(cos[0,2]) #(compares the first transcript with the third transcript)\n",
    "print(cos[0,3]) #(compares the first transcript with the fourth transcript)\n",
    "print(cos[0,4]) #(compares the first transcript with the fifth transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# 51: Rolling frame of Whole Transcripts - TF-IDF value\n",
    "# 52: Rolling frame of Pre-releases - TF-IDF value\n",
    "# 53: Rolling frame of Management Sentences (Their Replies) - TF-IDF\n",
    "# 54: Rolling frame of Analyst Sentences (Their Replies) - TF-IDF\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "rolling_path_of_five_transcript = [\n",
    "    'sectors/tech/AAPL/AAPL20224.csv', \n",
    "    'sectors/tech/AAPL/AAPL20223.csv',\n",
    "    'sectors/tech/AAPL/AAPL20222.csv',\n",
    "    'sectors/tech/AAPL/AAPL20221.csv',\n",
    "    'sectors/tech/AAPL/AAPL20214.csv'\n",
    "    ]\n",
    "\n",
    "    \n",
    "def getFeature51to54(sector, stock, rolling_path_of_five_transcript):\n",
    "\n",
    "    WHOLE_rolling_frame_of_five_transcripts = []\n",
    "    PRERELEASE_rolling_frame_of_five_transcripts = []\n",
    "    MANAGEMENT_SENTENCES_rolling_frame_of_five_transcripts = []\n",
    "    ANALYST_SENTENCES_rolling_frame_of_five_transcripts = []\n",
    "\n",
    "\n",
    "    for path in rolling_path_of_five_transcript:\n",
    "        wholeTranscript = get_transcript(path)\n",
    "        transcript_safe_harbour, transcript_questions = split_transcript(wholeTranscript)\n",
    "        speaker_names = get_file_speaker_names(sector, stock)\n",
    "        analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "        not_current_analyst_names = []\n",
    "        for names in speaker_names:\n",
    "            if names not in analyst_names:\n",
    "                not_current_analyst_names.append(names)\n",
    "        analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "        wholeTranscript = deepCleanTranscript(wholeTranscript)\n",
    "        transcript_safe_harbour = deepCleanTranscript(transcript_safe_harbour)\n",
    "        management_sentences = deepCleanTranscript(management_sentences)\n",
    "        analyst_sentences = deepCleanTranscript(analyst_sentences)\n",
    "\n",
    "        WHOLE_rolling_frame_of_five_transcripts.append(wholeTranscript)\n",
    "        PRERELEASE_rolling_frame_of_five_transcripts.append(transcript_safe_harbour)\n",
    "        MANAGEMENT_SENTENCES_rolling_frame_of_five_transcripts.append(management_sentences)\n",
    "        ANALYST_SENTENCES_rolling_frame_of_five_transcripts.append(analyst_sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sectors/tech/AAPL/AAPL20224.csv',\n",
       " 'sectors/tech/AAPL/AAPL20223.csv',\n",
       " 'sectors/tech/AAPL/AAPL20222.csv',\n",
       " 'sectors/tech/AAPL/AAPL20221.csv',\n",
       " 'sectors/tech/AAPL/AAPL20214.csv']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[1-9]**[1-9]*[1-4].*')\n",
    "sector_files.sort(reverse=True)\n",
    "\n",
    "rolling_path_of_five_transcript = sector_files[0:5]\n",
    "rolling_path_of_five_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 51: Rolling frame of Whole Transcripts - TF-IDF value\n",
    "# 52: Rolling frame of Pre-releases - TF-IDF value\n",
    "# 53: Rolling frame of Management Sentences (Their Replies) - TF-IDF\n",
    "# 54: Rolling frame of Analyst Sentences (Their Replies) - TF-IDF\n",
    "\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"tech\"\n",
    "stock = \"AAPL\"\n",
    "\n",
    "path = \"sectors/tech/AAPL/AAPL20224.csv\"\n",
    "\n",
    "mytranscript = get_transcript(path)\n",
    "transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "speaker_names = get_file_speaker_names(sector, stock)\n",
    "analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "\n",
    "not_current_analyst_names = []\n",
    "\n",
    "for names in speaker_names:\n",
    "    if names not in analyst_names:\n",
    "        not_current_analyst_names.append(names)\n",
    "        \n",
    "analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "\n",
    "fea_ext_list5to22 = getFeature5to22(transcript_safe_harbour, speaker_names)\n",
    "print(fea_ext_list5to22)\n",
    "\n",
    "fea_ext_list23to43 = getFeature23to43(analyst_sentences, management_sentences, speaker_names)\n",
    "print(fea_ext_list23to43)\n",
    "\n",
    "fea_ext_list44to50 = getFeature44to50(mytranscript, speaker_names)\n",
    "print(fea_ext_list44to50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = \"semiconductors\"\n",
    "stock = \"AEHR\"\n",
    "\n",
    "sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[1-9]**[1-9]*[1-4].*')\n",
    "sector_files.sort(reverse=True)\n",
    "for path in sector_files: # for every .csv path of that stock\n",
    "    mytranscript = get_transcript(path)\n",
    "    transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "    speaker_names = get_file_speaker_names(sector, stock)\n",
    "    analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of .csv file info for each stock:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# META DATA\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 0. Year of transcript release\n",
    "# 1. Quarter of transcript release\n",
    "# 2. Date of transcript release\n",
    "# 3. Earnings Transcript contents\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Feature Extractions:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 4. EPS surprise value\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# Transcript Features:\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Pre release:\n",
    "# 5. Whole pre-release - net sentiment\n",
    "# 6. Whole pre-release - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 7. Whole pre-release - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 8. Whole pre-release - net word complexity\n",
    "#\n",
    "#\n",
    "# 9. Specific foward looking statment - sentiment\n",
    "# 10. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 11. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 12. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 13. Non Specific Forward looking statement - sentiment \n",
    "# 14. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 15. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 16. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 17. Not Foward looking statement - sentiment\n",
    "# 18. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 19. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 20. Not Foward looking statement - word complexity\n",
    "#\n",
    "# 21: #of_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "# 22: #of_non_specific/#of_non_specific+#of_not_fls+#of_specific\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# Questions & Answers:\n",
    "# 23. Whole Q&A - net sentiment\n",
    "# 24. Whole Q&A – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 25. Whole Q&A – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 26. Whole Q&A - net word complexity\n",
    "# \n",
    "# 27. all question (aggregate) - sentiment\n",
    "# 28. all question (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 29. all question (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 30. all question (aggregate) - word complex\n",
    "#\n",
    "# 31. all reply (aggregate) - sentiment\n",
    "# 32. all reply (aggregate) – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 33. all reply (aggregate) – #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 34. all reply (aggregate) - word complex\n",
    "#\n",
    "# For all replies (aggregate):\n",
    "# 35. Specific foward looking statment - sentiment\n",
    "# 36. Specific foward looking statment - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 37. Specific foward looking statment - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 38. Specific foward looking statment - word complexity\n",
    "#\n",
    "# 39. Non Specific Forward looking statement - sentiment \n",
    "# 40. Non Specific Forward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 41. Non Specific Forward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 42. Non Specific Forward looking statement - word complexity\n",
    "#\n",
    "# 43. Not Foward looking statement - sentiment\n",
    "# 44. Not Foward looking statement - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 45. Not Foward looking statement - #of_negative(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 46. Not Foward looking statement - word complexity\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# With specific words analysis:\n",
    "# Sentences that includes the word:\n",
    "# all of these words can be plural (e.g. cost and costs)\n",
    "# 47: \"margin\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 48: \"cost\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 49: \"revenue\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 50: \"earnings\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 51: \"growth\" - #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 52: \"EBIDTA\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 53: \"leverage\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 54: \"debt\" -  #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# 55: \"price\" – #of_positive(sentiment)/#of_negative+positive+neutral(sentiment)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 56: Rolling frame of Whole Transcripts - TF-IDF value\n",
    "# 57: Rolling frame of Pre-releases - TF-IDF value\n",
    "# 58: Rolling frame of Management Sentences (Their Replies) - TF-IDF\n",
    "#\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "#\n",
    "# 59: Stock price difference between Day 0 (Day earnings call is released) and Day 10\n",
    "# 60: Stock price difference between Day 0 (Day earnings call is released) and Day 20\n",
    "# 61: Stock price difference between Day 0 (Day earnings call is released) and Day 30\n",
    "# 62: Stock price difference between Day 0 (Day earnings call is released) and Day 40\n",
    "# 63: Stock price difference between Day 0 (Day earnings call is released) and Day 50\n",
    "# 64: Stock price difference between Day 0 (Day earnings call is released) and Day 60\n",
    "# 65: Stock price difference between Day 0 (Day earnings call is released) and Day 70\n",
    "# 66: Stock price difference between Day 0 (Day earnings call is released) and Day 80\n",
    "# 67: Stock price difference between Day 0 (Day earnings call is released) and Day 90\n",
    "#\n",
    "# 68: Stock price difference between Day 1 (Day earnings call is released) and Day 10\n",
    "# 69: Stock price difference between Day 1 (Day earnings call is released) and Day 20\n",
    "# 70: Stock price difference between Day 1 (Day earnings call is released) and Day 30\n",
    "# 71: Stock price difference between Day 1 (Day earnings call is released) and Day 40\n",
    "# 72: Stock price difference between Day 1 (Day earnings call is released) and Day 50\n",
    "# 73: Stock price difference between Day 1 (Day earnings call is released) and Day 60\n",
    "# 74: Stock price difference between Day 1 (Day earnings call is released) and Day 70\n",
    "# 75: Stock price difference between Day 1 (Day earnings call is released) and Day 80\n",
    "# 76: Stock price difference between Day 1 (Day earnings call is released) and Day 90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete operator name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.spglobal.com/marketintelligence/en/news-insights/blog/analyzing-sentiment-in-quarterly-earnings-calls-q2-2022\n",
    "\n",
    "\n",
    "# https://www.amenityanalytics.com/case-studies/earnings-call-transcript-analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TF-IDF ----> from management sentences (Replies + pre-release)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
