2020
3
2019-11-22 17:00:00
"Operator: Ladies and gentlemen, thank you for standing by, and welcome to the Splunk Third Quarter 2020 Conference Call. [Operator Instructions]. I would now like to hand the conference over to your speaker today, Mr. Ken Tinsley, Corporate Treasurer and Vice President of Investor Relations. Please go ahead, sir.
Ken Tinsley: Great. Thank you, Catherine, and good afternoon, everyone. With me on the call today are Doug Merritt; Jason Child; and Chief Technology Officer, Tim Tully.After market closed today, we issued a press release, which is also posted on our website. Also note that we have posted supplemental material on the Investor Relations web page as well. This conference call is being broadcast live via webcast. And following the call, an audio replay will be available on our website.On today's call, we will be making forward-looking statements, including financial guidance and expectations, including our forecast for our fourth quarter and full year of fiscal 2020; duration, revenue mix and long-term cash flows; and our expectations regarding our products, technology, strategy, customers, markets, industry acquisitions and investments. These statements reflect our best judgment based on factors currently known to us, and actual events or results may differ materially. Please refer to documents we file with the SEC, including the Form 8-K filed with today's press release. Those documents contain risks and other factors that may cause actual results to differ from those contained in our forward-looking statements. These forward-looking statements are being made as of today, and we disclaim any obligation to update or revise these statements. If this call is reviewed after today, the information presented during this call may not contain current or accurate information.We will also discuss non-GAAP financial measures, which are not prepared in accordance with generally accepted accounting principles. A reconciliation of GAAP and non-GAAP results is provided in the press release and on our website.With that, now over to Doug.
Douglas Merritt: Thank you, Ken, and thank you all for joining us. We had a great Q3. Our execution was excellent, and our results were strong. I'm particularly proud of our 40% software revenue growth, 30% total revenue growth and 78% cloud growth. We completed 3 strategic acquisitions. We welcomed over 11,000 of our customers and partners to our user conference in Las Vegas. And we unveiled the industry's first Data-to-Everything platform, our expanded vision for bringing data to every question, decision and action.With the rollout of our Data-to-Everything platform, we introduced a new pricing approach so customers can focus on the value that data brings to their organizations. Our new pricing options, predictive pricing, infrastructure-based pricing and rapid adoption packages meaningfully expand our opportunity worldwide while simultaneously making it easier to do business with us. Customer feedback so far has been extremely positive.In addition, Splunk Cloud received FedRAMP certification, a strategic acknowledgment of the importance of our technology and a critical step in positioning us to serve our public sector customers as we power digital transformation across the U.S. government. With the authorization, we can now offer a single solution capable of solving IT, security and the mission challenges for agencies where data-driven decisive actions matter for the success of the mission.As you know, Splunk is prosecuting massive opportunities across IT operations, security operations and software development operations. Data has emerged as the strategic asset as more and more enterprises and government agencies around the globe are investing in data-centric digital strategies. We believe data is the answer to many of the world's most pressing problems and its greatest opportunities. It also represents a necessary strategy for all organizations to thrive, let alone survive, in this new paradigm. From improving corporate performance, to optimizing government responsiveness, to fighting crime, to medical research, to firefighting or to the rollout of Porsche's new electric car, we believe every problem is a data problem, and our customers think so, too.For example, one of the best new stories I heard about at .conf19 was how Domino's Pizza has revealed their secret sauce, Splunk, to more teams across their company. Over the past couple of years, Domino's has driven a prevalent culture shift. They simply do not make any business decision unless they can support it with data. The global pizza delivery leader uses Splunk to help them manage IT operations, protect their customers and their brand from security threats as well as improve their marketing, communications and their trademarks, speedy and high-quality customer experience, all with data.Ultimately, with Splunk, Domino's can ask questions and act on data in realtime to keep their business thriving. Airbus Defence and Space expanded a long-time partnership with Splunk in order to improve business efficiency and overall security. Splunk enables Airbus to better protect and monitor the IT landscape throughout Europe, which ensures end-to-end visibility across global devices, more reliable systems and reduced operational incidents and team workloads. And long-time customer, Anaplan, significantly expanded their use of Splunk Cloud and Enterprise Security. With Splunk, Anaplan can take action on IT data across the business to resolve issues within seconds before it impacts their customers.Anaplan is also using Splunk across sales and marketing to help further drive customer adoption of their products, a great example of a customer who has truly embraced Data-to-Everything.Splunk is the only company enabling customers to investigate, monitor, analyze and act on massive volumes of diverse and disconnected data. Why is this so important? According to IDC, digital transformation investments are projected to reach over $1 trillion in 2019, and they're expected to reach $6 trillion in 4 years. Splunk is well positioned to be the data foundation for this digital transformation. We are already the strategic provider to over 90 of the Fortune 100. And our 19,000 customers represent every industry, every size of organization and every function.In order to serve our growing customer base and the growing number of use cases, we are investing in products and technologies that extend Splunk's value and relevance to all of our customers' decisions, regardless of whether the data they need sits in the Splunk index or somewhere else.Building the Splunk Data-to-Everything platform has been a multiyear, multibillion-dollar commitment. Through a combination of internal development and strategic acquisitions, we've invested in excess of $2 billion over the past couple of years to help the world turn data into doing. While our focus is to use these technologies to further strengthen our IT and security use cases, we are also expanding our market for the long term. Splunk's ability to access more data and analyze it faster allows us to appeal to new users and solve new use cases.In summary, I am proud of our Q3 results. And I want to thank our customers, partners and our Splunkers for their commitment to turn data into doing.Now I'll turn it over to Tim Tully, our Chief Technology Officer, to take you through our product and technology strategy.
Timothy Tully: Thank you, Doug. It's my pleasure to be on the call. Splunk is bringing innovation to everything via three highly focused initiatives: build, buy and invest. This was clearer than ever in Q3 when we announced more innovative products, acquisitions and programs than ever before and to Splunk beyond the index for the first time in our history. As of the end of Q3, Splunk had over 450 issued patents in over 650 pending patent applications. Over the past 2 years, we've more than doubled the size of our R&D team. And today, we have 12 global R&D sites. We've closed the acquisition of 8 new companies since September 2017. And as we announced during our September Data-to-Everything launch event, Splunk has committed a $150 million venture arm to invest in the wider data ecosystem.We hosted our biggest conf ever, both in attendance and product innovation. Whether you're a tech nerd, data nerd or design nerd, there were dozens of amazing things to see during our user conference. Since I'm all 3 of those, every day of .conf was off the charts. Over 11,000 people came from around the world to Las Vegas to attend over 420 keynotes and breakout sessions. Over 2 days, we announced 20 new releases in beta products, including the general availability of Splunk Data Stream Processor, Splunk Data Fabric Search and Splunk Enterprise 8.0 and the beta release of several new products, including Splunk Mission Control. I couldn't walk far without being stopped by excited customers and partners.Within our innovation strategy to build, buy and invest, Splunk is focused on building products that enable unbounded machine learning and realtime processing while offering indulgent design, which fits the product form to function in a beautiful way. Furthermore, we are bringing Splunk everywhere you are, through mobile and collaborative capabilities to our products.In Q3, we merged these priorities as we announced products enabling customers to innovate on Splunk Enterprise and beyond the index. First, we announced the general availability of Splunk Data Stream Processor, a realtime stream engine, which sits outside the Splunk index and is powered by our industry-leading unbounded machine learning, which uses adaptive learning in realtime and sub-tunes with no volume or cardinality limits. We are convicted in our feeling that the future of machine learning is in the stream, not in the batch world, and Splunk is positioned to be the industry leader in this space.We also released Data Fabric Search, which supercharges the speed and scale of Splunk Enterprise by an order of magnitude or more in addition to adding federated search capability across Splunk deployments and non-Splunk resources.And finally, this quarter demonstrated the ways we're leading in consumer-grade design with collaborative features through our new user interface frameworks. This was on full display at .conf, where we announced 2 new products that reside on our multi-tenant cloud-native platform.The first was Mission Control, the hybrid cloud SaaS product for the security operation center that connects to and brings together our on-premises Enterprise Security, UBA and Phantom products into one single beautiful pane of glass. The second product delivered on the new cloud platform was Splunk Investigate, a cloud-based product based on workbook style investigation flows to search and analyze data collaboratively and in realtime. DevOps and DevSecOps customers are those new to Splunk, where smaller datasets can use Splunk Investigate for easy data onboarding, root cause analysis, incident management, troubleshooting production issues and much more.Next, I'd like to talk about our acquisition strategy that both enhances the product portfolio and helps deepen our roots in the Open Source community. Through our acquisitions of Omnition and Streamlio, Splunk is positioned as a top contributor to OpenTelemetry in the Apache Pulsar. This week, we announced additional contributions and donations to help make OpenTelemetry the standard for data collection and to advance the observability initiatives of the project's community.Our investment in OpenTelemetry is important. Making data collection open and accessible to everyone is a key imperative for Splunk. We are 100% behind OpenTelemetry as the new standard for the observability world. This new investment comes in addition to our existing commitments to Open Source projects such as Apache Kafka, Flink, Spark, Kubernetes and more, which are already at the heart of our products.Consumers have welcomed our embrace of Open Source to enhance ease of use, validating that we are building the products our customers want and which transform their organizations. With our purchases of SignalFx and Omnition, Splunk has the best solutions for realtime monitoring of infrastructure, microservices and applications, combining the 3 pillars of observability, logs, metrics and traces. We are laser-focused on integrating SignalFx and Omnition into Splunk for a unified observability experience.As I mentioned, the third piece of Splunk innovation is our investment strategy. In Q3, we introduced Splunk Ventures and 2 inaugural funds to introduce the next generation of data analytics. Splunk Ventures is complementary to our existing operations and provides portfolio companies with a variety of benefits, including access to Splunk technology, go-to-market support and engagement with Splunk leaders. True to Splunk, we designed our inaugural funds with a focus on outcomes. We established a $100 million Splunk Ventures Innovation Fund for early-stage startups, transforming data into business value, and a $50 million Splunk Ventures Social Impact Fund to invest in startups using data to create social good.We have already made a number of investments. And at .conf19, we were proud to announce our first Social Impact Fund investment in Zonehaven, a start-up developing technology to help fire departments get ahead of wildfires. Splunk Ventures' goal is to fund companies that will continue building the product ecosystem surrounding the Splunk data platform. Splunk Ventures will in turn help accelerate new technology in the data ecosystem to achieve world-changing outcomes.When we say we're bringing innovation to everything, we mean everything. Whether we're building, buying or investing, our intent is to enable our customers to bring data to every question, decision and action.Thanks again for joining us. Now over to Jason.
Jason Child: Thanks, Tim, and good afternoon, everyone. Appreciate you joining us today. Q3 execution was solid, highlighted by strength in our public sector business. Third quarter revenues were $626 million, up 30% year-over-year. Cloud revenue was $80 million, up 78% over last year. In Q3, software revenues, which is the total of license and cloud, were $454 million, up 40% year-over-year.As we outlined last quarter, customers have been opting for Term and Cloud contracts over perpetual faster than we anticipated. As a result, we've accelerated the completion of our shift to a renewable model and discontinued new perpetual license offerings effective November 1. In Q3, 92% of software bookings were either Term or Cloud, slightly lower than Q2, due to some customers making a final perpetual purchase. We maintain our expectation that we'll exit this year with renewable mix in the high 90s and reach nearly 100% next year and beyond.With the shift to a renewable model largely complete, we've taken a fresh look at which operating metrics are the most relevant to our go-forward model. With the diminishing contribution from perpetual licenses, we believe an annualized metric on Term and Cloud better reflects the underlying growth of business. Today, we're providing total ARR, or annual recurring revenue, which we define as annualized value of Cloud, Term license and maintenance contracts as of the last day of the period. Note that ARR excludes all perpetual license and nonrecurring professional services. As you'd expect, this metric is intended to provide a view into normalized bookings activity for the period. For the quarter ended October 31, total ARR was $1.44 billion, comprised of $368 million from cloud and $1.07 billion from term license and maintenance contracts. This compares to total ARR of $939 million in Q3 of last year, representing 53% growth year-over-year. For your benefit, we've included a 5-quarter look back at ARR in the slides published with today's release. Our initial outlook for next year is for ARR growth to be in the mid-40% range.We ended Q3 with total RPO of $1.45 billion, up 52% over Q3 of last year. The portion of RPO, which we expect to recognize as revenue over the next 12 months, was $862 million at quarter end, up 42% year-over-year. RPO bookings was $839 million, up 42% from Q3 of last year. In the quarter, we recorded 134 orders greater than $1 million in total contract value, up 21% from 111 last year.In our public sector business, we had a strong third quarter, booking several mid-8-figure orders with U.S. government agencies. These agencies can require a contract term that's greater than 3 years and in many cases, up to 5 years. With orders of this magnitude and their extended duration, our overall duration skewed to 36 months this quarter, up from our recent levels of around 33 months. We believe this was specific to pub-sec momentum stemming from the federal fiscal year-end in Q3 and will revert to prior levels in Q4. On margins, which are non-GAAP. Q3 overall gross margin was 86%, up slightly on a year-over-year basis due to scale in our cloud business. Q3 operating margin was 17%, slightly better than planned, driven by our solid top line performance.Turning to guidance. We expect our high-growth trajectory to continue for the remainder of the year. Q4 revenues should reach approximately $780 million, with a non-GAAP operating margin of 23%. For the full year, we are now expecting total revenues of $2.35 billion, up from $2.3 billion, and we maintain our non-GAAP operating margin target of 14%.Looking further out, we intend to walk you through our updated long-term model at our Analyst Day in spring, but we think it's important to get a view into future cash flows relating to the shift from perpetual to ratable invoicing for Cloud and Term contracts that we discussed last quarter. To reiterate, the billing change to 1 year is a cash flow timing consideration only and is driven completely by customer preference and market trends. Today, we are reiterating the full year operating cash flow target we updated last quarter and expect roughly the same level for next year in fiscal '21. As billings from previously booked multiyear contracts begin to contribute more meaningful -- meaningfully to cash collections, cash flow should turn positive in FY '22 and approach $1 billion in FY '23. Again, we're providing this for high-level visibility, and we'll detail other growth and operational targets at a later time.In closing, it was an excellent quarter, and we're looking forward to a strong finish to the year.With that, let's open it up for questions.
Operator: [Operator Instructions]. Our first question comes from Raimo Lenschow with Barclays.
Raimo Lenschow: And congratulations. Doug, I had two quick questions. So Doug, can you talk a little bit about the reception you got from the customer base around the pricing changes? You obviously gave us like bandwidths for what people want to pay on ingestion and then on from a compute usage as well. Like what was the feedback? Do you see any kind of early indications of which way people are thinking? And I have a follow-up.
Douglas Merritt: Thanks, Raimo. Yes. So as you would guess, I've been on the road pretty much consistently in the past two months with our strike event .conf and then our continued customer visit series, and one of the top things we talk about is pricing. What I've seen universally is a high enthusiasm and almost a sigh of relief from customers as we talked about, both the broadband for a data volume-based approach or an infrastructure component for people who just want to get away from data volumes altogether.One of my favorites is a couple of visits, 2 visits with a large European bank, who -- clients who crisply talked about a wildfire of use cases that are spreading across the bank again because they moved to infrastructure-based pricing and could focus on the value being produced from these cases instead of worrying about the volume of data. So really excited about it.That said, we have to lean in hard on marketing, communications and sales enablement, so every customer hears about it, as there are still many customers that I talked to where it's new news or not entirely understood by them. So a lot of work for our teams to do to ensure that all customers get the understanding and the value that has come in with what we're doing with the Splunk portfolio.
Raimo Lenschow: Perfect. And Jason, thanks for the extra disclosure, that's really, really helpful. Can we talk on AR? So obviously now, you kind of still have like the effect of more subscription coming through there. Is there an idea at what time should we see like normalized ARR? Is that like -- is it the next year's story or a 2-year story? How do I have to think about that? So it's obviously, it's a new number, and we all need to get our heads around that.
Jason Child: Yes. I would say, first, we provided the five quarter trend just to try to make it a little easier for you guys to try to predict it. I'd say, second, the most -- I think probably the most important thing to think about is ARR is, we think, the most durable growth metric to think about how this business is actually performing annually. And if you think about the kind of customer base, only about 5% of our perp customers have actually converted to Term or Cloud. And so there's a lot of growth to come.So in terms of normalization, I don't know -- we don't expect there to be a significant deceleration. And that actually brings me to my third point. We did give an early view of what next year looks like. And I said mid-40% range is kind of a good place to start from at this point, and we'll certainly update you further in the future.
Operator: And our next question comes from Brad Zelnick with Crédit Suisse.
Brad Zelnick: And congrats, guys, on a great quarter. I've got one for Jason and one for Doug and a follow-up. Jason, if we look to Slide 14 with the $1 billion in operating cash flow in fiscal '23, and I appreciate you said you'll share more at Analyst Day and as we move forward, but what are the assumptions that get you there? And more specifically, perhaps, what are the most sensitive assumptions that can cause you to end up above or below?
Jason Child: Sure. So first, thanks. We feel great about the quarter. I would say the sensitivities, when it comes to cash, there's a lot. The first thing is why is this taking kind of 3 full years to snap back. And you've probably looked at companies that have gone through kind of transformations more than I have. But we're, in short, going from collecting the vast majority of the cash upfront, to now moving into ratable billings. Since our average duration is 3 years, we're effectively moving from -- it's going to take us about 3 years to kind of get through that transition to when you'll see the cash flow as a percentage of revenue or top line flow through.And so I feel very confident where we're going to be in '23. The movement between them is going to move a bit, depending on a bunch of variables. So what's the cloud mix versus on-prem? What is the ramping of deals? How much in year 1 versus 2 and 3, et cetera. So those are the reasons why I have to be a little more opaque through the transition. But again, very confident on where the numbers will be post transition, which will be in 2023 -- or I'm sorry, fiscal '23, which is '22, I guess.
Brad Zelnick: Excellent. That's great context. I appreciate it. And just my follow-up, it's great to hear Tim speak about the commitment to OpenTelemetry. Can you talk about the impact you think it will have in leveling the playing field in the observability space, how it benefits customers and what it means for Splunk and its competitors?
Timothy Tully: Yes. I think moving to an Open Source framework to collect data is really the best move for the customer. What it helps them do is not feel like they're trapped by any one particular vendor and gives them a common substrate upon which to operate. Ideally, that one vendor is Splunk, but we're always trying to do what's best for the customer and best for the user. And we think that some of the donations that we made to OpenTelemetry is really what's in the best interest of the power Splunk user.
Operator: And the next question comes from Keith Weiss with Morgan Stanley.
Keith Weiss: So you've seen a lot of real strength in those current RPO bookings numbers and the ARR numbers. So that's really nice to see flowing through the model. I wanted to ask about a couple of kind of the new pieces of the equation. Probably the one that kind of really peaked our interest was SignalFx and sort of getting more fully into their cloud monitoring and observability space. Any kind of initial read that you could give us in terms of customer interest, how we should be thinking about the timing of that kind of ramping up within in your business over the next year?
Douglas Merritt: Thanks, Keith. Yes, we are, as you guessed, we're super excited about SignalFx, but also our Mission, Streamlio, the release of Data Stream Processor. Streaming is an amazing complement to what we do with the index, and what we're doing with the combination of that portfolio is really exciting. Tim has been talking about unbounded learning and our completely unique approach within Data Streaming Processor of applying machine learning to streaming data rather than data at rest.But the interest that we've seen since the SignalFx announcement as kind of the edge of the wedge and the entire basket of capabilities is very exciting. It's clear in high tech, in any online business, that moving to a next-gen DevOps model is critical for their success than having a fully agile CI/CD backbone as key to how they move forward. And I think the portfolio that we now have is heads and tails above anything else in the industry on a feature function scale and role capability perspective.Tim, do you have any thoughts based on all the customers you've been interacting with?
Timothy Tully: Yes. The customers have been really positive. They recognize sort of the scalability of what SignalFx provides, especially compared to other vendors. They know that SignalFx is an upmarket product that blends well with our customer base. The other thing they've been really excited about is our acquisition of Omnition. There -- a lot of our customers are starting to cross into all sort of facets of observability around logs, metrics and traces. And they really appreciate what Omnition brings to the table in terms of tracing products. And it gives you a much better view of your application that lives on the microservices-based architecture. And it's much easier to consume for the user as opposed to traditional tracing solutions which are sort of this wall of text. So they're really buying into the story around the power of the combined Splunk SignalFx and Omnition portfolio.And then the last thing I'd add around that is I was really pleasantly excited to see that customers have figured out what was sort of my walking hypothesis, which was that our Data Stream Processor products would be a perfect marriage with the observability suite. And our customers have really already figured out that if they can move DSP together with SignalFx, the data platform story that they're building with our products is just amazing. So I've been really pleasantly surprised to see that.
Douglas Merritt: And Tim, why? What is the DSP?
Timothy Tully: Yes. A lot of customers, they have logs sitting around and they want to get to a place where they have logs, metrics and traces together. And so they're using our Data Stream Processor product to take those logs, ingest them into the stream inside of DSP and convert it to metrics. And that's what they use to ingest data into SignalFx. And it's been a really powerful thing to see. And what's going to happen is they're going to take that sort of recipe and build out other use cases on top of the platform. So we're really excited.
Keith Weiss: Okay. And that sounds really exciting. And maybe one quick one for Jason on the gross margin side of the equation. With cloud scaling so rapidly, I've been expecting that number to be coming down, but you're still seeing year-on-year increases in the overall gross margin. I'm assuming this isn't something that we should be modeling on a going-forward basis, but it probably should be at some point declining, right?
Jason Child: Over time, yes, because cloud margins today are in the mid-50s. They -- as of right now, my view is over the next couple of years, they probably get up to 70%. So as you see the mix of cloud grow, it will come down, since on-prem, we're obviously in the high 90s.But I would say Q3 was a little higher more because of mix. You saw a really strong pub-sec business that is all on-prem because our -- or largely on-prem since our certification for FedRAMP actually went through late in the quarter. So -- but I would expect to see it start to get closer to the 70s percent range as our cloud mix grows.
Operator: Our next question comes from Phil Winslow with Wells Fargo.
Philip Winslow: And congrats on a great quarter. I just wanted to dig in on Data Stream Processor. The feedback that we got at .conf was very positive on this. I know you touched on it earlier, but how are you positioning DSP versus some of the other streaming products that are out there on the market sort of on a stand-alone and then in the context of the Splunk portfolio? And then also the Streamlio acquisition. I mean I've heard people talk about sort of Pulsar being copped at 2.0, and obviously, Kafka and Flink are part of your existing capabilities. How do you think about Pulsar and Streamlio in the context of those two?
Timothy Tully: Yes. So the first part is a lot of customers are playing with DSP-like things. Mostly, we see a lot of people who have Kafka sitting around. And for the most part, the use cases we see there are around using Kafka more as a staging device or a message to you to sort of place data before there's later processing happening.Nothing a lot of sort of processing in the stream itself as of yet. So I think that's where sort of some of the confusion starts to lie. DSP is actually a superset of all that. We actually use Kafka itself underneath the hood. That's part of our solution, and we actually vendor it as part of the solution.So we're much more than just a staging area for data. The stream processing, in general, allows you to do realtime unloading, monitoring, alerts, realtime aggregation, dimension lookup, PII masking. But most importantly, it allows you to route data to multiple Splunk deployments or even non-Splunk data sources, if it came to that. So customers are really seeing the extension of the data platform story that Doug and I have been talking about for close to 2 years at this point. And I think customers really understand that DSP is a key piece of that equation.In terms of Streamlio, yes, really great to see you picked up on that acquisition in the fact that Apache Pulsar is underneath the hood there. With that acquisition, it's a fantastic team of amazing technologists at the core. That team is ex-Twitter guys and ex-Yahoo! guys who have been doing this for quite some time. And for us, largely, it was sort of additive to what we're trying to accomplish with Data Stream Processor, and it brings a wealth of talent and expertise to the equation. And we think that Pulsar is an amazing piece of technology and really interesting, and we're looking for ways to leverage it inside the product potentially.
Douglas Merritt: And if I go back, Phil, to the four pillars of investigate, monitor, analyze and act, what you've seen, and what, hopefully, our investors and our customers see, is we've been rallying around that framework for almost four years now to guide our organic-and-build approach as well as partner-and-purchase approach. And something like extremely on Pulsar, I think, is one of those lean-forward orientations.And we love what we've done with DSP. We purposely have built it in a way so that as new iterations of technology come forward, Open Source, in this case, are the product progresses, customers get a stable experience, and yet, the underpinnings of the product then will advantage of whatever next-generation enhancements or advancements come out in the community around us.So hopefully, all you guys are noticing we are actively embracing Open Source. We're contributing back to the Open Source community. The backbone of the overall engineering group continues to be augmented with talent from all around the world so that we can be a leading edge to serve our customers.
Operator: Our next question comes from Fatima Boolani with UBS.
Fatima Boolani: I have a question for Doug and a question for Jason. Doug, I'll start with you. Within the context of the pricing constructs and the new models that you've introduced, you mentioned sales enablement is a key focus area. So along those lines, at a high level, I was wondering if you would be able to share your perspectives on how these new constructs will impact incentives and compensation structures for your sales organization who are clearly used to filling in a different model for a very long time and then equally, within your indirect and channel partner distribution ecosystem. And then I have a follow-up for Jason, if I may.
Douglas Merritt: Sure. So I think the -- when we look at data volume, that is a proxy for infrastructure, and that's why I think the initial founders came up with that metric. So from a shift to the sales force, banding data volumes, I think, just simplifies their life and makes their communications more crisp with customers. Where I think the enablement does come in is the translation on virtual CPU or virtual core, is something where that rep needs to spend time with the customer or understand what is their use case and from an infrastructure basis, what they're going to need to bring that use case to life.In the past, it was data was a proxy. Now as we all know and have been talking about with the customers, data is still a proxy, but what you do with the data, are you excessively accruing the data, are they high-cardinality queries or not or find to be having a meaningful decision factor for those customers on whether they're going to get an easier translation of value by leaning on an infrastructure metric, or are they going to get an easier translation of value by having this elegantly banded tiering solution? So that the -- we remain customer success as our #1 priority, and that's why we offer two options. And I've seen that I can never predict exactly, going into a deal, which one the customer is going to lean on. And I think that optionality is going to be something appreciated by our customer base.
Fatima Boolani: I appreciate that color. And Jason, for you. You mentioned that only about 5% of your perpetual customer base has actually transitioned and/or migrated over to Term or Cloud form factors. So on the one hand, that's certainly an opportunity for you to convert the base. On the other hand, I'm wondering how we should internalize, just from an accounting perspective, how you would manage around horse trading between an erstwhile perpetual contract, moving to Term, and in some of these newer pricing models that are kind of working through the system. So if you can help put some guardrails around that for us from an accounting standpoint, that would be really helpful.
Jason Child: Well, I guess just to clarify, the approximately 5% of perp installed base, those are folks that are currently in our renewal subscription stream -- I'm sorry, in our maintenance renewal subscription stream. And so what those folks will either do is continue that maintenance stream or they will decide to upgrade into Term or Cloud. And when they upgrade, on average, they're upgrading at somewhere around 3x or higher. So I would think of it as either they're going to remain and we have a very high retention rate, or they're going to upgrade into a higher dollar amount. Does that answer your question?
Fatima Boolani: Yes. That's very helpful and clear.
Operator: Our next question comes from Brent Thill with Jefferies.
Brent Thill: Doug, you mentioned the new pricing is meaningfully expanding your opportunity. I know it's really early, but if you could give us your thoughts on what's surprising you the most in terms of new case studies that are coming to the platform from a case study, or is there a geo or some other area that you're seeing maybe perk up that you hadn't before?
Douglas Merritt: Thanks, Brent. As we've talked about for a while, the variety of use case is always blown, I think, all this way around Splunk. And as more things are being censored and more departments are getting a bit more aware of how data can play a role in driving their logistics for, to the manufacturing forward or marketing or sales or HR or finance, I think we're seeing that expansion of use case across those. And at .conf, something like the keynote where Porsche and the execs from Porsche and the Taycan walk through why Splunk is integral to the rollout and successful servicing of the Taycan, I think is one of those examples that could have been true 2 or 3 years ago. But with the augmented reality capability that we've introduced, with things like DSP to make sure that you can filter and stream that data and maybe even learn from it realtime as it's flowing across the stream, that provides much more enhanced capability for those use cases.What I really see the impact with pricing being is when I get above a 0.5 terabyte or a terabyte or 5 terabytes. That's when the light bulb usually goes on with these accounts, have, ""Oh my gosh, I've got a true Data-to-Everything Fabric here for use. And this could be a 100-terabyte or a 5-petabyte backbone. How do I make sure I can manage those costs?"" And that's where I've seen customers with either option, the unlimited banded option on a term basis or the virtual infrastructure option, be able to now have the feeling of control and not feel like the data volume is going to be something that overwhelms them on a lack of predictability or escalate -- too rapidly escalating price basis.So that's the virality that I think I mentioned with that European bank was based on. ""Oh my gosh, I've got control of infrastructure. I don't care whether we go to 50 terabytes or 150 terabytes. It will be maybe a performance factor I'm going to have to pay attention to, but I'm not going to be gated from these use cases.""
Operator: And our next question comes from Kash Rangan with Bank of America.
Kasthuri Rangan: I've got a deep cash flow question for Tim and a digital stream processor question for Jason. Relax. That's not really what I wanted to ask you. .conf was just absolutely fascinating. It looked so upbeat. Doug, a question for you, high level. It could be a very long complicated question, but I'm just going to dumb it down. In what investors consider to be a crowded landscape of DevOps, more broadly speaking, what does it take to win, and why do you think Splunk is going to win, especially given your acquisitions? And for Jason, I do have a cash flow question for you, though. If you can just help us understand why the turn in cash flow is a little bit longer than what we might have expected, maybe that was incorrect. But I would assume that since the renewable stuff has been running relatively high in the last 4 quarters, I would have thought that you started to switch to ASC 606 Q1 of calendar '18. So about the time we get into calendar 2021, you should start to see the full free cash flow conversion. So obviously, that's not happening. But can you just level-set us with the details on why you expect the turn to happen in calendar '22 and not in calendar '21?
Douglas Merritt: Awesome. I will start, Kash, with just the intro. I'm going to hand it over to Tim for the observability piece. What we saw in our due diligence, and I think I said this when we announced the acquisition of SignalFx, that we looked at every single entity in the marketplace and usually, 2 or 3x as far as engineered, engineer conversations. And why we were so excited about SignalFx and then the addition of Omnition and how we can deal with Apache Pulsar and Streamlio for DSP is the viral nature, the complexity of this next-generation development environment. You think public cloud makes it simple. In some sense, it does, but it also provides a lot of optionality and a ton of complexity as everything becomes ephemeral and short-lived potentially, creates a lot of streaming issues and a lot of potential noise.And we feel that the split, that ultimately, any company that is successful in that arena is going to get to volume and insight demands that someone like Splunk and the portfolio we have is geared to be able to handle. Tens to hundreds of teams trying to interoperate together, hundreds of terabytes to petabytes of data on a continuous streaming basis, that's a very, very challenging environment to actually operate effectively in. And we've distinguished ourselves through the years as being easy to use, but go to the corner case of scale and performance. And that's a key component of what we've been evaluating, thinking through and driving our observability platform around. And Tim, you probably can add better color than that high-level framework.
Timothy Tully: Yes. Doug mentioned the fact that we took a survey of the landscape and sort of looked at all the traditional vendors. And one of the reasons I'm so confident and really excited about what we have is I think we've built the world's most scalable observability platform after buying SignalFx. And what we did is we then took on the Omnition and built what I saw as being the best tracing product in the entire world. It's just far more easier to use as I sort of indicated on one of the earlier questions.And so what we did is we took what I saw has been the best metrics and traces solutions in the world and married it together with what's already the best logging platform, which is us. Obviously, that's Splunk. And so I think those 3 together are just a fantastic recipe for that DevOps user who wants an integrated solution, and that's certainly what we're laser-focused on for the next further months here, is making sure that metrics and traces, which is already part of SignalFx, come together with Splunk. And that's my #1 focus.And then the last thing is, I just -- you probably heard me at other places talk about the consumerization of the enterprise. I'm just really excited about what we're doing on the user interface front and making these products easier to use and much more consumable and adding mobility to the equation as much as possible. That piece to me is really, really important for the DevOps or DevSecOps user in the future. They're going to want to sort of be on the top of all that, wherever they are, whenever they want. And so that combination of the platform, the scale and just the user interface, I just think, makes it an unbeatable equation.
Jason Child: On the cash flow question. Okay. So let me just kind of break it down into 3 pieces. First, I'd say, you mentioned 606. So 606 doesn't have any impact on cash. It's just on the revenue recognition. And that's why I tried to talk about absolute dollars and not as a percentage of revenue. So to try to just keep those apart. I would say, second, if you look at inflow, inflow is going to be -- of course, was driven by the change to this ratable billing approach, which we really have been moving a little bit, but we kind of completed that move in earnest middle of this year. And so while that is not necessarily a huge percentage going from, what, last quarter was 58% to 33%, when you're growing, those percentages make a significant difference.Then I would say that ties to the second point, which is -- or part of the second point, which is we also have an increased amount of ramping bills. So a lot of the big deals that we're doing, because they're paying on either ingestion or vCPU, they want to pay as they're utilizing. They will pay a smaller amount in year 1 and then bigger in year two and then bigger in year 3. And so when you add that on top of going to ratable billing, it pushes more cash out. It takes a little longer for this transition to work itself through.And then the very last piece, I'd say, the third piece is outflows. Our outflows are going to continue where they've been. And that is while we are growing our OpEx, we've been growing it less than revenue, we are going to still continue to grow in a relatively aggressive fashion as we invest into further technology, as Tim and Doug have talked a lot about on the call, as well as, of course, building out the go-to-market function to be able to sell all those technologies. So the fact that we're not cutting back on OpEx to try to show cash earlier. So that's why it really will take us 3 years to get to the cycle. But when we come out, we will have stronger cash flow that we've ever had, more than 3x, one of our best years been in the past.
Operator: Our next question comes from Mark Murphy with JPMorgan.
Mark Murphy: I'll add my congrats. Jason, you had mentioned several mid-8-figure orders with government agencies. I just wanted to clarify. I'm assuming those are, for instance, $50 million total contract value over 3, 4, 5 years. And am I interpreting that -- part of that correctly? Or is that inaccurate?
Jason Child: That's the -- no, they're not that big. I mean they're -- when we say mid-8-figure, not mid- between 8 and 9. But no, so they're definitely, I would say, closer to the $10 million to $20-ish million range rather than the $50 million range.
Mark Murphy: Okay. That's great. The second question is I'm -- I guess I'm a little surprised you're guiding ARR growth to the mid-40s. Next fiscal year, I would have guessed maybe you'd start out something closer to 30% just because it's such a big number, and you're guiding that five quarters out. So I'm just curious, are you seeing something unusual in the pipeline, or is it being lifted? Is that forecast being lifted a bit by this public sector performance? Or is there something else going on that's kind of providing you that level of confidence going out that far?
Jason Child: Well, no, it's just very strong growth. And the way ARR works is it's a kind of a layer cake, if you will, of -- for us, our average duration is about 3 years. So you have the contracts we sold this year, what we sold a year ago and what we sold 2 years ago. And so we know that, next year, roughly about 2/3 is going to be sold going into next year. And we also feel good about what is going to happen next year. And so that actually gives us pretty good confidence into the pipeline and the ramp that we expect to see next year.So yes, so I appreciate that the numbers are large. But it's really -- I mean, honestly, it's just the business has been performing extremely well for quite a while. And I think my -- I've only been here 6 months, but my view would be there hasn't maybe been complete understanding. And I think maybe 606 made it a little confusing since you're pulling forward the license revenue on-prem, and so it made it hard to figure out what was really kind of the annual consumption. And so hopefully, this will provide a better view for just how strong we have been performing and how strong we expect to continue performing.
Operator: Our next question comes from Michael Turits with Raymond James.
Michael Turits: It's maybe a question for Doug and Jason together. On the amount of business that's still on maintenance, you talked about -- and Tim had mentioned this in terms of migrating over to Term or Cloud. How much of a contributor to ARR is that? Is that a meaningful driver for that ARR growth? And just to come back, I think you said that it was 3x. Did you mean that's 3x like you go from 25 in maintenance, to 75 in annual ARR?
Douglas Merritt: You want -- so I was confused at the very beginning of the question.
Michael Turits: So I just want to know if the migrations from maintenance to Term or Cloud is part of that driver for ARR.
Douglas Merritt: Got it. Got it. Yes. So that's what I thought you're saying. I want to make sure it's in the same page. So why I think Jason is leading with that 5% number is there could easily be a perception that, that strong ARR growth was us just converting our perpetual contracts to term contracts and not expanding our footprint within accounts, not forming new use cases and not forming new customers. And that -- of the ARR number, only 5% of our perpetual customers have transitioned off of perpetual into term.And for me, just going back to what Jason just said and as a response to, I think, Kash's question, that's the indicator of the core strength of the underlying business. We are continuing to help our customers find new use cases, penetrate new departments and continue to ensure that the new customers we bring on have that same great cohort analysis that we've seen. And I think that's why he leaned in on that number to make sure that people were not confused or that they got the purity of that ARR number.
Jason Child: Right. That's exactly right. Doug gets it. Yes. No, I guess I would add to the -- I mean if you kind of break down the $1.45 billion -- roughly $1.45 billion, and we said about $369 million of it is cloud, the remaining 1 point -- kind of $1-ish billion or so of that, about 35% of it or so is maintenance. And so -- and then a portion of that is term and a portion of that is perpetual. So there's -- I don't know if that helps kind of provide the magnitude, but that's how it breaks down.
Michael Turits: Okay. And then, Doug, when the customers that have seen the new -- that have adopted new pricing, I know it's early stage, but do we have some sense for the elasticity there? In other words, when we're going to what's essentially a lower unit price, is that driving more consumption such that you're getting an increase in total contract value?
Douglas Merritt: Yes. It's a good question. So the way that our price curve has always worked is the higher volume you get, the lower per-gigabyte fee there is. And what we've done with a banded pricing versus infrastructure pricing is really extend that curve out much more visibly all the way to an unlimited framework. So there's always been an advantage to continue more data. What we -- the key thing with the banding was term helped people feel like it was more affordable because we were stretching out payments basically for higher volumes over 3 years. So we saw, as we converted from perp to term, people are now feeling more comfortable moving from 100 gig to 500 gig.I think what the banding does, and that's just a figurative example, I think the banding does is make you feel more comfortable to go from 1 terabyte to 20 terabytes, which correlates back to the volume of data and the volume of use cases that are out there. So that what I have seen with the -- not -- it's not prolific yet, so we have few data points, probably less than 50 customers total that have converted to either infrastructure or the new predictive price framework, is that a lot like our cohort as they consume in a new way, they are dramatically increasing their footprint with Splunk. I haven't seen anyone stay flat or go down yet.
Operator: Our next question comes from Keith Bachman with BMO.
Keith Bachman: The first question I'll throw out there is on the RPO side. Both the current and the total RPO had a pretty meaningful snapback. And I just wanted to see if there's anything you wanted to call out. And/or is there anything that we should be thinking about for both RPOs as we think about Q4?
Jason Child: I'll take that. This is Jason. I would say the -- well, RPO, at 42%, RPO bookings at 42% up versus 17% prior period -- or I'm sorry, 19% total of 17% current RPO bookings. On the 19%, remember, that was roughly 400 basis points of this kind of onetime impact, so it was really more like 23%. This quarter, at 42%, if you took out SignalFX, which we acquired last quarter and therefore consolidated into RPO, that would have taken it down to about 35% on RPO bookings.Now still very strong growth and still a pretty significant acceleration from last quarter. The one thing about RPO bookings. It is what we're selling within the quarter. We're now so -- we have a lot of large deals. We talked about the 8-figure deals. Those can move -- just 1 or 2 deals that shifted 1 quarter versus the next can cause some lumpiness in RPO bookings. And that's why I think it's a good number to look at but not to overreact too much one way or the other. That is why I really like ARR as the durable growth metric that is going to even out some of that volatility that you'll see with RPO bookings.
Keith Bachman: Okay. Fair enough. Doug, the last one for me for you is similar to the previous question. But as you've gotten more data, what do you think the impact on the pricing changes are to Splunk? Because in many ways, if you're going from consumption to core, they are highly correlated. And have you had any more incremental thoughts, not even out talking to customers about what your opportunities to upsell are during this process of transition or potential transition, I should say?
Douglas Merritt: Yes. I think that upsell motion really did start two years ago, that the Splunk Enterprise indexing upsell motions started two years ago when we leaned in much more aggressively with Term and Cloud, again, because the ease of expansion, I think, went up. First of all, we have a perpetual motion. What I've seen consistently in all of my customer conversations is -- which is, again, why I think there's been so much sensitivity around the volume, data volume-based pricing, is that there is so much opportunity and appetite for data out there.Now the interesting thing about the portfolio. So there's a very purposeful pricing to make it easier for people to get very high volumes of data in the index and not feel like they were out of control on the costs that will go with it. But what's interesting about the portfolio and what Tim and I have been talking about for 2 years now is the data at rest and the index is really important and very valuable. But it's got to be complemented by -- if you want to be a Data-to-Everything platform, you need the complement of being able to add value to streaming data and being able to go after a wide variety of data at rest and not just data that has to be in the Splunk index. And then you add in the automation orchestration response capabilities, so that act pillar that centered around the Phantom acquisition.And that, really, when I think out the next 2, 3 years, the activity of expansion of Splunk Enterprise index is awesome, and our reps are doing a really good job there. And I think there's a lot of value for customers when that happens. And then we begin to add in DSP, DFS, orchestration, automation and then some of the apps on top, like the developer portfolio with SignalFx, Omnition, et cetera, revamped ITSI, the Mission Control, plus CS, plus Phantom, plus UBA. That's really the multiproduct, high-attached, continued growth lever journey that we are looking forward to and you guys should be taking a look at as well.
Operator: And our next question comes from Andrew Nowinski with D.A. Davidson.
Andrew Nowinski: I just wanted to ask you a question on, I guess, international revenue looked like it modestly decelerated. Was that just Brexit-related? Or were there any execution issues in the region?
Douglas Merritt: It's the major -- the major driver of that was the overperformance of public sector, which is, for us, that we have -- we sold to governments around the world. That public sector number is the U.S. government and which those goes back into the U.S. number versus international.And then we are very focused on continued execution capability and cadence internationally, both in EMEA and APAC. And as we've all talked about in that mid- to high 20s, there's no reason that can't be mid- to high 30s or mid- to high 40s. So there's a lot of work that we've got to do to make sure that, that happens. But the opportunity across the rest of the world is that data is a neutral player. Governments, commercial organizations, companies located in every country need to take advantage of data. So there is ample opportunity. Everywhere we set them, we've got to make sure that we can pursue that effectively.
Andrew Nowinski: Okay. And then last one for me. Obviously, current RPO was very strong. And I know you get a lot of 8-figure deals, but the new logo adds, I guess, were down on a year-over-year basis. And given your sales capacity is likely higher now versus last year, I was wondering why would that be down and whether that might be attributable to your licensing changes.
Douglas Merritt: I don't think it's attributable to licensing changes. It's obviously something that we've been very focused on, and we have -- and as I've talked about, I'm frustrated by how stubborn it -- that metric is. We have not seen the exponential growth that we know exists. What Susan and Christian, her global head of sales and I keep circling around is there's so much opportunity within the accounts where there's clear momentum, that on a time and attention basis, so reps would end up spending much more time there than they do prospecting. We did carve out in that new team. Those -- they have been here for 2 quarters now, 2-plus quarters. They're starting to get ramped. We are doing a whole multitude of different acts, including portfolio enhancements, easier consumption through cloud, et cetera, to move that number. But I don't think there's nothing that any of us can find that's foundational other than time and attention and some of the deliveries that we've been focused on that will eventually go there.
Operator: And our final question comes from Chris Merwin with Goldman Sachs.
Christopher Merwin: Do you mind just updating us where you are in the process? I know it's still very early, but the process of integrating SignalFx, do you plan to bring to market a fully integrated solution when you start to try to cross-sell that with the existing log products? Just curious on any updates there.
Timothy Tully: Yes. This is Tim. So in the short term, what you're going to see is we're taking the Omnition acquisition and folding it directly into the SignalFx user experience. And so you're going to see a combined metrics and traces product in the observability suite in a very short time here, early -- sometime in Q1. And then the sort of advancements around pulling together the rest of logs from Splunk with the rest of the observability suite will be sometime in middle of next year. That being said, that would be the collapsed user interface. We will already be there with the linking between the 2. So you're just a link click away from really going back and forth between the 2 products.
Operator: And that's all the time we have for questions. I'd like to turn the call back to Mr. Ken Tinsley for any closing remarks.
Ken Tinsley: Great. Thank you, Catherine. I appreciate your help today. And thanks, everybody, for joining us. Have a great night.
Operator: Ladies and gentlemen, this concludes today's conference call. Thank you for participating. You may now disconnect. Everyone, have a great day."
0.000741
