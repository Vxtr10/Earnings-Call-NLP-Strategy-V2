{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (1.22.4)\n",
      "Requirement already satisfied: filelock in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (0.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (21.3)\n",
      "Requirement already satisfied: requests in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from transformers==4.25.1) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.25.1) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.25.1) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages (from requests->transformers==4.25.1) (1.26.10)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.0.dev0\n",
      "    Uninstalling transformers-4.26.0.dev0:\n",
      "      Successfully uninstalled transformers-4.26.0.dev0\n",
      "Successfully installed transformers-4.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.25.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using finbert (91% accuracy) instead of Loughran and McDonalds (61% accuracy)\n",
    "https://towardsdatascience.com/how-nlp-has-evolved-for-financial-sentiment-analysis-fb2990d9b3ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a â\\x80\\x93 I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's â\\x80\\x93 and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\\n\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"\"\"\n",
    "I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a â I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's â and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\n",
    "\"\"\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a  I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.  And it's  and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\""
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr2 = re.sub(r'[^A-Za-z0-9.:,!\\' ]', '', mystr) #removes special characters such as the â from above\n",
    "mystr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I mean, I can comment a little bit about it. I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency. And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation. And it's and again, it's not just for us, it's for all of our competitors as well. They are all seeing the same deterioration.\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr3 = re.sub('\\s+', ' ', mystr2) #removes multiple spaces to a single space (i.e. there is a  I --> there is a I)\n",
    "mystr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.9691150188446045}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)\n",
    "results = sentiment_nlp(mystr3)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenise string to individual sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I mean, I can comment a little bit about it.',\n",
       " \"I mean, the corridor that we did very well in with Cuba and there is a I don't know how else to explain it, but there's a black market currency and a regular currency.\",\n",
       " \"And people are basically choosing to do business in cash in Cuba because they can buy way more on the black market versus paying for things here, where we have to obviously not do that and that's really the situation.\",\n",
       " \"And it's and again, it's not just for us, it's for all of our competitors as well.\",\n",
       " 'They are all seeing the same deterioration.']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystrtok = (nltk_tokenizer.tokenize(mystr3))\n",
    "mystrtok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLS-Classification:\n",
    "\n",
    "Forward-looking statements (FLS) inform investors of managers’ beliefs and opinions about firm's future events or results. Identifying forward-looking statements from corporate reports can assist investors in financial analysis. FinBERT-FLS is a FinBERT model fine-tuned on 3,500 manually annotated sentences from Management Discussion and Analysis section of annual reports of Russell 3000 firms.\n",
    "\n",
    "There are 3 classifications of FLS:\n",
    "Specific foward looking statment  (S-FLS)\n",
    "Non-specific foward looking statment (NS-FLS)\n",
    "Not foward looking statment (N-FLS)\n",
    "\n",
    "We are trying to take the average sentiments of all different classes of FLS statements.\n",
    "E.g.\n",
    "4 sentences that are classed as S-FLS, with sentiments [0.6, 0.7, 0.8, 0.9]\n",
    "so the average sentiment is 0.75\n",
    "--> Hence, repeat for NS-FLS and NFLS\n",
    "\n",
    "But first, I need to change the sentiment into a single numerical value\n",
    "i.e. {'label': 'Negative', 'score': 0.5338950753211975} --> single value\n",
    "\n",
    "so for \n",
    "sentence1 = {'label': 'Negative', 'score': 0.5338950753211975}\n",
    "sentence2 = {'label': 'Neutral', 'score': 0.999138355255127}\n",
    "sentence3 = {'label': 'Positive', 'score': 0.9999885559082031}\n",
    "\n",
    "negative can be mapped to -1\n",
    "neutral to 0\n",
    "positive to 1\n",
    "\n",
    "the sentiment score probability is multiplied by their -1, 0 or 1\n",
    "\n",
    "so:\n",
    "sentence1 = -0.5338950753211975\n",
    "sentence2 = 0\n",
    "sentence 3 = 0.9999885559082031\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "given \n",
    "[{'label': 'Not FLS', 'score': 0.9779328107833862},\n",
    " {'label': 'Not FLS', 'score': 0.9796538949012756},\n",
    " {'label': 'Specific FLS', 'score': 0.8796855211257935}]\n",
    "\n",
    "and given\n",
    "[{'label': 'Negative', 'score': 0.5338950753211975},\n",
    " {'label': 'Neutral', 'score': 0.999138355255127},\n",
    " {'label': 'Positive', 'score': 0.9999885559082031}]\n",
    "\n",
    "Not FlS score = (-0.5339+0)/2 = -0.26695\n",
    "Specific FLS score = 0.99998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLS classification\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marcelo Fischer : Just to correct the income from operations, the loss from operations for net2phone was minus $1.8 billion.',\n",
       " 'Of EBITDA itself was about minus $100,000, okay?',\n",
       " 'So we are getting closer to reaching EBITDA profitability and based on our 2023 projections, we do hope to exit 2023 with net2phone being an EBITDA-positive company, even as it continues to grow at rates which are probably higher than the average UCaaS play on basically.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltkstr = \"Marcelo Fischer : Just to correct the income from operations, the loss from operations for net2phone was minus $1.8 billion. Of EBITDA itself was about minus $100,000, okay? So we are getting closer to reaching EBITDA profitability and based on our 2023 projections, we do hope to exit 2023 with net2phone being an EBITDA-positive company, even as it continues to grow at rates which are probably higher than the average UCaaS play on basically.\"\n",
    "\n",
    "nltkstr= (nltk_tokenizer.tokenize(nltkstr))\n",
    "nltkstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Not FLS', 'score': 0.9779328107833862},\n",
       " {'label': 'Not FLS', 'score': 0.9796538949012756},\n",
       " {'label': 'Specific FLS', 'score': 0.8796855211257935}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#performs FLS-classifications on the above 3 sentences\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)\n",
    "fls_results = fls_nlp(nltkstr)\n",
    "fls_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Negative', 'score': 0.5338950753211975},\n",
       " {'label': 'Neutral', 'score': 0.999138355255127},\n",
       " {'label': 'Positive', 'score': 0.9999885559082031}]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_results = sentiment_nlp(nltkstr)\n",
    "sent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'Negative', 'score': 0.5338950753211975}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiments:\n",
      "Not FLS: -0.26694753766059875\n",
      "Specific FLS: 0.9999885559082031\n",
      "Not-specific FLS: 0\n"
     ]
    }
   ],
   "source": [
    "def map_sentiments(sent_results, i):\n",
    "    if sent_results[i]['label'] == 'Negative':\n",
    "        return -1*sent_results[i]['score']\n",
    "    \n",
    "    elif sent_results[i]['label'] == 'Neutral':\n",
    "        return 0\n",
    "    \n",
    "    elif sent_results[i]['label'] == 'Positive':\n",
    "        return sent_results[i]['score']\n",
    "\n",
    "n_flslist = []\n",
    "s_flslist = []\n",
    "ns_flslist = []\n",
    "for i in range(0,len(fls_results)):\n",
    "    if fls_results[i]['label'] == 'Not FLS':\n",
    "        n_flslist.append(map_sentiments(sent_results, i))\n",
    "    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "        s_flslist.append(map_sentiments(sent_results, i))\n",
    "    elif fls_results[i]['label'] == 'Non-specific FLS':\n",
    "        ns_flslist.append(map_sentiments(sent_results, i))\n",
    "\n",
    "def average_of_list(mylist):\n",
    "    try:\n",
    "        return sum(mylist)/len(mylist)\n",
    "    except ZeroDivisionError: #if empty list, return 0 (assumes to be neutral)\n",
    "        return 0\n",
    "\n",
    "\n",
    "n_fls = average_of_list(n_flslist)\n",
    "s_fls = average_of_list(s_flslist)\n",
    "ns_fls = average_of_list(ns_flslist)\n",
    "\n",
    "print(\"Sentiments:\")\n",
    "print(\"Not FLS:\", n_fls)\n",
    "print(\"Specific FLS:\", s_fls)\n",
    "print(\"Not-specific FLS:\", ns_fls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather a list of all speaker and analyst names for each stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_speaker_name(mytranscript, speaker_name_list):\n",
    "    for sentence in mytranscript:\n",
    "        # text clean up\n",
    "        sentence = re.sub(r'[^A-Za-z0-9.,:!\\' ]', '', sentence)\n",
    "        sentence = re.sub('\\s+', ' ', sentence) \n",
    "\n",
    "        # finds the speaker:\n",
    "        colon_pos = sentence.find(\":\")\n",
    "        speaker_name = sentence[:colon_pos]\n",
    "\n",
    "        # appends speaker name to speaker_name_list:\n",
    "        speaker_name_list.append(speaker_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_speaker_names(sector, stock):\n",
    "    speaker_name_list = []\n",
    "    sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[1-9]**[1-9]*[1-4].*')\n",
    "    sector_files.sort(reverse=True)\n",
    "    for path in sector_files: # for every path of the stock\n",
    "        mytranscript = pd.read_csv(path).iloc[[2]].values[0][0] \n",
    "        mytranscript = mytranscript.splitlines() # finds transcript\n",
    "        find_speaker_name(mytranscript, speaker_name_list) # finds all speaker names\n",
    "    speaker_name_list = list(set(speaker_name_list)) #removes all speaker names duplicates \n",
    "    write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "    if not os.path.exists(write_path):\n",
    "        np.savetxt(write_path, speaker_name_list, delimiter =\", \", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automobiles sector start ................................................................\n",
      "XPEL start\n",
      "SUCCESS\n",
      "THRM start\n",
      "SUCCESS\n",
      "APTV start\n",
      "SUCCESS\n",
      "XPEV start\n",
      "SUCCESS\n",
      "WGO start\n",
      "SUCCESS\n",
      "CPS start\n",
      "SUCCESS\n",
      "LAZR start\n",
      "SUCCESS\n",
      "GT start\n",
      "SUCCESS\n",
      "AYRO start\n",
      "SUCCESS\n",
      "MPAA start\n",
      "SUCCESS\n",
      "DAN start\n",
      "SUCCESS\n",
      "TSLA start\n",
      "SUCCESS\n",
      "RIDE start\n",
      "SUCCESS\n",
      "QS start\n",
      "SUCCESS\n",
      "SRI start\n",
      "SUCCESS\n",
      "GNTX start\n",
      "SUCCESS\n",
      "ALV start\n",
      "SUCCESS\n",
      "F start\n",
      "SUCCESS\n",
      "CAAS start\n",
      "SUCCESS\n",
      "ARVL start\n",
      "SUCCESS\n",
      "SUP start\n",
      "SUCCESS\n",
      "BWA start\n",
      "SUCCESS\n",
      "FOXF start\n",
      "SUCCESS\n",
      "REE start\n",
      "SUCCESS\n",
      "VC start\n",
      "SUCCESS\n",
      "TEN start\n",
      "SUCCESS\n",
      "GOEV start\n",
      "SUCCESS\n",
      "HOG start\n",
      "SUCCESS\n",
      "LEA start\n",
      "SUCCESS\n",
      "NIU start\n",
      "SUCCESS\n",
      "KNDI start\n",
      "SUCCESS\n",
      "RACE start\n",
      "SUCCESS\n",
      "SMP start\n",
      "SUCCESS\n",
      "NIO start\n",
      "SUCCESS\n",
      "GM start\n",
      "SUCCESS\n",
      "AXL start\n",
      "SUCCESS\n",
      "LI start\n",
      "SUCCESS\n",
      "GTX start\n",
      "SUCCESS\n",
      "PATK start\n",
      "SUCCESS\n",
      "LCID start\n",
      "SUCCESS\n",
      "FUV start\n",
      "SUCCESS\n",
      "WKHS start\n",
      "SUCCESS\n",
      "MOD start\n",
      "SUCCESS\n",
      "UFAB start\n",
      "SUCCESS\n",
      "FSR start\n",
      "SUCCESS\n",
      "SYPR start\n",
      "SUCCESS\n",
      "HZN start\n",
      "SUCCESS\n",
      "ADNT start\n",
      "SUCCESS\n",
      "RIVN start\n",
      "SUCCESS\n",
      "XL start\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "\"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "\"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "\"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "\"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "for sector in sectorlist:\n",
    "    print(sector, \"sector start\")\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    for stock in filelist:\n",
    "        find_all_speaker_names(sector, stock)\n",
    "    print(sector, \"sector complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "find_all_speaker_names(\"telecom\",\"IDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_name_list = list(set(speaker_name_list)) #removes all duplicates \n",
    "\n",
    "write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "\n",
    "if not os.path.exists(write_path):\n",
    "    np.savetxt(write_path, speaker_name_list, delimiter =\", \", fmt ='% s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_name_list = list(set(speaker_name_list)) #removes all duplicates \n",
    "\n",
    "write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "\n",
    "if not os.path.exists(write_path):\n",
    "    np.savetxt(write_path, speaker_name_list, delimiter =\", \", fmt ='% s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the transcript:\n",
    "1. into pre-release\n",
    "2. into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather list of all speaker names for every stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.spglobal.com/marketintelligence/en/news-insights/blog/analyzing-sentiment-in-quarterly-earnings-calls-q2-2022\n",
    "\n",
    "\n",
    "https://www.amenityanalytics.com/case-studies/earnings-call-transcript-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whole transcript --> net sentiment\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "Pre release:\n",
    "Whole pre-release --> net sentiment, word complex\n",
    "\n",
    "Specific foward looking statment (aggregate to a single string) --> one sentiment, word complex\n",
    "Non Specific Forward looking statement (aggregate) --> one sentiment, word complex\n",
    "Not Foward looking statement (aggregate) --> one sentiment, word complex\n",
    "------------------------------------------------------------------------------------------------\n",
    "Q&A:\n",
    "Whole Q&A --> net sentiment, word complex\n",
    "\n",
    "all question (aggregate)--> sentiment, word complex\n",
    "all reply (aggregate) --> sentiment, word complex\n",
    "\n",
    "[if none below, return 0]\n",
    "Specific foward looking statment --> one sentiment, word complex\n",
    "Non Specific Forward looking statement --> one sentiment, word complex\n",
    "Not Foward looking statement --> one sentiment, word complex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF IDF value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired from https://www.spglobal.com/marketintelligence/en/news-insights/blog/analyzing-sentiment-in-quarterly-earnings-calls-q2-2022\n",
    "\n",
    "USE DICTIONARY??\n",
    "\n",
    "able to search for the current quarter sentiment\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "[on a seperate .csv file in every sector folder]\n",
    "\n",
    "[PER QUARTER]:\n",
    "\n",
    "average of net sentiment of each sector in that quarter\n",
    "average net word complexity of each sector in that quarter\n",
    "\n",
    "average sentiment of each sector (from forward looking statements)\n",
    "average sentiment of each sector (from non specific forward looking statements)\n",
    "average sentiment of each sector (from forward looking statements)\n",
    "\n",
    "average TF-IDF change (similarity change) in that quarter\n",
    "\n",
    "number of firms that beat EPS\n",
    "number of firms that didn't beat EPS\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
