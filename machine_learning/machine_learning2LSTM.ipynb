{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv\n",
    "import statistics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_types(mydf):\n",
    "    if mydf.isnull().values.any():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "X_data = []\n",
    "Y_data = []\n",
    "\n",
    "sectors = [\"automobiles\", \"banks\"]\n",
    "for sector in sectors:\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for stock in filelist:\n",
    "        sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "        sector_files.sort(reverse=True)\n",
    "\n",
    "        testpath = sector_files[0]\n",
    "        testdata_from_csv = pd.read_csv(testpath)\n",
    "        try:\n",
    "            market_cap = float(testdata_from_csv.iloc[[-1]].values[0][0])\n",
    "            if (market_cap > 500000000):\n",
    "                number_to_loop = len(sector_files)\n",
    "                if number_to_loop%3 != 0:\n",
    "                    number_to_loop = number_to_loop-(number_to_loop%3) #gets nearest (inclusive) multiples of 3\n",
    "                for i in range(0, number_to_loop+2, 3): # for every .csv path of that stock\n",
    "                    path = sector_files[i]\n",
    "                    path2 = sector_files[i+1]\n",
    "                    path3 = sector_files[i+2]\n",
    "                    data_from_csv = pd.read_csv(path)\n",
    "                    data_from_csv2 = pd.read_csv(path2)\n",
    "                    data_from_csv3 = pd.read_csv(path3)\n",
    "                    Checks_None = remove_none_types(data_from_csv)\n",
    "                    Checks_None2 = remove_none_types(data_from_csv2)\n",
    "                    Checks_None3 = remove_none_types(data_from_csv3)\n",
    "                    if (Checks_None == True) and (Checks_None2 == True) and (Checks_None3 == True):\n",
    "                        single_X_data = (np.array((data_from_csv[3:85].astype(float))).flatten()) \n",
    "                        single_X_data2 = (np.array((data_from_csv2[3:85].astype(float))).flatten()) \n",
    "                        single_X_data3 = (np.array((data_from_csv3[3:85].astype(float))).flatten()) \n",
    "                        if ((single_X_data.shape[0]) == 82) and ((single_X_data2.shape[0]) == 82) and ((single_X_data3.shape[0]) == 82):\n",
    "                            X_data.append(single_X_data)\n",
    "                            X_data.append(single_X_data2)\n",
    "                            X_data.append(single_X_data3)\n",
    "                            single_Y_data = np.array(data_from_csv[85:95].astype(float)).flatten()\n",
    "                            Y_data.append(single_Y_data)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "#[\n",
    "#   [#AAPL\n",
    "#       [ [earnings call 1], [earnings call 2], [earnings call 3]\n",
    "#   ],\n",
    "#   [#MSFT\n",
    "#       [ [earnings call 1], [earnings call 2], [earnings call 3]\n",
    "#   ]\n",
    "#\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "ORI_X_data = np.array(X_data)\n",
    "\n",
    "ORI_Y_data = np.array(Y_data)\n",
    "# ORI_Y_data = ORI_Y_data[:,:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_types(mydf):\n",
    "    if mydf.isnull().values.any():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "X_data = []\n",
    "Y_data = []\n",
    "\n",
    "sectors = [\"automobiles\", \"banks\"]\n",
    "for sector in sectors:\n",
    "    filelist = os.listdir(\"sectors/\"+sector)\n",
    "    try:\n",
    "        filelist.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for stock in filelist:\n",
    "        sector_files = glob.glob('sectors/'+str(sector)+'/'+str(stock)+'/'+str(stock)+'20*[0-9]**[0-9]*[1-4].*')\n",
    "        sector_files.sort(reverse=True)\n",
    "\n",
    "        testpath = sector_files[0]\n",
    "        testdata_from_csv = pd.read_csv(testpath)\n",
    "        try:\n",
    "            market_cap = float(testdata_from_csv.iloc[[-1]].values[0][0])\n",
    "            if (market_cap > 5000000000):\n",
    "                number_to_loop = len(sector_files)\n",
    "                if number_to_loop%2 != 0:\n",
    "                    number_to_loop = number_to_loop-(number_to_loop%2) \n",
    "                for i in range(0, number_to_loop+1, 2): # for every .csv path of that stock\n",
    "                    path = sector_files[i]\n",
    "                    path2 = sector_files[i+1]\n",
    "                    data_from_csv = pd.read_csv(path)\n",
    "                    data_from_csv2 = pd.read_csv(path2)\n",
    "                    Checks_None = remove_none_types(data_from_csv)\n",
    "                    Checks_None2 = remove_none_types(data_from_csv2)\n",
    "                    if (Checks_None == True) and (Checks_None2 == True):\n",
    "                        single_X_data = (np.array((data_from_csv[3:85].astype(float))).flatten()) \n",
    "                        single_X_data2 = (np.array((data_from_csv2[3:85].astype(float))).flatten()) \n",
    "                        if ((single_X_data.shape[0]) == 82) and ((single_X_data2.shape[0]) == 82):\n",
    "                            X_data.append(single_X_data)\n",
    "                            X_data.append(single_X_data2)\n",
    "                            single_Y_data = np.array(data_from_csv[85:95].astype(float)).flatten()\n",
    "                            Y_data.append(single_Y_data)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "#[\n",
    "#   [#AAPL\n",
    "#       [ [earnings call 1], [earnings call 2], [earnings call 3]\n",
    "#   ],\n",
    "#   [#MSFT\n",
    "#       [ [earnings call 1], [earnings call 2], [earnings call 3]\n",
    "#   ]\n",
    "#\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "ORI_X_data = np.array(X_data)\n",
    "\n",
    "ORI_Y_data = np.array(Y_data)\n",
    "# ORI_Y_data = ORI_Y_data[:,:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05746078,  1.61578883,  1.19836207, ...,  0.18097152,\n",
       "         0.97246949,  0.17851685],\n",
       "       [-0.00547779,  1.27385356,  1.73668944, ...,  0.73719168,\n",
       "        -0.71082599, -0.03991479],\n",
       "       [ 0.03978639,  1.04040048,  0.97985731, ..., -0.26304118,\n",
       "         0.72942698,  1.22779005],\n",
       "       ...,\n",
       "       [ 0.0239148 ,  0.12939548,  0.47761612, ...,  0.76529605,\n",
       "         0.96407242,  0.95758538],\n",
       "       [ 0.09939671,  0.33414797,  0.48577949, ...,  1.70478436,\n",
       "         1.08904976,  0.31056905],\n",
       "       [ 0.02159196,  0.73154351,  1.11011976, ...,  1.07723495,\n",
       "         0.12043322,  1.63181319]])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(ORI_X_data)\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_data.reshape(-1,2,82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18715475,  0.11144807, -0.00389906, ..., -0.02821215,\n",
       "         0.05969993,  0.16895602],\n",
       "       [-0.11340496,  0.01913649, -0.14375942, ..., -0.13056379,\n",
       "        -0.16138605,  0.00708335],\n",
       "       [-0.05403898, -0.07270197, -0.09632308, ..., -0.09712789,\n",
       "        -0.09952134, -0.23015699],\n",
       "       ...,\n",
       "       [ 0.03036057,  0.08349139,  0.12142812, ...,  0.08838353,\n",
       "         0.05637264, -0.04335492],\n",
       "       [ 0.00063696,  0.05286605,  0.06496794, ...,  0.0690536 ,\n",
       "         0.12612933,  0.08829283],\n",
       "       [ 0.0246459 ,  0.06531101,  0.05668503, ...,  0.09865488,\n",
       "         0.14555631,  0.09351488]])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data = ORI_Y_data.reshape(-1,10)\n",
    "Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.18715475,  0.11144807, -0.00389906,  0.08621249,  0.19820205],\n",
       "       [-0.11340496,  0.01913649, -0.14375942, -0.17411388, -0.00820138],\n",
       "       [-0.05403898, -0.07270197, -0.09632308, -0.09871867, -0.22947077],\n",
       "       ...,\n",
       "       [ 0.03036057,  0.08349139,  0.12142812,  0.08844501, -0.01431067],\n",
       "       [ 0.00063696,  0.05286605,  0.06496794,  0.12182607,  0.08413365],\n",
       "       [ 0.0246459 ,  0.06531101,  0.05668503,  0.10179527,  0.05174122]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data = Y_data[:,:5]\n",
    "Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.05746078,  1.61578883,  1.19836207, -0.94898107,\n",
       "         -1.7928399 , -0.43349364, -0.34921608,  0.36061076,\n",
       "         -0.9799415 ,  0.04178318, -0.65343353, -0.68321556,\n",
       "         -0.78907642,  1.88950502,  1.44990238, -1.07781823,\n",
       "         -2.48825064,  0.40898008, -0.38844046, -1.09370378,\n",
       "         -0.45283743,  1.42538434,  0.54283879, -1.06638608,\n",
       "          0.02918357,  1.81081021,  0.9372234 ,  0.90189923,\n",
       "          0.90652208, -0.23034271, -1.0792426 , -0.12845517,\n",
       "         -0.66158995, -0.89173257, -0.95332343, -1.23395959,\n",
       "          0.59529097,  1.16821967,  1.28754962, -0.20256845,\n",
       "         -0.44954612, -0.06943422,  0.95426796,  1.08577778,\n",
       "          1.17066863, -0.34951063,  0.71465049,  0.56516673,\n",
       "         -0.59115395,  0.01474719, -0.52396011, -0.9403486 ,\n",
       "         -0.28894902, -0.04152869,  0.70111423, -0.51731615,\n",
       "          0.3053057 ,  0.        ,  1.31697272,  1.67113009,\n",
       "         -0.29380625, -0.51080982,  0.28828981,  1.81676084,\n",
       "          0.62616457,  0.50478739, -0.38910694, -0.59879491,\n",
       "         -0.78726569, -0.32315839, -1.37644537,  0.62028254,\n",
       "         -0.31020537,  0.41401008,  0.75725444,  0.7254439 ,\n",
       "         -3.34771953, -0.63937324, -0.16276364,  0.18097152,\n",
       "          0.97246949,  0.17851685],\n",
       "        [-0.00547779,  1.27385356,  1.73668944,  0.59414091,\n",
       "         -1.63512644,  0.59976729,  0.73513628,  0.17542243,\n",
       "         -0.47318107,  2.52079051,  2.64225092, -0.68321556,\n",
       "         -0.07085997,  1.1386663 ,  1.68575907,  0.70855111,\n",
       "         -2.62471703,  0.86648879, -0.24250165, -0.37364147,\n",
       "          0.52129252,  1.38310418,  0.46639811,  0.15167617,\n",
       "          1.3367935 ,  1.45004794,  0.498839  ,  0.14835355,\n",
       "          0.73211969,  0.71912331,  0.21953569,  1.76602686,\n",
       "          1.55649838, -0.89173257,  2.22584043,  2.0345713 ,\n",
       "         -1.03571613, -0.02068319,  0.66520596,  0.94017398,\n",
       "          1.1409376 ,  0.8330544 , -1.28591917,  1.25030447,\n",
       "          1.50675831, -0.26981418,  0.29701061,  0.49216184,\n",
       "          0.15609616,  0.5231321 ,  0.76554755,  0.21799087,\n",
       "          0.40911685,  0.68986582,  0.4043966 ,  1.74461472,\n",
       "          1.71688324,  0.        ,  0.99556911,  0.69978319,\n",
       "         -0.94579788,  0.08399758,  0.48915895,  0.88736799,\n",
       "          1.80739897,  1.88406153, -0.38910694, -0.59879491,\n",
       "         -0.78726569, -0.32315839, -0.61110901, -1.82143923,\n",
       "         -1.55977012,  0.66906977,  0.32596975,  0.29401365,\n",
       "         -3.24347514, -2.24455535, -2.70787765,  0.73719168,\n",
       "         -0.71082599, -0.03991479]],\n",
       "\n",
       "       [[ 0.03978639,  1.04040048,  0.97985731, -0.3810253 ,\n",
       "         -2.0595137 , -0.66421539, -0.44195675,  0.61402637,\n",
       "         -0.69667371, -2.02405651, -0.65343353,  1.64185591,\n",
       "         -0.89146524,  1.37435071,  1.21857977, -0.64222479,\n",
       "         -2.59472441,  0.56505145, -0.53617345,  0.02235035,\n",
       "          0.1203634 ,  0.43516989, -1.40333559,  0.52991756,\n",
       "          1.09732054,  0.83665887, -0.93084039,  0.5149789 ,\n",
       "          0.23382715, -0.36971387, -1.45763773,  1.00824391,\n",
       "          0.66926305, -0.89173257,  0.93125053,  0.945061  ,\n",
       "          0.32345646,  0.47470487,  0.24206898, -0.36307182,\n",
       "          0.27968055, -0.2772441 , -0.96855933,  0.05924059,\n",
       "          0.24642202,  0.12866805,  0.90365743,  0.79173365,\n",
       "         -0.60733339, -0.94485298, -0.69904084,  0.67430642,\n",
       "          1.18440337,  0.96264699, -0.96929616,  0.4520482 ,\n",
       "          0.3053057 ,  0.        ,  0.92125544,  0.95161387,\n",
       "         -0.56546943,  0.21934114,  0.02046429, -0.413782  ,\n",
       "          0.89990505,  0.82308142, -0.38910694, -0.59879491,\n",
       "         -0.78726569, -0.32315839, -0.38573824, -0.21957742,\n",
       "          0.52234278,  0.3885683 ,  0.5125246 ,  0.60626086,\n",
       "         -0.13948801, -0.10568445, -0.10579096, -0.26304118,\n",
       "          0.72942698,  1.22779005],\n",
       "        [ 0.03826094,  1.68555763,  1.87833077,  0.00766442,\n",
       "         -1.75700928,  0.33914241,  0.65768254,  0.45887395,\n",
       "         -0.29119895, -1.79663718, -0.65343353,  2.80439165,\n",
       "         -1.59809756,  1.8111782 ,  1.90154956, -0.25634587,\n",
       "         -2.60972072,  2.2298627 , -0.92822   ,  0.09772591,\n",
       "         -0.20236418, -0.58057894,  0.57518541,  0.14207501,\n",
       "         -0.01120816, -0.40681666,  1.03561491,  0.20128685,\n",
       "         -0.04646241, -0.44593248, -0.05647504, -0.28801757,\n",
       "          0.00383655,  0.32657465,  1.54926656,  1.21743858,\n",
       "         -1.03571613,  0.16987664, -0.0402911 , -0.32995208,\n",
       "          0.77169172,  0.68623807, -0.5934977 , -0.24487565,\n",
       "          0.26853318,  0.82076876,  0.68545318,  0.76733413,\n",
       "         -0.21819728,  0.2576478 ,  0.30721264, -0.00554832,\n",
       "          0.7979098 ,  0.98709436,  0.01561563,  1.7448556 ,\n",
       "          1.71688324,  0.        ,  1.04036854,  1.0178851 ,\n",
       "         -0.70559044,  1.03799534,  0.95785361, -0.413782  ,\n",
       "          2.26860237,  2.41455158, -0.38910694, -0.59879491,\n",
       "         -0.78726569, -0.32315839, -0.33025152,  0.29049117,\n",
       "         -0.08290607,  0.36060101,  0.18045703, -0.45619033,\n",
       "         -0.50230007, -0.0979894 , -1.09676325,  0.13619361,\n",
       "         -0.38571758, -0.30042487]]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 2, 82)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(423, 5)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245, 5)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.42, random_state=42)\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victor/miniforge3/envs/NLP_env/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2023-02-08 09:11:45.867352: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:46.629127: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:47.302197: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:47.720318: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:48.079529: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:48.412736: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:49.093921: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:49.688317: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:50.327681: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:50.957996: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:51.642304: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.2585"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 09:11:57.768440: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:58.055471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:58.324862: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:58.586034: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:58.884411: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-08 09:11:59.159482: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 19s 2s/step - loss: 0.2534 - accuracy: 0.2585 - val_loss: 0.3186 - val_accuracy: 0.1939\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 4s 773ms/step - loss: 0.2533 - accuracy: 0.2721 - val_loss: 0.3185 - val_accuracy: 0.2041\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 2s 515ms/step - loss: 0.2533 - accuracy: 0.2517 - val_loss: 0.3185 - val_accuracy: 0.2143\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.2530 - accuracy: 0.2857 - val_loss: 0.3184 - val_accuracy: 0.2245\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.2528 - accuracy: 0.2925 - val_loss: 0.3183 - val_accuracy: 0.2449\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 145ms/step - loss: 0.2532 - accuracy: 0.2721 - val_loss: 0.3183 - val_accuracy: 0.2755\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.2528 - accuracy: 0.2653 - val_loss: 0.3182 - val_accuracy: 0.2449\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 108ms/step - loss: 0.2526 - accuracy: 0.2925 - val_loss: 0.3181 - val_accuracy: 0.2551\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 112ms/step - loss: 0.2532 - accuracy: 0.2041 - val_loss: 0.3181 - val_accuracy: 0.2143\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.2542 - accuracy: 0.2245 - val_loss: 0.3183 - val_accuracy: 0.2041\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.2532 - accuracy: 0.2585 - val_loss: 0.3193 - val_accuracy: 0.1735\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.2493 - accuracy: 0.2925 - val_loss: 0.3217 - val_accuracy: 0.1735\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2522 - accuracy: 0.2925 - val_loss: 0.3261 - val_accuracy: 0.1735\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2650 - accuracy: 0.2585 - val_loss: 0.3306 - val_accuracy: 0.1735\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2672 - accuracy: 0.2517 - val_loss: 0.3339 - val_accuracy: 0.1837\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.2661 - accuracy: 0.2585 - val_loss: 0.3365 - val_accuracy: 0.1837\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 108ms/step - loss: 0.2729 - accuracy: 0.2381 - val_loss: 0.3393 - val_accuracy: 0.1939\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.2714 - accuracy: 0.2517 - val_loss: 0.3417 - val_accuracy: 0.2041\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2659 - accuracy: 0.2925 - val_loss: 0.3451 - val_accuracy: 0.2041\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2768 - accuracy: 0.2449 - val_loss: 0.3480 - val_accuracy: 0.1939\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.2668 - accuracy: 0.2653 - val_loss: 0.3508 - val_accuracy: 0.2041\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2729 - accuracy: 0.1973 - val_loss: 0.3537 - val_accuracy: 0.1939\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2723 - accuracy: 0.2245 - val_loss: 0.3562 - val_accuracy: 0.1939\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2682 - accuracy: 0.2245 - val_loss: 0.3585 - val_accuracy: 0.1837\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2767 - accuracy: 0.2585 - val_loss: 0.3592 - val_accuracy: 0.1837\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.2756 - accuracy: 0.2109 - val_loss: 0.3611 - val_accuracy: 0.1837\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2691 - accuracy: 0.2789 - val_loss: 0.3639 - val_accuracy: 0.1735\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2687 - accuracy: 0.2585 - val_loss: 0.3667 - val_accuracy: 0.1633\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2725 - accuracy: 0.2857 - val_loss: 0.3680 - val_accuracy: 0.1735\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2707 - accuracy: 0.2653 - val_loss: 0.3710 - val_accuracy: 0.1735\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.2662 - accuracy: 0.2449 - val_loss: 0.3729 - val_accuracy: 0.1735\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2751 - accuracy: 0.2653 - val_loss: 0.3744 - val_accuracy: 0.1735\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2670 - accuracy: 0.2857 - val_loss: 0.3762 - val_accuracy: 0.1837\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2651 - accuracy: 0.2449 - val_loss: 0.3775 - val_accuracy: 0.1837\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2704 - accuracy: 0.1769 - val_loss: 0.3768 - val_accuracy: 0.1837\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.2714 - accuracy: 0.2313 - val_loss: 0.3762 - val_accuracy: 0.1735\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.2692 - accuracy: 0.2857 - val_loss: 0.3779 - val_accuracy: 0.1735\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2856 - accuracy: 0.2381 - val_loss: 0.3788 - val_accuracy: 0.1735\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2788 - accuracy: 0.2517 - val_loss: 0.3778 - val_accuracy: 0.1735\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.2697 - accuracy: 0.2993 - val_loss: 0.3775 - val_accuracy: 0.1735\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2594 - accuracy: 0.2245 - val_loss: 0.3784 - val_accuracy: 0.1837\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.2711 - accuracy: 0.2381 - val_loss: 0.3794 - val_accuracy: 0.1837\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.2906 - accuracy: 0.2381 - val_loss: 0.3803 - val_accuracy: 0.1837\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2651 - accuracy: 0.2449 - val_loss: 0.3814 - val_accuracy: 0.1735\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2726 - accuracy: 0.2585 - val_loss: 0.3825 - val_accuracy: 0.1735\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2878 - accuracy: 0.1973 - val_loss: 0.3821 - val_accuracy: 0.1735\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2712 - accuracy: 0.2721 - val_loss: 0.3811 - val_accuracy: 0.1735\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.2680 - accuracy: 0.2585 - val_loss: 0.3796 - val_accuracy: 0.1735\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2927 - accuracy: 0.2517 - val_loss: 0.3786 - val_accuracy: 0.1735\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.2844 - accuracy: 0.2517 - val_loss: 0.3792 - val_accuracy: 0.1735\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.2815 - accuracy: 0.2585 - val_loss: 0.3789 - val_accuracy: 0.1735\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.2816 - accuracy: 0.2313 - val_loss: 0.3781 - val_accuracy: 0.1837\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.2765 - accuracy: 0.2653 - val_loss: 0.3782 - val_accuracy: 0.1837\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2794 - accuracy: 0.1973 - val_loss: 0.3782 - val_accuracy: 0.1735\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.2940 - accuracy: 0.2653 - val_loss: 0.3776 - val_accuracy: 0.1735\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.2848 - accuracy: 0.1973 - val_loss: 0.3768 - val_accuracy: 0.1735\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2791 - accuracy: 0.2857 - val_loss: 0.3752 - val_accuracy: 0.1837\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2897 - accuracy: 0.2381 - val_loss: 0.3744 - val_accuracy: 0.1837\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2829 - accuracy: 0.2517 - val_loss: 0.3751 - val_accuracy: 0.1735\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.2906 - accuracy: 0.2245 - val_loss: 0.3741 - val_accuracy: 0.1837\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.2815 - accuracy: 0.2381 - val_loss: 0.3716 - val_accuracy: 0.1837\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.2841 - accuracy: 0.2449 - val_loss: 0.3709 - val_accuracy: 0.1735\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.3045 - accuracy: 0.2313 - val_loss: 0.3704 - val_accuracy: 0.1735\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 106ms/step - loss: 0.2856 - accuracy: 0.2109 - val_loss: 0.3692 - val_accuracy: 0.1735\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2811 - accuracy: 0.2449 - val_loss: 0.3683 - val_accuracy: 0.1735\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.2998 - accuracy: 0.2313 - val_loss: 0.3675 - val_accuracy: 0.1735\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.2920 - accuracy: 0.2109 - val_loss: 0.3666 - val_accuracy: 0.1735\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 112ms/step - loss: 0.3081 - accuracy: 0.1905 - val_loss: 0.3658 - val_accuracy: 0.1735\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.3083 - accuracy: 0.2653 - val_loss: 0.3648 - val_accuracy: 0.1735\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3010 - accuracy: 0.2517 - val_loss: 0.3640 - val_accuracy: 0.1735\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.2791 - accuracy: 0.2653 - val_loss: 0.3632 - val_accuracy: 0.1735\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2986 - accuracy: 0.2381 - val_loss: 0.3622 - val_accuracy: 0.1735\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.3209 - accuracy: 0.2381 - val_loss: 0.3614 - val_accuracy: 0.1735\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.3081 - accuracy: 0.2041 - val_loss: 0.3605 - val_accuracy: 0.1735\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2878 - accuracy: 0.2449 - val_loss: 0.3600 - val_accuracy: 0.1735\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.2900 - accuracy: 0.2653 - val_loss: 0.3594 - val_accuracy: 0.1735\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2837 - accuracy: 0.2449 - val_loss: 0.3586 - val_accuracy: 0.1735\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2839 - accuracy: 0.2177 - val_loss: 0.3583 - val_accuracy: 0.1735\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.2974 - accuracy: 0.2313 - val_loss: 0.3580 - val_accuracy: 0.1735\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.3123 - accuracy: 0.1973 - val_loss: 0.3575 - val_accuracy: 0.1735\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.2684 - accuracy: 0.2585 - val_loss: 0.3571 - val_accuracy: 0.1735\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.2776 - accuracy: 0.2177 - val_loss: 0.3564 - val_accuracy: 0.1837\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.3123 - accuracy: 0.2041 - val_loss: 0.3557 - val_accuracy: 0.1735\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.3079 - accuracy: 0.2109 - val_loss: 0.3552 - val_accuracy: 0.1735\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2957 - accuracy: 0.1905 - val_loss: 0.3546 - val_accuracy: 0.1735\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 109ms/step - loss: 0.3112 - accuracy: 0.2177 - val_loss: 0.3541 - val_accuracy: 0.1735\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.3034 - accuracy: 0.2721 - val_loss: 0.3536 - val_accuracy: 0.1735\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.2905 - accuracy: 0.2721 - val_loss: 0.3532 - val_accuracy: 0.1735\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 105ms/step - loss: 0.2925 - accuracy: 0.2109 - val_loss: 0.3527 - val_accuracy: 0.1735\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2533 - accuracy: 0.2721 - val_loss: 0.3523 - val_accuracy: 0.1735\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 109ms/step - loss: 0.2866 - accuracy: 0.2653 - val_loss: 0.3519 - val_accuracy: 0.1735\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2871 - accuracy: 0.1973 - val_loss: 0.3515 - val_accuracy: 0.1735\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2819 - accuracy: 0.1769 - val_loss: 0.3511 - val_accuracy: 0.1735\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.3007 - accuracy: 0.2109 - val_loss: 0.3507 - val_accuracy: 0.1735\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2998 - accuracy: 0.2857 - val_loss: 0.3505 - val_accuracy: 0.1735\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2843 - accuracy: 0.2585 - val_loss: 0.3501 - val_accuracy: 0.1735\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2725 - accuracy: 0.1973 - val_loss: 0.3500 - val_accuracy: 0.1735\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2812 - accuracy: 0.1973 - val_loss: 0.3498 - val_accuracy: 0.1735\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2891 - accuracy: 0.2177 - val_loss: 0.3497 - val_accuracy: 0.1735\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2870 - accuracy: 0.1701 - val_loss: 0.3496 - val_accuracy: 0.1735\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.3050 - accuracy: 0.2653 - val_loss: 0.3493 - val_accuracy: 0.1735\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2854 - accuracy: 0.2449 - val_loss: 0.3492 - val_accuracy: 0.1735\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2985 - accuracy: 0.2449 - val_loss: 0.3490 - val_accuracy: 0.1735\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2712 - accuracy: 0.2789 - val_loss: 0.3488 - val_accuracy: 0.1735\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2830 - accuracy: 0.2177 - val_loss: 0.3487 - val_accuracy: 0.1735\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2788 - accuracy: 0.1837 - val_loss: 0.3485 - val_accuracy: 0.1735\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2920 - accuracy: 0.1633 - val_loss: 0.3484 - val_accuracy: 0.1735\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2921 - accuracy: 0.2585 - val_loss: 0.3483 - val_accuracy: 0.1735\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2804 - accuracy: 0.2449 - val_loss: 0.3481 - val_accuracy: 0.1735\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.3006 - accuracy: 0.2245 - val_loss: 0.3479 - val_accuracy: 0.1735\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2743 - accuracy: 0.2517 - val_loss: 0.3478 - val_accuracy: 0.1735\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3186 - accuracy: 0.2041 - val_loss: 0.3477 - val_accuracy: 0.1735\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.2928 - accuracy: 0.3129 - val_loss: 0.3476 - val_accuracy: 0.1735\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2889 - accuracy: 0.2517 - val_loss: 0.3476 - val_accuracy: 0.1735\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3027 - accuracy: 0.2381 - val_loss: 0.3472 - val_accuracy: 0.1735\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2959 - accuracy: 0.2245 - val_loss: 0.3469 - val_accuracy: 0.1735\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2873 - accuracy: 0.2313 - val_loss: 0.3466 - val_accuracy: 0.1735\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.3076 - accuracy: 0.1701 - val_loss: 0.3460 - val_accuracy: 0.1735\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.3245 - accuracy: 0.2449 - val_loss: 0.3457 - val_accuracy: 0.1735\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2572 - accuracy: 0.2245 - val_loss: 0.3459 - val_accuracy: 0.1735\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2759 - accuracy: 0.2313 - val_loss: 0.3456 - val_accuracy: 0.1735\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.3000 - accuracy: 0.2925 - val_loss: 0.3448 - val_accuracy: 0.1735\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2848 - accuracy: 0.2109 - val_loss: 0.3443 - val_accuracy: 0.1735\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2905 - accuracy: 0.2925 - val_loss: 0.3439 - val_accuracy: 0.1735\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2761 - accuracy: 0.2313 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.2881 - accuracy: 0.2381 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.3072 - accuracy: 0.2177 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2926 - accuracy: 0.3061 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2759 - accuracy: 0.2925 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2651 - accuracy: 0.2381 - val_loss: 0.3438 - val_accuracy: 0.1837\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2906 - accuracy: 0.2449 - val_loss: 0.3439 - val_accuracy: 0.1837\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.2699 - accuracy: 0.2517 - val_loss: 0.3439 - val_accuracy: 0.1837\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2944 - accuracy: 0.2245 - val_loss: 0.3439 - val_accuracy: 0.1837\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2763 - accuracy: 0.1973 - val_loss: 0.3439 - val_accuracy: 0.1837\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.2718 - accuracy: 0.2381 - val_loss: 0.3439 - val_accuracy: 0.1837\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.3160 - accuracy: 0.1837 - val_loss: 0.3438 - val_accuracy: 0.1837\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 0.2821 - accuracy: 0.2585 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.3086 - accuracy: 0.2653 - val_loss: 0.3437 - val_accuracy: 0.1837\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.2875 - accuracy: 0.2381 - val_loss: 0.3436 - val_accuracy: 0.1837\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.3024 - accuracy: 0.2585 - val_loss: 0.3434 - val_accuracy: 0.1837\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2891 - accuracy: 0.2245 - val_loss: 0.3434 - val_accuracy: 0.1837\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2739 - accuracy: 0.2585 - val_loss: 0.3434 - val_accuracy: 0.1837\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2852 - accuracy: 0.2653 - val_loss: 0.3434 - val_accuracy: 0.1837\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.3138 - accuracy: 0.2313 - val_loss: 0.3433 - val_accuracy: 0.1837\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2662 - accuracy: 0.2449 - val_loss: 0.3433 - val_accuracy: 0.1837\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.3006 - accuracy: 0.1973 - val_loss: 0.3432 - val_accuracy: 0.1837\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2893 - accuracy: 0.2245 - val_loss: 0.3432 - val_accuracy: 0.1837\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2731 - accuracy: 0.1905 - val_loss: 0.3432 - val_accuracy: 0.1837\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2835 - accuracy: 0.2789 - val_loss: 0.3431 - val_accuracy: 0.1837\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2700 - accuracy: 0.2925 - val_loss: 0.3432 - val_accuracy: 0.1837\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2748 - accuracy: 0.2245 - val_loss: 0.3433 - val_accuracy: 0.1837\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2761 - accuracy: 0.2517 - val_loss: 0.3432 - val_accuracy: 0.1837\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.3125 - accuracy: 0.2313 - val_loss: 0.3433 - val_accuracy: 0.1837\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.2672 - accuracy: 0.2585 - val_loss: 0.3433 - val_accuracy: 0.1837\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2768 - accuracy: 0.2721 - val_loss: 0.3450 - val_accuracy: 0.1837\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2846 - accuracy: 0.2381 - val_loss: 0.3516 - val_accuracy: 0.1837\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.3080 - accuracy: 0.2381 - val_loss: 0.3520 - val_accuracy: 0.1837\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.3097 - accuracy: 0.2245 - val_loss: 0.3521 - val_accuracy: 0.1837\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2987 - accuracy: 0.2313 - val_loss: 0.3521 - val_accuracy: 0.1837\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2712 - accuracy: 0.2449 - val_loss: 0.3521 - val_accuracy: 0.1837\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.3039 - accuracy: 0.2653 - val_loss: 0.3522 - val_accuracy: 0.1837\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2811 - accuracy: 0.2721 - val_loss: 0.3521 - val_accuracy: 0.1837\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.3043 - accuracy: 0.2041 - val_loss: 0.3521 - val_accuracy: 0.1837\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2691 - accuracy: 0.2177 - val_loss: 0.3522 - val_accuracy: 0.1837\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.2701 - accuracy: 0.2585 - val_loss: 0.3523 - val_accuracy: 0.1837\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2890 - accuracy: 0.2585 - val_loss: 0.3520 - val_accuracy: 0.1837\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.2816 - accuracy: 0.2381 - val_loss: 0.3459 - val_accuracy: 0.1837\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.3011 - accuracy: 0.2585 - val_loss: 0.3451 - val_accuracy: 0.1837\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.2754 - accuracy: 0.2381 - val_loss: 0.3449 - val_accuracy: 0.1837\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.2909 - accuracy: 0.2245 - val_loss: 0.3448 - val_accuracy: 0.1837\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.3116 - accuracy: 0.2585 - val_loss: 0.3447 - val_accuracy: 0.1837\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.3041 - accuracy: 0.1633 - val_loss: 0.3447 - val_accuracy: 0.1837\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2721 - accuracy: 0.2313 - val_loss: 0.3446 - val_accuracy: 0.1837\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2930 - accuracy: 0.2449 - val_loss: 0.3445 - val_accuracy: 0.1837\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.2886 - accuracy: 0.2381 - val_loss: 0.3444 - val_accuracy: 0.1837\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2753 - accuracy: 0.2449 - val_loss: 0.3444 - val_accuracy: 0.1837\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2899 - accuracy: 0.2177 - val_loss: 0.3443 - val_accuracy: 0.1837\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2878 - accuracy: 0.2517 - val_loss: 0.3443 - val_accuracy: 0.1837\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2755 - accuracy: 0.2313 - val_loss: 0.3443 - val_accuracy: 0.1837\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2921 - accuracy: 0.2857 - val_loss: 0.3443 - val_accuracy: 0.1837\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2929 - accuracy: 0.2517 - val_loss: 0.3442 - val_accuracy: 0.1837\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.2664 - accuracy: 0.2381 - val_loss: 0.3443 - val_accuracy: 0.1939\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.3075 - accuracy: 0.2177 - val_loss: 0.3443 - val_accuracy: 0.1939\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.3078 - accuracy: 0.2177 - val_loss: 0.3436 - val_accuracy: 0.2041\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.3113 - accuracy: 0.2585 - val_loss: 0.3435 - val_accuracy: 0.1939\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2766 - accuracy: 0.2449 - val_loss: 0.3433 - val_accuracy: 0.1939\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.3032 - accuracy: 0.2517 - val_loss: 0.3432 - val_accuracy: 0.1939\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 147ms/step - loss: 0.2834 - accuracy: 0.2381 - val_loss: 0.3432 - val_accuracy: 0.1939\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 104ms/step - loss: 0.2965 - accuracy: 0.2381 - val_loss: 0.3431 - val_accuracy: 0.1939\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.2973 - accuracy: 0.1837 - val_loss: 0.3430 - val_accuracy: 0.1939\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.3031 - accuracy: 0.2449 - val_loss: 0.3430 - val_accuracy: 0.1939\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.2728 - accuracy: 0.2245 - val_loss: 0.3430 - val_accuracy: 0.1939\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.2897 - accuracy: 0.2245 - val_loss: 0.3431 - val_accuracy: 0.1939\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.2955 - accuracy: 0.2381 - val_loss: 0.3431 - val_accuracy: 0.1939\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.2928 - accuracy: 0.2381 - val_loss: 0.3431 - val_accuracy: 0.1939\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.3136 - accuracy: 0.1769 - val_loss: 0.3432 - val_accuracy: 0.1939\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.2871 - accuracy: 0.2653 - val_loss: 0.3432 - val_accuracy: 0.1939\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.2988 - accuracy: 0.2857 - val_loss: 0.3431 - val_accuracy: 0.1837\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.2949 - accuracy: 0.2177 - val_loss: 0.3431 - val_accuracy: 0.1837\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.2846 - accuracy: 0.2789 - val_loss: 0.3431 - val_accuracy: 0.1837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x60e92bbe0>"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Dropout, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "# Generate toy data for demonstration purposes\n",
    "timesteps = 2\n",
    "input_dim = 82\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(82, input_shape=(timesteps,input_dim), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, input_shape=(timesteps,input_dim), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(8, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, Y_train, epochs=200, batch_size=30, validation_split=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'R2: {r2:.2f}')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(Y_pred, Y_test)\n",
    "plt.ylabel(\"Actual Stock Returns\")\n",
    "plt.xlabel(\"Predicted Stock Returns\")\n",
    "plt.title(\"Predicted vs Actual Stock Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1758499 , 0.1115374 , 0.11806431, 0.35915485, 0.23539351],\n",
       "       [0.14748642, 0.32831475, 0.3417308 , 0.0678051 , 0.114663  ],\n",
       "       [0.17585044, 0.11153541, 0.11806197, 0.35916072, 0.23539153],\n",
       "       [0.1758501 , 0.11153518, 0.11806197, 0.35916072, 0.23539197],\n",
       "       [0.17584987, 0.11153525, 0.11806183, 0.35916096, 0.23539214]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08045977, -0.13218381, -0.09578568, -0.13601514, -0.58045981],\n",
       "       [-0.03092562, -0.00497413, -0.0341696 , -0.020545  , -0.00700478],\n",
       "       [-0.0061308 , -0.01426791, -0.08781975, -0.07338441, -0.05344972],\n",
       "       [ 0.06046141,  0.05724236,  0.05042499, -0.08421559, -0.08364814],\n",
       "       [-0.03031538,  0.02369234,  0.11084151,  0.09242959,  0.06174332]])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
