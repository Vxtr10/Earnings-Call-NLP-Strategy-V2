{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/victor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import textstat\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import yfinance as yf\n",
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "import csv\n",
    "import statistics\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentiment_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "sentiment_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "sentiment_nlp = pipeline(\"text-classification\", model=sentiment_finbert, tokenizer=sentiment_tokenizer)\n",
    "fls_finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls',num_labels=3)\n",
    "fls_tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "fls_nlp = pipeline(\"text-classification\", model=fls_finbert, tokenizer=fls_tokenizer)\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from firebase_admin import auth\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"info/rfc_banks_model.pkl\", \"rb\") as f:\n",
    "    banks_rfc = pickle.load(f)\n",
    "\n",
    "with open(\"info/rfc_automobiles_model.pkl\", \"rb\") as f:\n",
    "    automobiles_rfc = pickle.load(f)\n",
    "\n",
    "with open(\"info/rfc_consumer_durables_model.pkl\", \"rb\") as f:\n",
    "    consumer_durables_rfc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to firebase\n",
    "cred = credentials.Certificate('info/computing-nea-a3b68-firebase-adminsdk-m2onq-44557de7c9.json')\n",
    "app = firebase_admin.initialize_app(cred)\n",
    "\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current user emails and password:\n",
    "\n",
    "# user 1:\n",
    "# email = testing1@gmail.com\n",
    "# password = test123\n",
    "\n",
    "\n",
    "# user 2:\n",
    "# email = testingperson2@gmail.com\n",
    "# password = test456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_user_details(user_id):\n",
    "    year = int(input(\"What year were you born?\"))\n",
    "    month = int(input(\"What month were you born?\"))\n",
    "    day = int(input(\"What day were you born?\"))\n",
    "\n",
    "    dob = datetime(year=year, month=month, day=day)\n",
    "\n",
    "    first_name = input(\"What's your first name?\")\n",
    "    last_name = input(\"What's your last name?\")\n",
    "    sex = input(\"What's your sex?\")\n",
    "\n",
    "    user_details = {\n",
    "        'dob': dob,\n",
    "        'first_name': first_name,\n",
    "        'last_name': last_name,\n",
    "        'sex': sex\n",
    "    }\n",
    "\n",
    "    doc_ref = db.collection(u'userdetails').document(user_id)\n",
    "    doc_ref.set(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"testing1@gmail.com\"\n",
    "password = \"test123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID: Yxn9r9bkK8fpxHEXgTMSo69oQGD2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "submit_user_details() missing 1 required positional argument: 'user_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/victor/Desktop/Computing_NEA/Basic_User_Interface.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/victor/Desktop/Computing_NEA/Basic_User_Interface.ipynb#ch0000003?line=14'>15</a>\u001b[0m user_id \u001b[39m=\u001b[39m user\u001b[39m.\u001b[39muid\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/victor/Desktop/Computing_NEA/Basic_User_Interface.ipynb#ch0000003?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUser ID:\u001b[39m\u001b[39m\"\u001b[39m, user_id)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/victor/Desktop/Computing_NEA/Basic_User_Interface.ipynb#ch0000003?line=18'>19</a>\u001b[0m submit_user_details()\n",
      "\u001b[0;31mTypeError\u001b[0m: submit_user_details() missing 1 required positional argument: 'user_id'"
     ]
    }
   ],
   "source": [
    "# create a user account\n",
    "\n",
    "# email = input(\"Enter your email: \")\n",
    "# password = input(\"Enter your password: \")\n",
    "\n",
    "try:\n",
    "    user = auth.create_user(email=email, password=password)\n",
    "\n",
    "except Exception as exception_message:\n",
    "    print(\"There has been an error creating an account:\", exception_message)\n",
    "\n",
    "user_id = user.uid\n",
    "\n",
    "print(\"User ID:\", user_id)\n",
    "\n",
    "submit_user_details(user.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully logged in!\n",
      "User ID: Yxn9r9bkK8fpxHEXgTMSo69oQGD2\n"
     ]
    }
   ],
   "source": [
    "# login user account\n",
    "\n",
    "\n",
    "# email = input(\"Enter your email: \")\n",
    "# password = input(\"Enter your password: \")\n",
    "\n",
    "try:\n",
    "    user = auth.get_user_by_email(email)\n",
    "    print(\"You have successfully logged in!\")\n",
    "except Exception as exception_message:\n",
    "    print(\"There has been an error signing in:\", exception_message)\n",
    "\n",
    "user_id = user.uid\n",
    "\n",
    "print(\"User ID:\", user_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully updated your email address\n"
     ]
    }
   ],
   "source": [
    "# Update user email\n",
    "new_email = 'replaceEmail@gmail.com'\n",
    "try:\n",
    "    user = auth.update_user(\n",
    "        user.uid,\n",
    "        email=new_email,\n",
    "    )\n",
    "    print(\"You have successfully updated your email address\")\n",
    "except Exception as exception_message:\n",
    "    print(\"There has been an error updating your account:\", exception_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update user details:\n",
    "submit_user_details(user.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have not deleted this account\n"
     ]
    }
   ],
   "source": [
    "# delete user account\n",
    "AreYouSure = input(\"Are you sure you want to delete this account? -- Y or N\")\n",
    "if AreYouSure == \"Y\":\n",
    "    try:\n",
    "        auth.delete_user(user.uid)\n",
    "\n",
    "        # also deletes it from the userdetails collection\n",
    "        doc_ref = db.collection('userdetails').document(user.uid)\n",
    "        doc_ref.delete()\n",
    "        print(\"You have successfully deleted this account\")\n",
    "    except Exception as exception_message:\n",
    "        print(\"There has been an error deleting your account:\", exception_message)\n",
    "\n",
    "else:\n",
    "    print(\"You have not deleted this account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = input(\"enter stock\")\n",
    "year = input(\"enter year\")\n",
    "quarter = input(\"enter quarter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stock you are searching is already in the global stockpresets collection, the program does not need to compute it again\n",
      "Successfully updated the stock to your view history\n"
     ]
    }
   ],
   "source": [
    "stock = \"GM\"\n",
    "year = 2022\n",
    "quarter = 4\n",
    "document_name, stock_data, stock_data_with_date = find_stock_data(stock, year, quarter, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current watchlists are:\n",
      "watchlist1\n",
      "watchlist3\n",
      "Successfully added GM20224 to the watchlist watchlist1!\n"
     ]
    }
   ],
   "source": [
    "print(\"Your current watchlists are:\")\n",
    "# from userdetails collection\n",
    "user_ref = db.collection('userdetails').document(user_id)\n",
    "collections = user_ref.collections()\n",
    "for collection in collections:\n",
    "    print(collection.id)\n",
    "\n",
    "watchlist_name = input(\"Please enter a watchlist to add this stock to, or enter 'N' if you don't want to save it to your watchlists (the programme will automatically create a watchlist for you if it doesn't exist)\")\n",
    "if watchlist_name == \"N\":\n",
    "    exit()\n",
    "else:\n",
    "    try:\n",
    "        watchlist_ref = user_ref.collection(watchlist_name)\n",
    "        stock_ref = watchlist_ref.document(document_name)\n",
    "        stock_ref.set(stock_data_with_date)\n",
    "        print(f'Successfully added {document_name} to the watchlist {watchlist_name}!')\n",
    "    except:\n",
    "        print(\"There has been an error with adding to your watchlist:\", exception_message)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stock_data(stock, year, quarter, user_id):\n",
    "    sectorlist = [\"automobiles\", \"banks\", \"capital-goods\", \"commercial-services\", \n",
    "    \"consumer-durables\", \"consumer-retailing\", \"consumer-services\", \"diversified-financials\",\n",
    "    \"energy\", \"food-beverage-tobacco\", \"healthcare\", \"household\", \"insurance\", \"materials\", \"media\", \n",
    "    \"pharmaceuticals-biotech\", \"real-estate\", \"retail\", \"semiconductors\", \"software\",\n",
    "    \"tech\", \"telecom\", \"transportation\", \"utilities\"]\n",
    "\n",
    "    for sectorin in sectorlist:\n",
    "        filelist = os.listdir(\"sectors/\"+sectorin)\n",
    "        try:\n",
    "            filelist.remove('.DS_Store')\n",
    "        except:\n",
    "            pass\n",
    "        for stockin in filelist:\n",
    "            if stockin == stock:\n",
    "                sector = sectorin\n",
    "\n",
    "    if sector == \"banks\":\n",
    "        loaded_rfc = banks_rfc\n",
    "\n",
    "    elif sector == \"automobiles\":\n",
    "        loaded_rfc = automobiles_rfc\n",
    "\n",
    "    elif sector == \"consumer-durables\":\n",
    "        loaded_rfc  = consumer_durables_rfc\n",
    "\n",
    "    else:\n",
    "        print(\"this sector doesn't have a preset model\")\n",
    "        exit()\n",
    "    \n",
    "    document_name = stock + str(year) + str(quarter)\n",
    "    doc_ref = db.collection(u'stockpresets').document(document_name)\n",
    "\n",
    "    # if the stock data has already been logged inside the \"stockpresets\" collection, then it will fetch this data from firebase instead of computing everything all over again\n",
    "    if doc_ref.get().exists:\n",
    "        print(\"The stock you are searching is already in the global stockpresets collection, the program does not need to compute it again\")\n",
    "\n",
    "        stock_data = doc_ref.get().to_dict()\n",
    "\n",
    "    else:\n",
    "        print(\"The stock you are searching is not already in the global stockpresets collection, please wait until the program computes the results.\")\n",
    "\n",
    "        document_name = stock + str(year) + str(quarter)\n",
    "\n",
    "        doc_ref = db.collection(u'stockpresets').document(document_name)\n",
    "\n",
    "        ticker = yf.Ticker(stock)\n",
    "        market_cap = ticker.fast_info['market_cap']\n",
    "        mytranscript, exact_date = get_transcript(stock, year, quarter)\n",
    "        earnings_list = ticker.get_earnings_dates(limit=30) #uses yfinance and gets historical eps consensus/reported data\n",
    "        earnings_list.reset_index(inplace=True)\n",
    "        earnings_list = earnings_list[earnings_list['Surprise(%)'].notna()]\n",
    "        eps_list = earnings_list.reset_index(drop=True)\n",
    "        for index in range(0, len(eps_list)):\n",
    "            percentage_surprise = float(eps_list['Surprise(%)'][index])\n",
    "            date = eps_list['Earnings Date'][index]\n",
    "            date = date.replace(tzinfo=None)\n",
    "            date = date.to_pydatetime()\n",
    "            mycase = check_day_diff(date, exact_date)\n",
    "            if mycase == True:\n",
    "                myEPS = percentage_surprise\n",
    "                break\n",
    "        transcript_safe_harbour, transcript_questions = split_transcript(mytranscript)\n",
    "        speaker_names = get_file_speaker_names(sector, stock)\n",
    "        analyst_names = find_analyst_names(speaker_names, transcript_questions)\n",
    "        analyst_sentences, management_sentences = get_analyst_management_sentences(analyst_names, transcript_questions)\n",
    "        fea_ext_list5to22 = getFeature5to22(transcript_safe_harbour, speaker_names)\n",
    "        fea_ext_list23to43 = getFeature23to43(analyst_sentences, management_sentences, speaker_names)\n",
    "        fea_ext_list44to73 = getFeature44to73(mytranscript, speaker_names)\n",
    "        X_dataset = [myEPS] + fea_ext_list5to22 + fea_ext_list23to43 + fea_ext_list44to73 + [market_cap] + [stock] + [year] + [quarter] + [sector]\n",
    "\n",
    "        X_data = np.array(X_dataset)\n",
    "        Single_X_data = X_data[:70]\n",
    "        Single_X_data = Single_X_data.reshape(1,-1)\n",
    "\n",
    "\n",
    "        Y_pred = loaded_rfc.predict(Single_X_data)\n",
    "\n",
    "        Y_pred_proba = loaded_rfc.predict_proba(Single_X_data)\n",
    "\n",
    "        if Y_pred == \"bin_2\":\n",
    "            direction = \"Up\"\n",
    "\n",
    "        elif Y_pred == \"bin_1\":\n",
    "            direction = \"Down\"\n",
    "            \n",
    "        stock_data = {\n",
    "            \"stock\": stock,\n",
    "            \"year\": year,\n",
    "            \"quarter\": quarter,\n",
    "            \"direction\": direction,\n",
    "            \"probabilityUp\": Y_pred_proba[0][1],\n",
    "            \"probabilityDown\": Y_pred_proba[0][0]\n",
    "        }\n",
    "\n",
    "        doc_ref = db.collection(u'stockpresets').document(document_name)\n",
    "        doc_ref.set(stock_data)\n",
    "        print(\"The stock information that will be uploaded to the database is:\", stock_data)\n",
    "        print(\"Successfully added the contents to the global database\")\n",
    "\n",
    "    try:\n",
    "        user_ref = db.collection('viewhistory').document(user_id)\n",
    "\n",
    "        # checks if the userID document exists or not, if not, it will be created\n",
    "        if not user_ref.get().exists:\n",
    "            user_ref.set({})\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        stock_view_filename = stock + current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # checks if the history collection exists\n",
    "        history_ref = user_ref.collection('history')\n",
    "        stock_ref = history_ref.document(stock_view_filename)\n",
    "        \n",
    "        stock_data_with_date = stock_data.copy()\n",
    "        stock_data_with_date[\"DateViewed\"] = current_time\n",
    "        stock_ref.set(stock_data_with_date)\n",
    "\n",
    "        print('Successfully updated the stock to your view history')\n",
    "    except Exception as exception_message:\n",
    "            print(\"There has been an error with updating your view history:\", exception_message)\n",
    "                \n",
    "    return document_name, stock_data, stock_data_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_day_diff(date1, exact_date):\n",
    "    day_difference = abs((exact_date - date1).days)\n",
    "    if day_difference < 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_transcript(stock, date_yr, date_qtr):\n",
    "    try:\n",
    "        url = 'https://roic.ai/transcripts/' + stock + '?y=' + str(date_yr) + '&q=' + str(date_qtr)\n",
    "        html_text = requests.get(url).text\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        script = soup.find_all('script')[15].text.strip()  \n",
    "        data = json.loads(script)\n",
    "        transcript_data = data['props']['pageProps']['transcriptdata']['content']  # loads the transcript content\n",
    "        json_dates = data['props']['pageProps']['data']['data']['earningscalls']\n",
    "        for info in json_dates:\n",
    "            if info[\"year\"] == year and info[\"quarter\"] == quarter:\n",
    "                exact_date = datetime.strptime(info[\"date\"], '%Y-%m-%d %H:%M:%S')\n",
    "    except:\n",
    "        print(\"sorry, the transcript cannot be found, please enter a different date\")\n",
    "        exit()\n",
    "        \n",
    "    return transcript_data, exact_date\n",
    "\n",
    "def get_transcript(stock, year, quarter):\n",
    "    mytranscript = \"\"\n",
    "    mytranscript, exact_date = find_transcript(stock, year, quarter)\n",
    "    mytranscript = re.sub(r'[^A-Za-z0-9.,:!\\'\\n ]', '', mytranscript)\n",
    "    mytranscript = mytranscript.replace(\".\", \". \")\n",
    "    mytranscript = re.sub('[^\\S\\n]+', ' ', mytranscript) #replaces multiple spaces to single space, without deleting newlines \\n in the process\n",
    "    mytranscript = mytranscript.splitlines() # finds transcript\n",
    "    return mytranscript, exact_date\n",
    "\n",
    "def split_transcript(mytranscript):    \n",
    "    transcript_safe_harbour, transcript_questions = \"\", \"\"\n",
    "    for i in range(0, len(mytranscript)):\n",
    "        speech_bubble = mytranscript[i].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space so the IF condition below can run smoothly\n",
    "        if (i > 1) and ((\"operator:\" in speech_bubble) and ((\"question\" in speech_bubble) or (\"go ahead\" in speech_bubble) or (\"operator instructions\" in speech_bubble))):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "        elif (i > 1 ) and (\"operator\" in speech_bubble) and (\"question\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i+1]\n",
    "            transcript_questions = mytranscript[i+1:]\n",
    "            break\n",
    "        elif (i > 1 ) and (\"operator:\" in speech_bubble) and (\"first\" in speech_bubble):\n",
    "            transcript_safe_harbour = mytranscript[0:i]\n",
    "            transcript_questions = mytranscript[i:]\n",
    "            break\n",
    "\n",
    "    return transcript_safe_harbour, transcript_questions\n",
    "\n",
    "def get_file_speaker_names(sector, stock):\n",
    "    write_path = \"sectors/\"+sector+\"/\"+stock+\"/\"+\"speaker names.csv\"\n",
    "    speaker_names = np.loadtxt(write_path, delimiter='\\t', dtype=str)\n",
    "    return speaker_names\n",
    "    \n",
    "def find_analyst_names(speaker_names, transcript_questions):\n",
    "    analyst_names = []\n",
    "    for index in range(0, len(transcript_questions)-1):\n",
    "        speech_bubble = transcript_questions[index].lower()\n",
    "        speech_bubble = re.sub(r'[^\\w\\s:]', ' ', speech_bubble) # regex: replaces all punctuations (except for \":\") with 1 open space \n",
    "        if (\"operator:\" in speech_bubble) or (\"operator :\" in speech_bubble):\n",
    "            for name in speaker_names:\n",
    "                namelist = name.split()\n",
    "                if (name.lower() != \"operator\") and (\"representative\" not in name.lower()) and (\"corporate\" not in name.lower()) and (\"company\" not in name.lower()):\n",
    "                    for name_2 in namelist: # cycle through each name in the name_list\n",
    "                        name_2 = name_2.lower()\n",
    "                        if (((\" \"+name_2+\" \" in speech_bubble) and len(name_2) > 2) and ((\"end\" not in speech_bubble) and (\"closing\" not in speech_bubble) and ((\"turn\" not in speech_bubble) or ((\"over\" not in speech_bubble))))) and (name_2 in transcript_questions[index+1].lower()):\n",
    "                            analyst_names.append(name)\n",
    "                    if \"unidentified\" in name.lower().split(): # finds name such as \"Unidentified Analyst\"\n",
    "                        analyst_names.append(name) \n",
    "                        \n",
    "    analyst_names = list(set(analyst_names)) # replaces duplicates        \n",
    "    return analyst_names\n",
    "\n",
    "def get_analyst_management_sentences(analyst_names, transcript_questions):\n",
    "    analyst_sentences = []\n",
    "    management_sentences = []\n",
    "    for index in range(0, len(transcript_questions)-1):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        if index > 3:\n",
    "            for name in analyst_names:\n",
    "                namelist = name.split()\n",
    "                if (speech_bubble not in analyst_sentences):\n",
    "                    if speaker_name in analyst_names:\n",
    "                        analyst_sentences.append(speech_bubble)\n",
    "\n",
    "                    elif (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "                        pass\n",
    "\n",
    "                    elif ((namelist[0] in speaker_name) or (namelist[-1] in speaker_name)) and ((\"operator:\" in transcript_questions[index-1].lower()) or (\"operator :\" in transcript_questions[index-1].lower())):\n",
    "\n",
    "                        analyst_names.append(speaker_name)\n",
    "                        analyst_sentences.append(speech_bubble)\n",
    "\n",
    "    # get management sentence\n",
    "    for index in range(0, len(transcript_questions)-2):\n",
    "        speech_bubble = transcript_questions[index]\n",
    "        colon_pos = speech_bubble.find(\":\")\n",
    "        speaker_name = speech_bubble[:colon_pos]\n",
    "        \n",
    "        # dont want operator's sentence \n",
    "        if (speaker_name.lower() == \"operator\") or (speaker_name.lower() == \"operator \"):\n",
    "            pass\n",
    "        \n",
    "        elif (speech_bubble not in analyst_sentences):\n",
    "            management_sentences.append(speech_bubble)\n",
    "    return analyst_sentences, management_sentences\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_word = False\n",
    "        self.fail = None\n",
    "        self.word = None\n",
    "\n",
    "class AhoCorasick:\n",
    "    def __init__(self, words):\n",
    "        self.root = TrieNode()\n",
    "        self.build_trie(words)\n",
    "        self.build_ac_automata()\n",
    "\n",
    "    def build_trie(self, words):\n",
    "        for word in words:\n",
    "            node = self.root\n",
    "            for char in word:\n",
    "                if char not in node.children:\n",
    "                    node.children[char] = TrieNode()\n",
    "                node = node.children[char]\n",
    "            node.is_word = True\n",
    "            node.word = word\n",
    "\n",
    "    def build_ac_automata(self):\n",
    "        queue = []\n",
    "\n",
    "        for node in self.root.children.values():\n",
    "            queue.append(node)\n",
    "            node.fail = self.root\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            node = queue.pop(0)\n",
    "            for char, child in node.children.items():\n",
    "                queue.append(child)\n",
    "                fail_node = node.fail\n",
    "                while fail_node is not None and char not in fail_node.children:\n",
    "                    fail_node = fail_node.fail\n",
    "                if fail_node is None:\n",
    "                    child.fail = self.root\n",
    "                else:\n",
    "                    child.fail = fail_node.children[char]\n",
    "                child.is_word |= child.fail.is_word\n",
    "\n",
    "    def remove_words(self, text):\n",
    "        node = self.root\n",
    "        new_text = text\n",
    "        for i, char in enumerate(text):\n",
    "            while node is not None and char not in node.children:\n",
    "                node = node.fail\n",
    "            if node is None:\n",
    "                node = self.root\n",
    "                continue\n",
    "            node = node.children[char]\n",
    "            if node.is_word:\n",
    "                new_text = new_text.replace(node.word, '')\n",
    "        return new_text\n",
    "\n",
    "\n",
    "def getFeature5to22(pre_release, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    net_sentiment_list = []\n",
    "    flesch_list = []\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    feature_extract_5 = 0\n",
    "    feature_extract_6 = 0\n",
    "    feature_extract_7 = 0\n",
    "    feature_extract_8 = 0\n",
    "    feature_extract_9 = 0\n",
    "    feature_extract_10 = 0\n",
    "    feature_extract_11 = 0\n",
    "    feature_extract_12 = 0\n",
    "    feature_extract_13 = 0\n",
    "    feature_extract_14 = 0\n",
    "    feature_extract_15 = 0\n",
    "    feature_extract_16 = 0\n",
    "    feature_extract_17 = 0\n",
    "    feature_extract_18 = 0\n",
    "    feature_extract_19 = 0\n",
    "    feature_extract_20 = 0\n",
    "    feature_extract_21 = 0\n",
    "    feature_extract_22 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in pre_release:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                flesch_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence) \n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, fls1_sentiment_list, net1_positive, net1_negative, net1_neutral = get_fls_features(s_flslist)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, fls2_sentiment_list, net2_positive, net2_negative, net2_neutral = get_fls_features(ns_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, fls3_sentiment_list, net3_positive, net3_negative, net3_neutral = get_fls_features(n_flslist)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            numb_s_flslist = len(s_flslist)\n",
    "            numb_ns_flslist = len(ns_flslist)\n",
    "            numb_n_flslist = len(n_flslist)\n",
    "            total = numb_s_flslist + numb_ns_flslist + numb_n_flslist\n",
    "\n",
    "            feature_extract_21 = numb_s_flslist/total\n",
    "            feature_extract_22 = numb_ns_flslist/total\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            net_positive = net1_positive + net2_positive + net3_positive\n",
    "            net_negative = net1_negative + net2_negative + net3_negative\n",
    "            net_neutral = net1_neutral + net2_neutral + net3_neutral\n",
    "            net_sentiment_list = fls1_sentiment_list + fls2_sentiment_list + fls3_sentiment_list\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_5 = statistics.mean(net_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_6 = net_positive/(net_negative+net_positive+net_neutral)\n",
    "            feature_extract_7 = net_negative/(net_negative+net_positive+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            feature_extract_8 = statistics.mean(flesch_list)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fea_ext_list5to22 = [feature_extract_5, feature_extract_6, feature_extract_7, feature_extract_8, feature_extract_9, feature_extract_10, feature_extract_11, feature_extract_12, feature_extract_13, feature_extract_14, feature_extract_15, feature_extract_16, feature_extract_17, feature_extract_18, feature_extract_19, feature_extract_20, feature_extract_21, feature_extract_22]\n",
    "\n",
    "    except:\n",
    "        fea_ext_list5to22 = [0]*18\n",
    "    \n",
    "    return fea_ext_list5to22\n",
    "\n",
    "\n",
    "def get_fls_features(flslist):\n",
    "    fls_sentiment_list = []\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "    \n",
    "    feature_extract_1 = 0\n",
    "    feature_extract_2 = 0\n",
    "    feature_extract_3 = 0\n",
    "    feature_extract_4 = 0\n",
    "\n",
    "    for each_fls in flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        fls_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "    try:\n",
    "        feature_extract_1 = statistics.mean(fls_sentiment_list)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_2 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_3 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_4 = textstat.flesch_reading_ease(' '.join(flslist))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return feature_extract_1, feature_extract_2, feature_extract_3, feature_extract_4, fls_sentiment_list, net_positive, net_negative, net_neutral\n",
    "\n",
    "\n",
    "def getFeature23to43(analyst_speech, management_speech, speaker_names):\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    n_flslist = []\n",
    "    s_flslist = []\n",
    "    ns_flslist = []\n",
    "\n",
    "    questions_complex_list = []\n",
    "    reply_complex_list = []\n",
    "    net_text_complex_list  = []\n",
    "\n",
    "    # list of sentiments for all S_FLS, N_FLS, NS_FLS classes\n",
    "    s_fls_sentiment_list = []\n",
    "    n_fls_sentiment_list = []\n",
    "    ns_fls_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"question\"\n",
    "    question_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences that are identified as a \"reply\"\n",
    "    reply_sentiment_list = []\n",
    "\n",
    "    # list of sentiments for all sentences in the Q&A section\n",
    "    net_sentiment_list = []\n",
    "\n",
    "    net_positive = 0\n",
    "    net_negative = 0\n",
    "    net_neutral = 0\n",
    "\n",
    "    Qpositive = 0\n",
    "    Qnegative = 0\n",
    "    Qneutral = 0\n",
    "\n",
    "    Rpositive = 0\n",
    "    Rnegative = 0\n",
    "    Rneutral = 0\n",
    "\n",
    "    SFLSpositive = 0\n",
    "    SFLSnegative = 0\n",
    "    SFLSneutral = 0\n",
    "\n",
    "    NSFLSpositive = 0\n",
    "    NSFLSnegative = 0\n",
    "    NSFLSneutral = 0\n",
    "\n",
    "    NFLSpositive = 0\n",
    "    NFLSnegative = 0\n",
    "    NFLSneutral = 0 \n",
    "\n",
    "    feature_extract_23 = 0\n",
    "    feature_extract_24 = 0\n",
    "    feature_extract_25 = 0\n",
    "    feature_extract_26 = 0\n",
    "\n",
    "    feature_extract_27 = 0\n",
    "    feature_extract_28 = 0\n",
    "    feature_extract_29 = 0\n",
    "    feature_extract_30 = 0\n",
    "\n",
    "    feature_extract_31 = 0\n",
    "    feature_extract_32 = 0\n",
    "    feature_extract_33 = 0\n",
    "    feature_extract_34 = 0\n",
    "\n",
    "    feature_extract_35 = 0\n",
    "    feature_extract_36 = 0\n",
    "    feature_extract_37 = 0\n",
    "    \n",
    "    feature_extract_38 = 0\n",
    "    feature_extract_39 = 0\n",
    "    feature_extract_40 = 0\n",
    "    \n",
    "    feature_extract_41 = 0\n",
    "    feature_extract_42 = 0\n",
    "    feature_extract_43 = 0\n",
    "\n",
    "    try:\n",
    "        for speech_bubble in analyst_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                questions_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    sentiment_result = sentiment_nlp(sentence)\n",
    "                    sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "                    question_sentiment_list.append(sentiment_score)\n",
    "                    net_sentiment_list.append(sentiment_score)\n",
    "\n",
    "                    if positivity_value == \"positive\":\n",
    "                        net_positive += 1\n",
    "                        Qpositive += 1\n",
    "\n",
    "                    elif positivity_value == \"negative\":\n",
    "                        net_negative += 1\n",
    "                        Qnegative += 1\n",
    "\n",
    "                    else:\n",
    "                        net_neutral += 1\n",
    "                        Qneutral += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        for speech_bubble in management_speech:\n",
    "            try:\n",
    "                new_speech_bubble = ac.remove_words(speech_bubble)\n",
    "                new_speech_bubble = re.sub('[^\\S\\n]+', ' ', new_speech_bubble)\n",
    "\n",
    "                if new_speech_bubble[0] == \" \": \n",
    "                    new_speech_bubble = new_speech_bubble.replace(\" \", \"\", 1) # replace the first space bar with an empty string, for example ' is is string is is string' to 'is is string is is string'\n",
    "                    \n",
    "                # gets text complexity\n",
    "                flesch_score = textstat.flesch_reading_ease(new_speech_bubble)\n",
    "                reply_complex_list.append(flesch_score)\n",
    "                net_text_complex_list.append(flesch_score)\n",
    "\n",
    "                new_speech_bubble_list = split_paragraph_into_sentences(new_speech_bubble)\n",
    "\n",
    "                fls_results = fls_nlp(new_speech_bubble_list)\n",
    "                \n",
    "                for i in range(0, len(new_speech_bubble_list)):\n",
    "                    sentence = new_speech_bubble_list[i]\n",
    "                    if fls_results[i]['label'] == 'Not FLS':\n",
    "                        n_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Specific FLS':\n",
    "                        s_flslist.append(sentence)\n",
    "                    elif fls_results[i]['label'] == 'Non-specific FLS':                    \n",
    "                        ns_flslist.append(sentence)\n",
    "                    \n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        # for \"n_flslist\":\n",
    "        n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, n_flslist, n_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NFLSpositive, NFLSnegative, NFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"s_flslist\":\n",
    "        s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, s_flslist, s_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, SFLSpositive, SFLSnegative, SFLSneutral, net_positive, net_negative, net_neutral)\n",
    "        \n",
    "        # for \"ns_flslist\":\n",
    "        ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral = get_SentimentLists_from_FLS(net_sentiment_list, ns_flslist, ns_fls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, NSFLSpositive, NSFLSnegative, NSFLSneutral, net_positive, net_negative, net_neutral)\n",
    "\n",
    "        feature_extract_23 = statistics.mean(net_sentiment_list)\n",
    "\n",
    "        try:\n",
    "            feature_extract_24 = net_positive/(net_positive+net_negative+net_neutral)\n",
    "            feature_extract_25 = net_negative/(net_positive+net_negative+net_neutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_26 = statistics.mean(net_text_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_27 = statistics.mean(question_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_28 = Qpositive/(Qpositive+Qnegative+Qneutral)\n",
    "            feature_extract_29 = Qnegative/(Qpositive+Qnegative+Qneutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_30 = statistics.mean(questions_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_31 = statistics.mean(reply_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_32 = Rpositive/(Rpositive+Rnegative+Rneutral)\n",
    "            feature_extract_33 = Rnegative/(Rpositive+Rnegative+Rneutral)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_34 = statistics.mean(reply_complex_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_35 = statistics.mean(s_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_36 = SFLSpositive/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "            feature_extract_37 = SFLSnegative/(SFLSpositive+SFLSnegative+SFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            feature_extract_38 = statistics.mean(ns_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_39 = NSFLSpositive/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "            feature_extract_40 = NSFLSnegative/(NSFLSpositive+NSFLSnegative+NSFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            feature_extract_41 = statistics.mean(n_fls_sentiment_list)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            feature_extract_42 = NFLSpositive/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "            feature_extract_43 = NFLSnegative/(NFLSpositive+NFLSnegative+NFLSneutral)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return [feature_extract_23, feature_extract_24, feature_extract_25, feature_extract_26, feature_extract_27, feature_extract_28, feature_extract_29, feature_extract_30, feature_extract_31, feature_extract_32, feature_extract_33, feature_extract_34, feature_extract_35, feature_extract_36, feature_extract_37, feature_extract_38, feature_extract_39, feature_extract_40, feature_extract_41, feature_extract_42, feature_extract_43]\n",
    "\n",
    "def get_SentimentLists_from_FLS(net_sentiment_list, THIS_flslist, THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral):\n",
    "    for each_fls_sentence in THIS_flslist:\n",
    "        sentiment_result = sentiment_nlp(each_fls_sentence)\n",
    "        sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "        THISfls_sentiment_list.append(sentiment_score)\n",
    "        reply_sentiment_list.append(sentiment_score)\n",
    "        net_sentiment_list.append(sentiment_score)\n",
    "        \n",
    "        if positivity_value == \"positive\":\n",
    "            net_positive += 1\n",
    "            FLSpositive += 1\n",
    "            Rpositive += 1\n",
    "        elif positivity_value == \"negative\":\n",
    "            net_negative += 1\n",
    "            FLSnegative += 1\n",
    "            Rnegative += 1\n",
    "\n",
    "        else:\n",
    "            net_neutral += 1\n",
    "            FLSneutral += 1\n",
    "            Rneutral += 1\n",
    "\n",
    "    return THISfls_sentiment_list, reply_sentiment_list, Rpositive, Rnegative, Rneutral, FLSpositive, FLSnegative, FLSneutral, net_positive, net_negative, net_neutral\n",
    "\n",
    "def deepCleanTranscript(mytranscript, speaker_names):\n",
    "    updatedTranscript = ' '.join(mytranscript)\n",
    "    new_speaker_names = [word + ':' for word in speaker_names]\n",
    "    ac = AhoCorasick(new_speaker_names)\n",
    "\n",
    "    updatedTranscript = ac.remove_words(updatedTranscript)\n",
    " \n",
    "    if updatedTranscript[0] == \" \": \n",
    "        updatedTranscript = updatedTranscript.replace(\" \", \"\", 1)\n",
    "\n",
    "    updatedTranscript = re.sub('[^\\S\\n]+', ' ', updatedTranscript)\n",
    "    updatedTranscript.lower()\n",
    "\n",
    "    return updatedTranscript\n",
    "\n",
    "def getFeature44to73(mytranscript, speaker_names):\n",
    "    marginSentimentList = []\n",
    "    mar_positive = 0\n",
    "    mar_negative = 0\n",
    "    mar_neutral = 0\n",
    "\n",
    "    costSentimentList = []\n",
    "    cost_positive = 0\n",
    "    cost_negative = 0\n",
    "    cost_neutral = 0\n",
    "\n",
    "    revenueSentimentList = []\n",
    "    rev_positive = 0\n",
    "    rev_negative = 0\n",
    "    rev_neutral = 0\n",
    "\n",
    "    earningsEBIDTASentimentList = []\n",
    "    ear_positive = 0\n",
    "    ear_negative = 0\n",
    "    ear_neutral = 0\n",
    "    \n",
    "    growthSentimentList = []\n",
    "    gro_positive = 0\n",
    "    gro_negative = 0\n",
    "    gro_neutral = 0\n",
    "\n",
    "    leverageDebtSentimentList = []\n",
    "    lev_positive = 0\n",
    "    lev_negative = 0\n",
    "    lev_neutral = 0\n",
    "\n",
    "    IndSentimentList = []\n",
    "    ind_positive = 0\n",
    "    ind_negative = 0\n",
    "    ind_neutral = 0\n",
    "\n",
    "    operationSentimentList = []\n",
    "    ope_positive = 0\n",
    "    ope_negative = 0\n",
    "    ope_neutral = 0\n",
    "\n",
    "    cashflowSentimentList = []\n",
    "    cash_positive = 0\n",
    "    cash_negative = 0\n",
    "    cash_neutral = 0\n",
    "\n",
    "    dividendSentimentList = []\n",
    "    div_positive = 0\n",
    "    div_negative = 0\n",
    "    div_neutral = 0\n",
    "\n",
    "    feature_extract_44 = 0\n",
    "    feature_extract_45 = 0\n",
    "    feature_extract_46 = 0\n",
    "    feature_extract_47 = 0\n",
    "    feature_extract_48 = 0\n",
    "    feature_extract_49 = 0\n",
    "    feature_extract_50 = 0\n",
    "    feature_extract_51 = 0\n",
    "    feature_extract_52 = 0\n",
    "    feature_extract_53 = 0\n",
    "    feature_extract_54 = 0\n",
    "    feature_extract_55 = 0\n",
    "    feature_extract_56 = 0\n",
    "    feature_extract_57 = 0\n",
    "    feature_extract_58 = 0\n",
    "    feature_extract_59 = 0\n",
    "    feature_extract_60 = 0\n",
    "    feautre_extract_61 = 0\n",
    "    feature_extract_62 = 0\n",
    "    feature_extract_63 = 0\n",
    "    feature_extract_64 = 0\n",
    "    feature_extract_65 = 0\n",
    "    feature_extract_66 = 0\n",
    "    feature_extract_67 = 0\n",
    "    feature_extract_68 = 0\n",
    "    feature_extract_69 = 0 \n",
    "    feature_extract_70 = 0\n",
    "    feature_extract_71 = 0\n",
    "    feature_extract_72 = 0\n",
    "    feature_extract_73 = 0\n",
    "\n",
    "    updatedTranscript = deepCleanTranscript(mytranscript, speaker_names)\n",
    "    \n",
    "    updatedTranscriptList = split_paragraph_into_sentences(updatedTranscript)\n",
    "\n",
    "    for mysentence in updatedTranscriptList:\n",
    "        if (\" margin\" in mysentence) or (\" return\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            marginSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                mar_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                mar_negative += 1\n",
    "\n",
    "            else:\n",
    "                mar_neutral += 1\n",
    "            \n",
    "\n",
    "        if \" cost\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            costSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                cost_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                cost_negative += 1\n",
    "\n",
    "            else:\n",
    "                cost_neutral += 1\n",
    "\n",
    "        if (\" revenue\" in mysentence) or (\" top line\" in mysentence) or (\" sales\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            revenueSentimentList.append(sentiment_score)\n",
    "\n",
    "            if positivity_value == \"positive\":\n",
    "                rev_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                rev_negative += 1\n",
    "\n",
    "            else:\n",
    "                rev_neutral += 1\n",
    "\n",
    "        if (\" earning\" in mysentence) or (\" EBIT\" in mysentence) or (\" profit\" in mysentence) or (\" bottom line\" in mysentence) or (\" net income\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            earningsEBIDTASentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ear_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ear_negative += 1\n",
    "\n",
    "            else:\n",
    "                ear_neutral += 1\n",
    "\n",
    "        if (\" growth\" in mysentence) or (\" organic\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            growthSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                gro_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                gro_negative += 1\n",
    "\n",
    "            else:\n",
    "                gro_neutral += 1\n",
    "\n",
    "        if (\" leverage\" in mysentence) or (\" debt\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            leverageDebtSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                lev_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                lev_negative += 1\n",
    "\n",
    "            else:\n",
    "                lev_neutral += 1\n",
    "\n",
    "        if (\" industry\" in mysentence) or (\" industr\" in mysentence) or (\" economy\" in mysentence) or (\" economi\" in mysentence) or (\" sector\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            IndSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ind_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ind_negative += 1\n",
    "\n",
    "            else:\n",
    "                ind_neutral += 1\n",
    "\n",
    "        if \" operation\" in mysentence:\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            operationSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                ope_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                ope_negative += 1\n",
    "\n",
    "            else:\n",
    "                ope_neutral += 1\n",
    "        \n",
    "        if (\" cashflow\" in mysentence) or (\" cash flow\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            cashflowSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                cash_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                cash_negative += 1\n",
    "\n",
    "            else:\n",
    "                cash_neutral += 1\n",
    "\n",
    "        if (\" dividend\" in mysentence) or (\" buyback\" in mysentence) or (\" repurchase\" in mysentence):\n",
    "            sentiment_result = sentiment_nlp(mysentence)\n",
    "            sentiment_score, positivity_value = map_sentiments(sentiment_result)\n",
    "            dividendSentimentList.append(sentiment_score)\n",
    "            if positivity_value == \"positive\":\n",
    "                div_positive += 1\n",
    "\n",
    "            elif positivity_value == \"negative\":\n",
    "                div_negative += 1\n",
    "\n",
    "            else:\n",
    "                div_neutral += 1\n",
    "\n",
    "    try:\n",
    "        feature_extract_44 = statistics.mean(marginSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_45 = mar_positive/(mar_positive+mar_negative+mar_neutral)\n",
    "        feature_extract_46 = mar_negative/(mar_positive+mar_negative+mar_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_47 = statistics.mean(costSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_48 = cost_positive/(cost_positive+cost_negative+cost_neutral)\n",
    "        feature_extract_49 = cost_negative/(cost_positive+cost_negative+cost_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_50 = statistics.mean(revenueSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_51 = rev_positive/(rev_positive+rev_negative+rev_neutral)\n",
    "        feature_extract_52 = rev_negative/(rev_positive+rev_negative+rev_neutral)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_53 = statistics.mean(earningsEBIDTASentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_54 = ear_positive/(ear_positive+ear_negative+ear_neutral)\n",
    "        feature_extract_55 = ear_negative/(ear_positive+ear_negative+ear_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_56 = statistics.mean(growthSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_57 = gro_positive/(gro_positive+gro_negative+gro_neutral)\n",
    "        feature_extract_58 = gro_negative/(gro_positive+gro_negative+gro_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_59 = statistics.mean(leverageDebtSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_60 = lev_positive/(lev_positive+lev_negative+lev_neutral)\n",
    "        feature_extract_61 = lev_negative/(lev_positive+lev_negative+lev_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_62 = statistics.mean(IndSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_63 = ind_positive/(ind_positive+ind_negative+ind_neutral)\n",
    "        feature_extract_64 = ind_negative/(ind_positive+ind_negative+ind_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_65 = statistics.mean(operationSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_66 = ope_positive/(ope_positive+ope_negative+ope_neutral)\n",
    "        feature_extract_67 = ope_negative/(ope_positive+ope_negative+ope_neutral)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_68 = statistics.mean(cashflowSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_69 = cash_positive/(cash_positive+cash_negative+cash_neutral)\n",
    "        feature_extract_70 = cash_negative/(cash_positive+cash_negative+cash_neutral)\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        feature_extract_71 = statistics.mean(dividendSentimentList)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feature_extract_72 = div_positive/(div_positive+div_negative+div_neutral)\n",
    "        feature_extract_73 = div_negative/(div_positive+div_negative+div_neutral)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return [feature_extract_44, feature_extract_45, feature_extract_46, feature_extract_47, feature_extract_48, feature_extract_49, feature_extract_50, feature_extract_51, feature_extract_52, feature_extract_53, feature_extract_54, feature_extract_55, feature_extract_56, feature_extract_57, feature_extract_58, feature_extract_59, feature_extract_60, feautre_extract_61, feature_extract_62, feature_extract_63, feature_extract_64, feature_extract_65, feature_extract_66, feature_extract_67, feature_extract_68, feature_extract_69, feature_extract_70, feature_extract_71, feature_extract_72, feature_extract_73]\n",
    "\n",
    "def map_sentiments(sentiment_result):\n",
    "    sentiment_result = sentiment_result[0]\n",
    "    if sentiment_result['label'] == 'Negative':\n",
    "        return -1 * sentiment_result['score'], \"negative\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Neutral':\n",
    "        return 0, \"neutral\"\n",
    "    \n",
    "    elif sentiment_result['label'] == 'Positive':\n",
    "        return sentiment_result['score'], \"positive\"\n",
    "\n",
    "\n",
    "def split_paragraph_into_sentences(temp):\n",
    "    sentences = nltk.sent_tokenize(temp)\n",
    "    return sentences\n",
    "\n",
    "def get_NLP_values(liststr):\n",
    "    # further analysis includes finding sentiment and word complexity.\n",
    "    if len(liststr) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    else:\n",
    "        # maps sentiment data so it outputs a single sentiment value\n",
    "        sentiment_result = sentiment_nlp(liststr)\n",
    "        # gets \n",
    "        sentiment_score = map_sentiments(sentiment_result)\n",
    "\n",
    "        # word complexity:\n",
    "        flesch_score = textstat.flesch_reading_ease(liststr)\n",
    "        gunning_fog_score = textstat.gunning_fog(liststr)\n",
    "\n",
    "        return sentiment_score, flesch_score, gunning_fog_score\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('NLP_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6271ad5a9cfee5fbc27e23facc018ff52eb81071ca31a423a1af489ce9841234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
